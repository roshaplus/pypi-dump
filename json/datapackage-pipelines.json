{
  "info": {
    "author": "Open Knowledge Foundation",
    "author_email": "info@okfn.org",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# Datapackage Pipelines\n\n[![Travis](https://img.shields.io/travis/frictionlessdata/datapackage-pipelines/master.svg)](https://travis-ci.org/frictionlessdata/datapackage-pipelines) [![Coveralls](http://img.shields.io/coveralls/frictionlessdata/datapackage-pipelines.svg?branch=master)](https://coveralls.io/r/frictionlessdata/datapackage-pipelines?branch=master)\n\n## The Basics\n\n### What is it?\n\n`datapackage-pipelines` is a framework for declarative stream-processing of tabular data. It is built upon the concepts and tooling of the Frictionless Data project.\n\n### Pipelines\n\nThe basic concept in this framework is the pipeline. \n\nA pipeline has a list of processing steps, and it generates a single *data package* as its output. Each step is executed in a _processor_ and consists of the following stages:\n\n- **Modify the data package descriptor** - For example: add metadata, add or remove resources, change resources' data schema etc.\n- **Process resources** - Each row of each resource is processed sequentially. The processor can drop rows, add new ones or modify their contents.\n- **Return stats** - If necessary, the processor can report a dictionary of data which will be returned to the user when the pipeline execution terminates. This can be used, for example, for calculating quality measures for the processed data.\n\nNot every processor needs to do all of these. In fact, you would often find each processing step doing only one of these.\n\n### `pipeline-spec.yaml` file\n\nPipelines are defined in a declarative way, and not in code. One or more pipelines can be defined in a `pipeline-spec.yaml` file. This file specifies the list of processors (referenced by name) and the execution parameters for each of the processors.\n\nHere's an example of a `pipeline-spec.yaml` file:\n\n```yaml\nworldbank-co2-emissions:\n  title: CO2 emission data from the World Bank\n  description: Data per year, provided in metric tons per capita.\n  pipeline:\n    -\n      run: add_metadata\n      parameters:\n        name: 'co2-emissions'\n        title: 'CO2 emissions (metric tons per capita)'\n        homepage: 'http://worldbank.org/'\n    -\n      run: add_resource\n      parameters:\n        name: 'global-data'\n        url: \"http://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel\"\n        format: xls\n        headers: 4\n    -\n      run: stream_remote_resources\n      cache: True\n    -\n      run: set_types\n      parameters:\n         resources: global-data\n         types:\n           \"[12][0-9]{3}\":\n              type: number\n    -\n      run: dump.to_zip\n      parameters:\n          out-file: co2-emissions-wb.zip     \n```\n\nIn this example we see one pipeline called `worldbank-co2-emissions`. Its pipeline consists of 4 steps:\n\n- `metadata`: This is a library processor  (see below), which modifies the data-package's descriptor (in our case: the initial, empty descriptor) - adding `name`, `title` and other properties to the datapackage.\n- `add_resource`: This is another library processor, which adds a single resource to the data-package.\n  This resource has a `name` and a `url`, pointing to the remote location of the data.\n- `stream_remote_resources`: This processor will convert remote resources (like the one we defined in the 1st step) to local resources, streaming the data to processors further down the pipeline (see more about streaming below).\n- `set_types`: This processor assigns data types to fields in the data. In this example, field headers looking like years will be assigned the `number` type.\n- `dump.to_zip`: Create a zipped and validated datapackage with the provided file name.\n\n### Mechanics \n\nAn important aspect of how the pipelines are run is the fact that data is passed in streams from one processor to another. If we get \"technical\" here, then each processor is run in its own dedicated process, where the datapackage is read from its `stdin` and output to its `stdout`. The important thing to note here is that no processor holds the entire data set at any point. \n\nThis limitation is by design - to keep the memory and disk requirements of each processor limited and independent of the dataset size.\n\n### Quick Start\n\nFirst off, create a `pipeline-spec.yaml` file in your current directory. You can take the above file if you just want to try it out.\n\nThen, you can either install `datapackage-pipelines` locally:\n\n```shell\n$ pip install datapackage-pipelines\n\n$ dpp\nAvailable Pipelines:\n- ./worldbank-co2-emissions (*)\n\n$ dpp run ./worldbank-co2-emissions\nINFO :Main:RUNNING ./worldbank-co2-emissions\nINFO :Main:- lib/add_metadata.py\nINFO :Main:- lib/add_resource.py\nINFO :Main:- lib/stream_remote_resources.py\nINFO :Main:- lib/dump/to_zip.py\nINFO :Main:DONE lib/add_metadata.py\nINFO :Main:DONE lib/add_resource.py\nINFO :Main:stream_remote_resources: OPENING http://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel\nINFO :Main:stream_remote_resources: TOTAL 264 rows\nINFO :Main:stream_remote_resources: Processed 264 rows\nINFO :Main:DONE lib/stream_remote_resources.py\nINFO :Main:dump.to_zip: INFO :Main:Processed 264 rows\nINFO :Main:DONE lib/dump/to_zip.py\nINFO :Main:RESULTS:\nINFO :Main:SUCCESS: ./worldbank-co2-emissions \n                    {'dataset-name': 'co2-emissions', 'total_row_count': 264}\n```\n\n(Requirements: _Python 3.6_ or higher)\n\nAlternatively, you could use our docker image:\n\n```shell\n$ docker run -it -v `pwd`:/pipelines:rw \\\n        frictionlessdata/datapackage-pipelines\n<available-pipelines>\n\n$ docker run -it -v `pwd`:/pipelines:rw \\\n       frictionlessdata/datapackage-pipelines run ./worldbank-co2-emissions\n<execution-logs>\n```\n\n### The Command Line Interface - `dpp`\n\nRunning a pipeline from the command line is done using the `dpp` tool.\n\nRunning `dpp` without any argument, will show the list of available pipelines. This is done by scanning the current directory and its subdirectories, searching for `pipeline-spec.yaml` files and extracting the list of pipeline specificiations described within.\n\nEach pipeline has an identifier, composed of the path to the `pipeline-spec.yaml` file and the name of the pipeline, as defined within that description file.\n\nIn order to run a pipeline, you use `dpp run <pipeline-id>`. \n\nYou can also use `dpp run all` for running all pipelines and `dpp run dirty` to run the just the _dirty_ pipelines (more on that later on).\n\n## Deeper look into pipelines\n\n### Processor Resolution\n\nAs previously seen, processors are referenced by name.\n\nThis name is, in fact, the name of a Python script containing the processing code (minus the `.py` extension). When trying to find where is the actual code that needs to be executed, the processor resolver will search in these predefined locations:\n\n- First of all, it will try to find a custom processor with that name in the directory of the `pipeline-spec.yaml` file. \n  Processor names support the dot notation, so you could write `mycode.custom_processor` and it will try to find a processor named `custom_processor.py` in the `mycode` directory, in the same path as the pipeline spec file. \n  For this specific resolving phase, if you would write `..custom_processor` it will try to find that processor in the parent directory of the pipeline spec file.\n  (read on for instructions on how to write custom processors)\n- In case the processor name looks like `myplugin.somename`, it will try to find a processor named `somename` in the `myplugin` plugin. That is - it will see if there's an installed plugin which is called `myplugin`, and if so, whether that plugin publishes a processor called `somename` (more on plugins below).\n- If no processor was found until this point, it will try to search for this processor in the processor search path. The processor search path is taken from the environment variable `DPP_PROCESSOR_PATH`. Each of the `:` separated paths in the path is considered as a possible starting point for resolving the processor.\n- Finally, it will try to find that processor in the Standard Processor Library which is bundled with this package.\n\n### Caching\n\nBy setting the `cached` property on a specific pipeline step to `True`, this step's output will be stored on disk (in the `.cache` directory, in the same location as the `pipeline-spec.yaml` file). \n\nRerunning the pipeline will make use of that cache, thus avoiding the execution of the cached step and its precursors. \n\nInternally, a hash is calculated for each step in the pipeline - which is based on the processor's code, it parameters and the hash of its predecessor. If a cache file exists with exactly the same hash as a specific step, then we can remove it (and its predecessors) and use that cache file as an input to the pipeline\n\nThis way, the cache becomes invalid in case the code or execution parameters changed (either for the cached processor or in any of the preceding processors). \n\n### Dirty tasks and keeping state\n\nThe cache hash is also used for seeing if a pipeline is \"dirty\". When a pipeline completes executing successfully, `dpp` stores the cache hash along with the pipeline id. If the stored hash is different than the currently calculated hash, it means that either the code or the execution parameters were modified, and that the pipeline needs to be re-run.\n\n`dpp` works with two storage backends. For running locally, it uses a python _sqlite DB_ to store the current state of each running task, including the last result and cache hash. The state DB file is stored in a file named `.dpp.db` in the same directory that `dpp` is being run from.\n\nFor other installations, especially ones using the task scheduler, it is recommended to work with the _Redis_ backend. In order to enable the Redis connection, simply set the `DPP_REDIS_HOST` environment variable to point to a running Redis instance.\n\n### Pipeline Dependencies\n\nYou can declare that a pipeline is dependent on another pipeline or datapackage. This dependency is considered when calculating the cache hashes of a pipeline, which in turn affect the validity of cache files and the \"dirty\" state:\n- For pipeline dependencies, the hash of that pipeline is used in the calculation\n- For datapackage dependencies, the `hash` property in the datapackage is used in the calculation\n\nIf the dependency is missing, then the pipeline is marked as 'unable to be executed'.\n\nDeclaring dependencies is done by a `dependencies` property to a pipeline definition in the `pipeline-spec.yaml` file.\nThis property should contain a list of dependencies, each one is an object with the following formats:\n- A single key named `pipeline` whose value is the pipeline id to depend on\n- A single key named `datapackage` whose value is the identifier (or URL) for the datapackage to depend on\n\nExample:\n```yaml\ncat-vs-dog-populations:\n  dependencies:\n    - \n      pipeline: ./geo/regoin-areal\n    - \n      datapackage: http://pets.net/data/dogs-per-regoin/datapackage.json\n    - \n      datapackage: http://pets.net/data/dogs-per-regoin\n  ...\n```\n\n### Validating\n\nEach processor's input is automatically validated for correctness:\n\n- The datapackage is always validated before being passed to a processor, so there's no possibility for a processor to modify a datapackage in a way that renders it invalid.\n\n- Data is not validated against its respective JSON Table Schema, unless explicitly requested by setting the `validate` flag to True in the step's info.\n  This is done for two main reasons:\n\n  - Performance wise, validating the data in every step is very CPU intensive\n  - In some cases you modify the schema in one step and the data in another, so you would only like to validate the data once all the changes were made\n\n  In any case, when using the `set_types` standard processor, it will validate and transform the input data with the new types..\n\n## The Standard Processor Library\n\nA few built in processors are provided with the library.\n\n### ***`add_metadata`***\n\nAdds meta-data to the data-package.\n\n_Parameters_:\n\nAny allowed property (according to the [spec]([http://specs.frictionlessdata.io/data-packages/#metadata)) can be provided here.\n\n*Example*:\n\n```yaml\n- run: add_metadata\n  parameters: \n    name: routes-to-mordor\n    license: CC-BY-SA-4\n    author: Frodo Baggins <frodo@shire.me>\n    contributors:\n      - samwise gamgee <samwise1992@yahoo.com>\n```\n\n### ***`add_resource`***\n\nAdds a new remote tabular resource to the data-package. \n\n_Parameters_:\n\nYou should provide the `name` and `url` attributes, and other optional attributes as defined in the [spec]([http://specs.frictionlessdata.io/data-packages/#resource-information).\n\nNote that `url` also supports `env://<environment-variable>`, which indicates that the resource url should be fetched from the indicated environment variable.  This is useful in case you are supplying a string with sensitive information (such as an SQL connection string for streaming from a database table).\n\nParameters are basically arguments that are passed to a `tabulator.Stream` instance (see the [API](https://github.com/frictionlessdata/tabulator-py#api-reference)).\nOther than those, you can pass a `constants` parameter which should be a mapping of headers to string values.\nWhen used in conjunction with `stream_remote_resources`, these constant values will be added to each generated row \n(as well as to the default schema). \n\nYou may also provide a schema here, or use the default schema generated by the `stream_remote_resources` processor.\nIn case `path` is specified, it will be used. If not, the `stream_remote_resources` processor will assign the `path` for you with a `csv` extension.\n\n*Example*:\n\n```yaml\n- run: add_resource\n  parameters: \n    url: http://example.com/my-excel-file.xlsx\n    sheet: 1\n    headers: 2\n- run: add_resource\n  parameters:\n    url: http://example.com/my-csv-file.csv\n    encoding: \"iso-8859-2\"\n```\n\n### ***`stream_remote_resources`***\n\nConverts remote resources to streamed resources.\n\nRemote resources are ones that link to a remote data source, but are not processed by the pipeline and are kept as-is.\n\nStreamed resources are ones that can be processed by the pipeline, and their output is saved as part of the resulting datapackage.\n\nIn case a resource has no schema, a default one is generated automatically here by creating a `string` field from each column in the data source.\n\n_Parameters_:\n\n- `resources` - Which resources to stream. Can be:\n\n  - List of strings, interpreted as resource names to stream\n  - String, interpreted as a regular expression to be used to match resource names\n\n  If omitted, all resources in datapackage are streamed.\n\n- `ignore-missing` - if true, then missing resources won't raise an error but will be treated as 'empty' (i.e. with zero rows). \n  Resources with empty URLs will be treated the same (i.e. will generate an 'empty' resource).\n\n*Example*:\n\n```yaml\n- run: stream_remote_resources\n  parameters: \n    resources: ['2014-data', '2015-data']\n- run: stream_remote_resources\n  parameters: \n    resources: '201[67]-data'\n```\n\nThis processor also supports loading plain-text resources (e.g. html pages) and handling them as tabular data - split into rows with a single \"data\" column.\nTo enable this behavior, add the following attribute to the resource: `\"format\": \"txt\"`.\n\n### ***`set_types`***\n\nSets data types and type options to fields in streamed resources, and make sure that the data still validates with the new types. \n\nThis allows to make modification to the existing table schema, and usually to the default schema from `stream_remote_resources`.\n\n_Parameters_:\n\n-  `resources` - Which resources to modify. Can be:\n\n   - List of strings, interpreted as resource names to stream\n   - String, interpreted as a regular expression to be used to match resource names\n\n   If omitted, all resources in datapackage are streamed.\n\n-  `types` - A map between field names and field definitions.\n   - _field name_ is either simply the name of a field, or a regular expression matching multiple fields.\n   - _field definition_ is an object adhering to the [JSON Table Schema spec](http://specs.frictionlessdata.io/table-schema/). You can use `null` instead of an object to remove a field from the schema.\n\n\n*Example*:\n\n```yaml\n- run: add_resources\n  parameters:\n    name: example-resource\n    url: http://example.com/my-csv-file.csv\n    encoding: \"iso-8859-2\"\n- run: stream_remote_resources\n- run: set_types\n  parameters:\n    resources: example-resource\n    types:\n      age: \n        type: integer\n      \"yealry_score_[0-9]{4}\": \n        type: number\n      \"date of birth\":\n        type: date\n        format: \"fmt:dd/mm/YYYY\"\n      \"social security number\": null\n```\n\n### ***`load_metadata`***\n\nLoads metadata from an existing data-package. \n\n_Parameters_:\n\nLoads the metadata from the data package located at `url`.\n\nAll properties of the loaded datapackage will be copied (except the `resources`)\n\n*Example*:\n\n```yaml\n- run: load_metadata\n  parameters: \n    url: http://example.com/my-datapackage/datapackage.json\n```\n\n### ***`load_resource`***\n\nLoads a tabular resource from an existing data-package. \n\n_Parameters_:\n\nLoads the resource specified in the `resource` parameter from the data package located at `url`.\n\n`resource` can be \n   - List of strings, interpreted as resource names to load\n   - String, interpreted as a regular expression to be used to match resource names\n   - an integer, indicating the index of the resource in the data package (0-based) \n\nAll properties of the loaded resource will be copied - `path` and `schema` included.\n\n*Example*:\n\n```yaml\n- run: load_resource\n  parameters: \n    url: http://example.com/my-datapackage/datapackage.json\n    resource: my-resource\n- run: load_resource\n  parameters:\n    url: http://example.com/my-other-datapackage/datapackage.json\n    resource: 1\n```\n\n\n### ***`concatenate`***\n\nConcatenates a number of streamed resources and converts them to a single resource.\n\n_Parameters_:\n\n- `sources` - Which resources to concatenate. Same semantics as `resources` in `stream_remote_resources`.\n\n  If omitted, all resources in datapackage are concatenated.\n\n  Resources to concatenate must appear in consecutive order within the data-package.\n\n- `target` - Target resource to hold the concatenated data. Should define at least the following properties:\n\n  - `name` - name of the resource\n  - `path` - path in the data-package for this file.\n\n  If omitted, the target resource will receive the name `concat` and will be saved at `data/concat.csv` in the datapackage.\n\n- `fields` - Mapping of fields between the sources and the target, so that the keys are the _target_ field names, and values are lists of _source_ field names.\n\n  This mapping is used to create the target resources schema.\n\n  Note that the target field name is _always_ assumed to be mapped to itself.\n\n*Example*:\n\n```yaml\n- run: concatenate\n  parameters: \n    target:\n      name: multi-year-report\n      path: data/multi-year-report.csv\n    sources: 'report-year-20[0-9]{2}'\n    fields:\n      activity: []\n      amount: ['2009_amount', 'Amount', 'AMOUNT [USD]', '$$$']    \n```\n\nIn this example we concatenate all resources that look like `report-year-<year>`, and output them to the `multi-year-report` resource.\n\nThe output contains two fields:\n\n- `activity` , which is called `activity` in all sources\n- `amount`, which has varying names in different resources (e.g. `Amount`, `2009_amount`, `amount` etc.)\n\n### ***`join`***\n\nJoins two streamed resources. \n\n\"Joining\" in our case means taking the *target* resource, and adding fields to each of its rows by looking up data in the _source_ resource. \n\nA special case for the join operation is when there is no target stream, and all unique rows from the source are used to create it. \nThis mode is called _deduplication_ mode - The target resource will be created and  deduplicated rows from the source will be added to it.\n\n_Parameters_:\n\n- `source` - information regarding the _source_ resource\n  - `name` - name of the resource\n  - `key` - One of\n    - List of field names which should be used as the lookup key\n    - String, which would be interpreted as a Python format string used to form the key (e.g. `{<field_name_1>}:{field_name_2}`)\n  - `delete` - delete from data-package after joining (`False` by default)\n- `target` - Target resource to hold the joined data. Should define at least the following properties:\n  - `name` - as in `source`\n  - `key` - as in `source`, or `null` for creating the target resource and performing _deduplication_.\n- `fields` - mapping of fields from the source resource to the target resource. \n  Keys should be field names in the target resource.\n  Values can define two attributes:\n  - `name` - field name in the source (by default is the same as the target field name)\n\n  - `aggregate` - aggregation strategy (how to handle multiple _source_ rows with the same key). Can take the following options: \n    - `sum` - summarise aggregated values. \n      For numeric values it's the arithmetic sum, for strings the concatenation of strings and for other types will error.\n\n    - `avg` - calculate the average of aggregated values.\n\n      For numeric values it's the arithmetic average and for other types will err.\n\n    - `max` - calculate the maximum of aggregated values.\n\n      For numeric values it's the arithmetic maximum, for strings the dictionary maximum and for other types will error.\n\n    - `min` - calculate the minimum of aggregated values.\n\n      For numeric values it's the arithmetic minimum, for strings the dictionary minimum and for other types will error.\n\n    - `first` - take the first value encountered\n\n    - `last` - take the last value encountered\n\n    - `count` - count the number of occurrences of a specific key\n      For this method, specifying `name` is not required. In case it is specified, `count` will count the number of non-null values for that source field.\n\n    - `set` - collect all distinct values of the aggregated field, unordered \n\n    - `array` - collect all values of the aggregated field, in order of appearance   \n\n    - `any` - pick any value.\n\n    By default, `aggregate` takes the `any` value.\n\n  If neither `name` or `aggregate` need to be specified, the mapping can map to the empty object `{}` or to `null`.\n- `full`  - Boolean,\n  - If `True` (the default), failed lookups in the source will result in \"null\" values at the source.\n  - if `False`, failed lookups in the source will result in dropping the row from the target.\n\n_Important: the \"source\" resource **must** appear before the \"target\" resource in the data-package._\n\n*Examples*:\n\n```yaml\n- run: join\n  parameters: \n    source:\n      name: world_population\n      key: [\"country_code\"]\n      delete: yes\n    target:\n      name: country_gdp_2015\n      key: [\"CC\"]\n    fields:\n      population:\n        name: \"census_2015\"        \n    full: true\n```\n\nThe above example aims to create a package containing the GDP and Population of each country in the world.\n\nWe have one resource (`world_population`) with data that looks like:\n\n| country_code | country_name   | census_2000 | census_2015 |\n| ------------ | -------------- | ----------- | ----------- |\n| UK           | United Kingdom | 58857004    | 64715810    |\n| ...          |                |             |             |\n\nAnd another resource (`country_gdp_2015`) with data that looks like:\n\n| CC   | GDP (\u00a3m) | Net Debt (\u00a3m) |\n| ---- | -------- | ------------- |\n| UK   | 1832318  | 1606600       |\n| ...  |          |               |\n\nThe `join` command will match rows in both datasets based on the `country_code` / `CC` fields, and then copying the value in the `census_2015` field into a new `population` field.\n\nThe resulting data package will have the `world_population` resource removed and the `country_gdp_2015` resource looking like:\n\n| CC   | GDP (\u00a3m) | Net Debt (\u00a3m) | population |\n| ---- | -------- | ------------- | ---------- |\n| UK   | 1832318  | 1606600       | 64715810   |\n| ...  |          |               |            |\n\n\n\nA more complex example:\n\n```yaml\n- run: join\n  parameters: \n    source:\n      name: screen_actor_salaries\n      key: \"{production} ({year})\"\n    target:\n      name: mgm_movies\n      key: \"{title}\"\n    fields:\n      num_actors:\n        aggregate: 'count'\n      average_salary:\n        name: salary\n        aggregate: 'avg'\n      total_salaries:\n        name: salary\n        aggregate: 'sum'\n    full: false\n```\n\nThis example aims to analyse salaries for screen actors in the MGM studios.\n\nOnce more, we have one resource (`screen_actor_salaries`) with data that looks like:\n\n| year | production                  | actor             | salary   |\n| ---- | --------------------------- | ----------------- | -------- |\n| 2016 | Vertigo 2                   | Mr. T             | 15000000 |\n| 2016 | Vertigo 2                   | Robert Downey Jr. | 7000000  |\n| 2015 | The Fall - Resurrection     | Jeniffer Lawrence | 18000000 |\n| 2015 | Alf - The Return to Melmack | The Rock          | 12000000 |\n| ...  |                             |                   |          |\n\nAnd another resource (`mgm_movies`) with data that looks like:\n\n| title                     | director      | producer     |\n| ------------------------- | ------------- | ------------ |\n| Vertigo 2 (2016)          | Lindsay Lohan | Lee Ka Shing |\n| iRobot - The Movie (2018) | Mr. T         | Mr. T        |\n| ...                       |               |              |\n\nThe `join` command will match rows in both datasets based on the movie name and production year. Notice how we overcome incompatible fields by using different key patterns.\n\nThe resulting dataset could look like:\n\n| title            | director      | producer     | num_actors | average_salary | total_salaries |\n| ---------------- | ------------- | ------------ | ---------- | -------------- | -------------- |\n| Vertigo 2 (2016) | Lindsay Lohan | Lee Ka Shing | 2          | 11000000       | 22000000       |\n| ...              |               |              |            |                |                |\n\n\n### ***`filter`***\n\nFilter streamed resources. \n\n`filter` accepts equality and inequality conditions and tests each row in the selected resources. If none of the conditions validate, the row will be discarded. \n\n_Parameters_:\n\n- `resources` - Which resources to apply the filter on. Same semantics as `resources` in `stream_remote_resources`.\n- `in` - Mapping of keys to values which translate to `row[key] == value` conditions  \n- `out` - Mapping of keys to values which translate to `row[key] != value` conditions\n\nBoth `in` and `out` should be a list of objects.\n\n*Examples*:\n\nFiltering just American and European countries, leaving out countries whose main language is English:\n```yaml\n- run: filter\n  parameters: \n    resources: world_population\n    in:\n      - continent: america \n      - continent: europe \n- run: filter\n  parameters: \n    resources: world_population\n    out:\n      - language: english \n```\n\n### ***`dump.to_sql`***\n\nSaves the datapackage to an SQL database.\n\n_Parameters_:\n\n- `engine` - Connection string for connecting to the SQL Database (URL syntax)\n  Also supports `env://<environment-variable>`, which indicates that the connection string should be fetched from the indicated environment variable.\n  If not specified, assumes a default of `env://DPP_DB_ENGINE`\n- `tables` - Mapping between resources and DB tables. Keys are table names, values are objects with the following attributes:\n  - `resource-name` - name of the resource that should be dumped to the table\n  - `mode` - How data should be written to the DB.   \n    Possible values:\n      - `rewrite` (the default) - rewrite the table, all previous data (if any) will be deleted.\n      - `append` - write new rows without changing already existing data.\n      - `update` - update the table based on a set of \"update keys\". \n        For each new row, see if there already an existing row in the DB which can be updated (that is, an existing row\n        with the same values in all of the update keys). \n        If so - update the rest of the columns in the existing row. Otherwise - insert a new row to the DB.\n  - `update_keys` - Only applicable for the `update` mode. A list of field names that should be used to check for row existence.\n        If left unspecified, will use the schema's `primaryKey` as default.\n  - `indexes` - TBD\n- `updated_column` - Optional name of a column that will be added to the spewed data with boolean value\n  - `true` - row was updated\n  - `false` - row was inserted\n- `updated_id_column` - Optional name of a column that will be added to the spewed data and contain the id of the updated row in DB.\n\n### ***`dump.to_path`***\n\nSaves the datapackage to a filesystem path.\n\n_Parameters_:\n\n- `out-path` - Name of the output path where `datapackage.json` will be stored.\n\n  This path will be created if it doesn't exist, as well as internal data-package paths.\n\n  If omitted, then `.` (the current directory) will be assumed.\n\n- `force-format` - Specifies whether to force all output files to be generated with the same format\n    - if `true` (the default), all resources will use the same format\n    - if `false`, format will be deduced from the file extension. Resources with unknown extenstions will be discrarded.\n- `format` - Specifies the type of output files to be generated (if `force-format` is true): `csv` (the default) or `json`\n- `handle-non-tabular` - Specifies whether non tabular resources (i.e. resources without a `schema`) should be dumped as well to the resulting datapackage.\n    (See note below for more details)\n- `counters` - Specifies whether to count rows, bytes or md5 hash of the data and where it should be stored. An object with the following properties:\n    - `datapackage-rowcount`: Where should a total row count of the datapackage be stored (default: `count_of_rows`)\n    - `datapackage-bytes`: Where should a total byte count of the datapackage be stored (default: `bytes`)\n    - `datapackage-hash`: Where should an md5 hash of the datapackage be stored (default: `hash`)\n    - `resource-rowcount`: Where should a total row count of each resource be stored (default: `count_of_rows`)\n    - `resource-bytes`: Where should a total byte count of each resource be stored (default: `bytes`)\n    - `resource-hash`: Where should an md5 hash of each resource be stored (default: `hash`)\n    Each of these attributes could be set to null in order to prevent the counting.\n    Each property could be a dot-separated string, for storing the data inside a nested object (e.g. `stats.rowcount`)\n\n### ***`dump.to_zip`***\n\nSaves the datapackage to a zipped archive.\n\n_Parameters_:\n\n- `out-file` - Name of the output file where the zipped data will be stored\n- `force-format` and `format` - Same as in `dump.to_path` \n- `handle-non-tabular` - Same as in `dump.to_path` \n- `counters` - Same as in `dump.to_path` \n\n#### *Note*\n\n`dump.to_path` and `dump.to_zip` processors will handle non-tabular resources as well.\nThese resources must have both a `url` and `path` properties, and _must not_ contain a `schema` property.\nIn such cases, the file will be downloaded from the `url` and placed in the provided `path`.\n\n## Custom Processors\n\nIt's quite reasonable that for any non-trivial processing task, you might encounter a problem that cannot be solved using the standard library processors.\n\nFor that you might need to write your own processor - here's how it's done.\n\nThere are two APIs for writing processors - the high level API and the low level API.\n\n**Important**: due to the way that pipeline execution is implemented, you **cannot** `print` from within a processor. In case you need to debug, _only_ use the `logging` module to print out anything you need.\n\n### High Level Processor API\n\nThe high-level API is quite useful for most processor kinds:\n\n```python\nfrom datapackage_pipelines.wrapper import process\n\ndef modify_datapackage(datapackage, parameters, stats):\n    # Do something with datapackage\n    return datapackage\n\ndef process_row(row, row_index, \n                resource_descriptor, resource_index,\n                parameters, stats):\n    # Do something with row\n    return row\n\nprocess(modify_datapackage=modify_datapackage,\n        process_row=process_row)\n```\n\nThe high level API consists of one method, `process` which takes two functions:\n\n- `modify_datapackage` - which makes changes (if necessary) to the data-package descriptor, e.g. adds metadata, adds resources, modifies resources' schema etc.\n\n  Can also be used for initialization code when needed.\n\n  It has these arguments:\n\n  - `datapackage` is the current data-package descriptor that needs to be modified.\n    The modified data-package descriptor needs to be returned.\n  - `parameters` is a dict containing the processor's parameters, as provided in the `pipeline-spec.yaml` file.\n  - `stats` is a dict which should be modified in order to collect metrics and measurements in the process (e.g. validation checks, row count etc.)\n\n- `process_row` - which modifies a single row in the stream. It receives these arguments:\n  - `row` is a dictionary containing the row to process\n  - `row_index` is the index of the row in the resource\n  - `resource_descriptor` is the descriptor object of the current resource being processed\n  - `resource_index` is the index of the resource in the data-package\n  - `parameters` is a dict containing the processor's parameters, as provided in the `pipeline-spec.yaml` file.\n  - `stats` is a dict which should be modified in order to collect metrics and measurements in the process (e.g. validation checks, row count etc.)\n\n  and yields zero or more processed rows.\n\n#### A few examples\n\n```python\n# Add license information\nfrom datapackage_pipelines.wrapper import process\n\ndef modify_datapackage(datapackage, parameters, stats):\n    datapackage['license'] = 'CC-BY-SA'\n    return datapackage\n\nprocess(modify_datapackage=modify_datapackage)\n```\n\n```python\n# Add new column with constant value to first resource\n# Column name and value are taken from the processor's parameters\nfrom datapackage_pipelines.wrapper import process\n\ndef modify_datapackage(datapackage, parameters, stats):\n    datapackage['resources'][0]['schema']['fields'].append({\n      'name': parameters['column-name'],\n      'type': 'string'\n    })\n    return datapackage\n\ndef process_row(row, row_index, resource_descriptor, resource_index, parameters, stats):\n    if resource_index == 0:\n        row[parameters['column-name']] = parameters['value']\n    return row\n\nprocess(modify_datapackage=modify_datapackage,\n        process_row=process_row)\n```\n\n```python\n# Row counter\nfrom datapackage_pipelines.wrapper import process\n\ndef modify_datapackage(datapackage, parameters, stats):\n    stats['row-count'] = 0\n    return datapackage\n\ndef process_row(row, row_index, resource_descriptor, resource_index, parameters, stats):\n    stats['row-count'] += 1\n    return row\n\nprocess(modify_datapackage=modify_datapackage,\n        process_row=process_row)\n```\n\n### Low Level Processor API\n\nIn some cases, the high-level API might be too restricting. In these cases you should consider using the low-level API.\n\n```python\nfrom datapackage_pipelines.wrapper import ingest, spew\n\nparameters, datapackage, resource_iterator = ingest()\n\n# Initialisation code, if needed\n\n# Do stuff with datapackage\n# ...\n\nstats = {}\n\n# and resources:\ndef new_resource_iterator(resource_iterator_):\n    def resource_processor(resource_):\n        # resource_.spec is the resource descriptor\n        for row in resource_:\n            # Do something with row\n            # Perhaps collect some stats here as well\n            yield row\n    for resource in resource_iterator_:\n        yield resource_processor(resource)\n\nspew(datapackage, new_resource_iterator(resource_iterator), stats)\n```\n\nThe above code snippet shows the structure of most low-level processors.\n\nWe always start with calling `ingest()` - this method gives us the execution parameters, the data-package descriptor (as outputed from the previous step) and an iterator on all streamed resources' rows.\n\nWe finish the processing by calling `spew()`, which sends the processed data to the next processor in the pipeline. `spew` receives a modified data-package descriptor, a (possibly new) iterator on the resources and a stats object which will be added to stats from previous steps and returned to the user upon completion of the pipeline.\n\n#### A more in-depth explanation\n\n`spew` writes the data it receives in the following order:\n\n- First, the `datapackage` parameter is written to the stream. \n  This means that all modifications to the data-package descriptor must be done _before_ `spew` is called.\n  One common pitfall is to modify the data-package descriptor inside the resource iterator - try to avoid that, as the descriptor that the next processor will receive will be wrong.\n- Then it starts iterating on the resources. For each resource, it iterates on its rows and writes each row to the stream.\n  This iteration process eventually causes an iteration on the original resource iterator (the one that's returned from `ingest`). In turn, this causes the process' input stream to be read. Because of the way buffering in operating systems work, \"slow\" processors will read their input slowly, causing the ones before them to sleep on IO while their more CPU intensive counterparts finish their processing. \"quick\" processors will not work aimlessly, but instead will either sleep while waiting for incoming data or while waiting for their output buffer to drain. \n  What is achieved here is that all rows in the data are processed more or less at the same time, and that no processor works too \"far ahead\" on rows that might fail in subsequent processing steps.\n- Finally, the stats are written to the stream. This means that stats can be modified during the iteration, and only the value after the iteration finishes will be used.\n\n#### A few examples\n\nWe'll start with the same processors from above, now implemented with the low level API.\n\n```python\n# Add license information\nfrom datapackage_pipelines.wrapper import ingest, spew\n\n_, datapackage, resource_iterator = ingest()\ndatapackage['license'] = 'MIT'\nspew(datapackage, resource_iterator)\n```\n\n```python\n# Add new column with constant value to first resource\n# Column name and value are taken from the processor's parameters\nfrom datapackage_pipelines.wrapper import ingest, spew\n\nparameters, datapackage, resource_iterator = ingest()\n\ndatapackage['resources'][0]['schema']['fields'].append({\n   'name': parameters['column-name'],\n   'type': 'string'\n})\n\ndef new_resource_iterator(resource_iterator_):\n    def resource_processor(resource_):\n        for row in resource_:\n            row[parameters['column-name']] = parameters['value']\n            yield row\n\n    first_resource = next(resource_iterator_)\n    yield(resource_processor(first_resource))\n\n    for resource in resource_iterator_:\n        yield resource\n\nspew(datapackage, new_resource_iterator(resource_iterator))\n```\n\n```python\n# Row counter\nfrom datapackage_pipelines.wrapper import ingest, spew\n\n_, datapackage, resource_iterator = ingest()\n\nstats = {'row-count': 0}\n\ndef new_resource_iterator(resource_iterator_):\n    def resource_processor(resource_):\n        for row in resource_:\n            stats['row-count'] += 1\n            yield row\n\n    for resource in resource_iterator_:\n        yield resource_processor(resource)\n\nspew(datapackage, new_resource_iterator(resource_iterator), stats)\n```\n\nThis next example shows how to implement a simple web scraper. Although not strictly required, web scrapers are usually the first processor in a pipeline. Therefore, they can ignore the incoming data-package and resource iterator, as there's no previous processor generating data:\n\n```python\n# Web Scraper\nimport requests\nfrom datapackage_pipelines.wrapper import ingest, spew\n\nparameters, _, _ = ingest()\n\nhost = parameters['ckan-instance']\npackage_list_api = 'https://{host}/api/3/action/package_list'\npackage_show_api = 'https://{host}/api/3/action/package_show'\n\ndef scrape_ckan(host_):\n    all_packages = requests.get(package_list_api.format(host=host_))\\\n                           .json()\\\n                           .get('result', [])\n    for package_id in all_packages:\n      params = dict(id=package_id)\n      package_info = requests.get(package_show_api.format(host=host_),\n                                  params=params)\\\n                             .json()\\\n                             .get('result')\n      if result is not None:\n        yield dict(\n            package_id=package_id,\n            author=package_info.get('author'),\n            title=package_info.get('title'),\n        )\n\ndatapackage = {\n  'resources': [\n    {\n      'name': 'package-list',\n      'schema': {\n        'fields': [\n          {'name': 'package_id', 'type': 'string'},\n          {'name': 'author',     'type': 'string'},\n          {'name': 'title',      'type': 'string'},\n        ]\n      }\n    }\n  ]\n}\n\nspew(datapackage, [scrape_ckan(host)])\n```\n\nIn this example we can see that the initial datapackage is generated from scratch, and the resource iterator is in fact a scraper, yielding rows as they are received from the CKAN instance API.\n\n## Plugins and Source Descriptors\n\nWhen writing pipelines in a specific problem domain, one might discover that the processing pipelines that are developed follow a certain pattern. Scraping, or fetching source data tends to be similar to one another. Processing, data cleaning, validation are often the same.\n\nIn order to ease maintenance and avoid boilerplate, a _`datapackage-pipelines` **plugin**_. \n\nPlugins are Python modules named `datapackage_pipelines_<plugin-name>`. Plugins can provide two facilities:\n\n- Processor packs - you can pack processors revolving a certain theme or for a specific purpose in a plugin. Any processor `foo` residing under the `datapackage_pipelines_<plugin-name>.processors` module can be used from within a pipeline as `<plugin-name>.foo`.\n- Pipeline templates - if the class `Generator` exists in the `datapackage_pipelines_<plugin-name>` module, it will be used to generate pipeline based on templates - which we call \"source descriptors\".\n\n### Source Descriptors\n\nA source descriptor is a yaml file containing information which is used to create a full pipeline.\n\n`dpp` will look for files named `<plugin-name>.source-spec.yaml` , and will treat them as input for the pipeline generating code - which should be implemented in a class called `Generator` in the `datapackage_pipelines_<plugin-name>` module.\n\nThis class should inherit from `GeneratorBase` and should implement two methods:\n\n- `generate_pipeline` - \n   which receives the source description and returns an iterator of tuples of the form `(id, details)`.\n   `id` might be a pipeline id, in which case details would be an object containing the pipeline definition.\n   If `id` is of the form `:module:`, then the details are treated as a source spec from the specified module. This way a generator a generator might generate other source specs. \n- `get_schema` - which should return a JSON Schema for validating the source description's structure\n\n#### Example\n\nLet's assume we write a `datapackage_pipelines_ckan` plugin, used to pull data out of [CKAN](https://ckan.org) instances.\n\nHere's how such a hypothetical generator would look like:\n\n```python\nimport os\nimport json\n\nfrom datapackage_pipelines.generators import \\\n    GeneratorBase, slugify, steps, SCHEDULE_MONTHLY\n\nSCHEMA_FILE = os.path.join(os.path.dirname(__file__), 'schema.json')\n\n\nclass Generator(GeneratorBase):\n\n    @classmethod\n    def get_schema(cls):\n        return json.load(open(SCHEMA_FILE))\n\n    @classmethod\n    def generate_pipeline(cls, source):\n        pipeline_id = dataset_name = slugify(source['name'])\n        host = source['ckan-instance']\n        action = source['data-kind']\n\n        if action == 'package-list':\n            schedule = SCHEDULE_MONTHLY\n            pipeline_steps = steps(*[\n                ('ckan.scraper', {\n                   'ckan-instance': host\n                }),\n                ('metadata', {\n                  'name': dataset_name\n                }),\n                ('dump.to_zip', {\n                   'out-file': 'ckan-datapackage.zip'\n                })])\n            pipeline_details = {\n                'pipeline': pipeline_steps,\n                'schedule': {'crontab': schedule}\n            }\n            yield pipeline_id, pipeline_details\n```\n\nIn this case, if we store a `ckan.source-spec.yaml` file looking like this:\n\n```yaml\nckan-instance: example.com\nname: example-com-list-of-packages\ndata-kind: package-list\n```\n\nThen when running `dpp` we will see an available pipeline named `./example-com-list-of-packages`\n\nThis pipeline would internally be composed of 3 steps: `ckan.scraper`, `metadata` and `dump.to_zip`.\n\n#### Validating Source Descriptors\n\nSource descriptors can have any structure that best matches the parameter domain of the output pipelines. However, it must have a consistent structure, backed by a JSON Schema file. In our case, the Schema might look like this:\n\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\":          { \"type\": \"string\" },\n    \"ckan-instance\": { \"type\": \"string\" },\n    \"data-kind\":     { \"type\": \"string\" }\n  },\n  \"required\": [ \"name\", \"ckan-instance\", \"data-kind\" ]\n}\n```\n\n`dpp` will ensure that source descriptor files conform to that schema before attempting to convert them into pipelines using the `Generator` class.\n\n#### Providing Processor Code\n\nIn some cases, a generator would prefer to provide the processor code as well (alongside the pipeline definition).\nIn order to to that, the generator can add a `code` attribute to any step containing the processor's code. When executed, this step won't try to resolve the processor as usual but will the provided code instead.\n\n## Running on a schedule\n\n`datapackage-pipelines` comes with a celery integration, allowing for pipelines to be run at specific times via a `crontab` like syntax.\n\nIn order to enable that, you simply add a `schedule` section to your `pipeline-spec.yaml` file (or return a schedule from the generator class, see above), like so:\n\n```yaml\nco2-information-cdiac:\n  pipeline:\n    -\n        ...\n  schedule:\n    crontab: '0 * * * *'          \n```\n\nIn this example, this pipeline is set to run every hour, on the hour.\n\nTo run the celery daemon, use `celery`'s command line interface to run `datapackage_pipelines.app`. Here's one way to do it:\n\n```shell\n$ python -m celery worker -B -A datapackage_pipelines.app\n```\n\nRunning this server will start by executing all \"dirty\" tasks, and continue by executing tasks based on their schedules.\n\nAs a shortcut for starting the scheduler and the dashboard (see below), you can use a prebuilt _Docker_ image:\n\n```bash\n$ docker run -v `pwd`:/pipelines:rw -p 5000:5000 \\\n        frictionlessdata/datapackage-pipelines server\n```\n\nAnd then browse to `http://<docker machine's IP address>:5000/` to see the current execution status dashboard.\n\n## Pipeline Dashboard\n\nWhen installed on a server or running using the task scheduler, it's often very hard to know exactly what's running and what's the status of each pipeline.\n\nTo make things easier, you can spin up the web dashboard which provides an overview of each pipeline's status, its basic info and the result of it latest execution.\n\nTo start the web server run `dpp serve` from the command line and browse to http://localhost:5000\n\nThe environment variable `DPP_BASE_PATH` will determine whether dashboard will be served from root or from another base path (example value: `/pipelines/`).\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/frictionlessdata/datapackage-pipelines",
    "keywords": "data",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "datapackage-pipelines",
    "platform": "",
    "project_url": "https://pypi.org/project/datapackage-pipelines/",
    "release_url": "https://pypi.org/project/datapackage-pipelines/1.1.5/",
    "requires_dist": [
      "plyvel; extra == 'speedup'",
      "tox; extra == 'develop'",
      "pylama; extra == 'develop'",
      "tabulator (>=1.0.0a)",
      "sqlalchemy",
      "six (>=1.9)",
      "requests",
      "redis",
      "pyyaml",
      "mistune",
      "jsontableschema-sql (>=0.6)",
      "jsontableschema",
      "flask-jsonpify",
      "flask-cors",
      "flask",
      "datapackage (<1.0)",
      "click",
      "celery",
      "cachetools",
      "awesome-slugify"
    ],
    "requires_python": "",
    "summary": "{{ DESCRIPTION }}",
    "version": "1.1.5"
  },
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "6094e04b038ec74d9984849b68f07009",
          "sha256": "80f93b518b99eda5c9d95d89b6725b003b6ad26ad678924e0df7e5394548ee5a"
        },
        "downloads": 163,
        "filename": "datapackage_pipelines-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6094e04b038ec74d9984849b68f07009",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 16430,
        "upload_time": "2016-08-31T16:19:13",
        "url": "https://files.pythonhosted.org/packages/2e/2e/1321ee272d5d9e15b7032ceff97660ad7ebb76255f51d26b49f6e15d14a1/datapackage_pipelines-0.1.0-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "ac454049d54cba50f37fbdbe8f5f61b3",
          "sha256": "6fc5ed9127b370a381a6de23199531625fe39a90f229ed90b1930545d2f511dc"
        },
        "downloads": 308,
        "filename": "datapackage-pipelines-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ac454049d54cba50f37fbdbe8f5f61b3",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 47571,
        "upload_time": "2016-08-31T16:19:15",
        "url": "https://files.pythonhosted.org/packages/36/97/8868a4ed8ca30a2ad501f001ef9e16f0d5a1fb20a136518f537d62be0211/datapackage-pipelines-0.1.0.tar.gz"
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "92dccd134639abadaf2545d4fa90c41c",
          "sha256": "f06856ca7777d487d3001257e8fcef4e299f95f04f332387b226bce1fbbfcd0f"
        },
        "downloads": 162,
        "filename": "datapackage_pipelines-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "92dccd134639abadaf2545d4fa90c41c",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 16423,
        "upload_time": "2016-08-31T16:25:21",
        "url": "https://files.pythonhosted.org/packages/2f/af/2c65cf6941e73b6374d194f5875789128483aa988cf509c54340dd0dab60/datapackage_pipelines-0.1.1-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "32e6053f68b82797f18698eb20040df9",
          "sha256": "fe8e2da34dba4a5797ffca6cbd55397560ecc38407c5bfca7ad20e4a76f7156c"
        },
        "downloads": 172,
        "filename": "datapackage-pipelines-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "32e6053f68b82797f18698eb20040df9",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 47558,
        "upload_time": "2016-08-31T16:25:23",
        "url": "https://files.pythonhosted.org/packages/2e/e0/6fc60e4dad7caddfa30adbb5af62f6933117a0a373da165b3369fe60916b/datapackage-pipelines-0.1.1.tar.gz"
      }
    ],
    "0.1.10": [
      {
        "comment_text": "",
        "digests": {
          "md5": "29d572cef9e36d9afbacc9e7c6239442",
          "sha256": "3c51b30840ecb368037752575938d0d2602b062aef4fb52d56ec2890146ecb8a"
        },
        "downloads": 117,
        "filename": "datapackage-pipelines-0.1.10.tar.gz",
        "has_sig": false,
        "md5_digest": "29d572cef9e36d9afbacc9e7c6239442",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 145163,
        "upload_time": "2016-10-18T05:54:16",
        "url": "https://files.pythonhosted.org/packages/96/a9/4ecded59ebe7e8c5165f4956902041e12e1b4a75fed80d7d324d58971874/datapackage-pipelines-0.1.10.tar.gz"
      }
    ],
    "0.1.11": [
      {
        "comment_text": "",
        "digests": {
          "md5": "7fb60fb8a77e436c06695fc57442306f",
          "sha256": "c957eff474c9d79aecb7f5767c0fa3e0aa5713a1686d8e8d0c400f89373aeb86"
        },
        "downloads": 155,
        "filename": "datapackage-pipelines-0.1.11.tar.gz",
        "has_sig": false,
        "md5_digest": "7fb60fb8a77e436c06695fc57442306f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 145296,
        "upload_time": "2016-10-18T11:18:44",
        "url": "https://files.pythonhosted.org/packages/00/4a/75b4cef06e78f19e6b7850f8c5f8d21fc3cfc533fb39e83acc657d39d8da/datapackage-pipelines-0.1.11.tar.gz"
      }
    ],
    "0.1.13": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4a6d4440dd4c95e7231a2bc2c24cf755",
          "sha256": "1631cd9a30af481b167d11596019be712a754f32e70f90ac999d5e32a4c148f4"
        },
        "downloads": 380,
        "filename": "datapackage-pipelines-0.1.13.tar.gz",
        "has_sig": false,
        "md5_digest": "4a6d4440dd4c95e7231a2bc2c24cf755",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23205,
        "upload_time": "2016-10-23T07:31:33",
        "url": "https://files.pythonhosted.org/packages/05/1e/6d1710d4592857c0bea8a5f58ccda7d509970eca92804c27fe03d944c743/datapackage-pipelines-0.1.13.tar.gz"
      }
    ],
    "0.1.16": [
      {
        "comment_text": "",
        "digests": {
          "md5": "07592b55ba292a03fb623df82daabe1d",
          "sha256": "585708bfb9f636c4b6a408072c88b22d236b82c5572829e87ea437fa1c7ea74b"
        },
        "downloads": 340,
        "filename": "datapackage-pipelines-0.1.16.tar.gz",
        "has_sig": false,
        "md5_digest": "07592b55ba292a03fb623df82daabe1d",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 27448,
        "upload_time": "2016-11-24T14:51:07",
        "url": "https://files.pythonhosted.org/packages/9b/d3/636e83f1b68aedf3307c97a59c0e95e1e3af515c14723c4413f9b465d25c/datapackage-pipelines-0.1.16.tar.gz"
      }
    ],
    "0.1.18": [
      {
        "comment_text": "",
        "digests": {
          "md5": "ac56a5917933fa304515aaf0e6a652bc",
          "sha256": "7cde56ca755a924689944adec1eced3ec51f25f669d0f225d0f85e4791cbd801"
        },
        "downloads": 80,
        "filename": "datapackage-pipelines-0.1.18.tar.gz",
        "has_sig": false,
        "md5_digest": "ac56a5917933fa304515aaf0e6a652bc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 27766,
        "upload_time": "2016-12-17T20:38:57",
        "url": "https://files.pythonhosted.org/packages/91/4a/1bed7e16b9facd75a50106d7d55e0f52ae396ecb680585e1035e415f8b06/datapackage-pipelines-0.1.18.tar.gz"
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "07bb08ee414ee9a673672bc307fd0129",
          "sha256": "84faf75beb5a4fc7219a381732307db2e88a6699b4d216fe1df96bca69abd737"
        },
        "downloads": 118,
        "filename": "datapackage-pipelines-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "07bb08ee414ee9a673672bc307fd0129",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 82928,
        "upload_time": "2016-09-05T18:17:14",
        "url": "https://files.pythonhosted.org/packages/f2/25/8d20c879308c52adec5636dff3aa84160b9423bff23549ee2741d98d924b/datapackage-pipelines-0.1.2.tar.gz"
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "df327d035eb60b0f5f8f76adbb6c8391",
          "sha256": "6f5939b2127adbaa5841a3e47da6ede690e03d1111423bd8846489b094890dc8"
        },
        "downloads": 126,
        "filename": "datapackage-pipelines-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "df327d035eb60b0f5f8f76adbb6c8391",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 140727,
        "upload_time": "2016-09-05T20:44:02",
        "url": "https://files.pythonhosted.org/packages/57/d8/1de086623fe77aab6dccbb9ef0ee3f0c5cd991613404fad0e7141d42e182/datapackage-pipelines-0.1.3.tar.gz"
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "feb27749e335c75b6f2c875f0ea27a1b",
          "sha256": "4d3918b711f285164753751d8203550fe47972da9e4db2f07b5d869846cb3d83"
        },
        "downloads": 92,
        "filename": "datapackage_pipelines-0.1.4-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "feb27749e335c75b6f2c875f0ea27a1b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 29065,
        "upload_time": "2016-09-13T16:17:26",
        "url": "https://files.pythonhosted.org/packages/35/b6/aedf7cbd95d0b78a3ca2500d65d210e6228564d4edb36d30e358fdad2bf7/datapackage_pipelines-0.1.4-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "71051e5c386f6dfc10752113cde800bb",
          "sha256": "dd9ccc16dd588686a28fc090228908b6d48faeb4df6163f3feec57a2ec97bd74"
        },
        "downloads": 93,
        "filename": "datapackage-pipelines-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "71051e5c386f6dfc10752113cde800bb",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 87477,
        "upload_time": "2016-09-13T16:17:28",
        "url": "https://files.pythonhosted.org/packages/e3/a6/7b78eede6287f25e98e82ac450443006141debdcc655c416d4c85c6ddf41/datapackage-pipelines-0.1.4.tar.gz"
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "34f0c7c36558fd5ba4e81e84e6961127",
          "sha256": "e33e8170f9425aa6af645427f15f0b985e5a794a231c004fa08d068c51fd04b3"
        },
        "downloads": 101,
        "filename": "datapackage-pipelines-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "34f0c7c36558fd5ba4e81e84e6961127",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 143697,
        "upload_time": "2016-09-30T13:58:24",
        "url": "https://files.pythonhosted.org/packages/4d/d8/285a209ac7ce41abf86a5472370fe874041802a0376c9a6eac90a38e3dd4/datapackage-pipelines-0.1.5.tar.gz"
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4f1d85a43873a7927d45e5165bc1fd2c",
          "sha256": "c3421106dfe63c0f6d1c1f2d4df671a0b95aa82d771fa6cec4dac55619d5dc9f"
        },
        "downloads": 198,
        "filename": "datapackage-pipelines-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "4f1d85a43873a7927d45e5165bc1fd2c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 143685,
        "upload_time": "2016-10-06T13:46:56",
        "url": "https://files.pythonhosted.org/packages/06/09/5d23094027fe12ce195208dd0474eda2304e4839f36494f8525b2f798242/datapackage-pipelines-0.1.6.tar.gz"
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b869d787ba93c3a8ab5c59115cba7b28",
          "sha256": "1b8f24eea47240202ceef71c045fa79cdbd9b17d665f9a99e0e36ebcf2d429fb"
        },
        "downloads": 96,
        "filename": "datapackage-pipelines-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "b869d787ba93c3a8ab5c59115cba7b28",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 143828,
        "upload_time": "2016-10-13T12:42:08",
        "url": "https://files.pythonhosted.org/packages/dc/d9/beaf6bef6b24eac8d3a98323724a4da83743c40c409b89e285ab0abffcd3/datapackage-pipelines-0.1.7.tar.gz"
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f7fc3454322703ece893aa14cc735bff",
          "sha256": "11c8ea75f26abf956baea4bb07e942eb22d77ec2c42f49739f628cfb25cabcd5"
        },
        "downloads": 108,
        "filename": "datapackage-pipelines-0.1.8.tar.gz",
        "has_sig": false,
        "md5_digest": "f7fc3454322703ece893aa14cc735bff",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 144130,
        "upload_time": "2016-10-15T20:43:00",
        "url": "https://files.pythonhosted.org/packages/80/1a/1efe159bfcd751c543367311c75c69d7593d0e50c963b22fde339bd953be/datapackage-pipelines-0.1.8.tar.gz"
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1f200211c5d3bd6ad83a43d1f793ec4f",
          "sha256": "f67b4b0d7d78f0418f8fe5806484b26f00af234d569a57dafed1f33a1342c164"
        },
        "downloads": 111,
        "filename": "datapackage-pipelines-0.1.9.tar.gz",
        "has_sig": false,
        "md5_digest": "1f200211c5d3bd6ad83a43d1f793ec4f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 144276,
        "upload_time": "2016-10-17T08:15:57",
        "url": "https://files.pythonhosted.org/packages/44/d5/dbbfe4c53aa0181adda4ca5e91dd5afd10f6b9bb8ae5df59c3381fae8746/datapackage-pipelines-0.1.9.tar.gz"
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "a1f675f294685eee727345c61407c1c4",
          "sha256": "7cf210d099f4ef46faed05db16977fc274f1d970a8ae0deaec10badfde513c4b"
        },
        "downloads": 13,
        "filename": "datapackage-pipelines-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a1f675f294685eee727345c61407c1c4",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134621,
        "upload_time": "2017-02-15T17:35:20",
        "url": "https://files.pythonhosted.org/packages/d6/c2/fbc288045cd4276c60cd8a5a350189eaa83de1b6d97f0a3754e95e4faebe/datapackage-pipelines-1.0.1.tar.gz"
      }
    ],
    "1.0.10": [
      {
        "comment_text": "",
        "digests": {
          "md5": "2b32a22650f66971a3729ac1c79cfaea",
          "sha256": "ce38db74ebd98a3b7165fc6789bdf58e9e7bb4f02313d53997309163ecf18cf7"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "2b32a22650f66971a3729ac1c79cfaea",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 223085,
        "upload_time": "2017-04-24T10:24:34",
        "url": "https://files.pythonhosted.org/packages/d5/39/f3e050e4a7d9950ef77ae4a0ee2636f78fae7fc171bbad2d49edcbef41ad/datapackage-pipelines-1.0.10.tar.gz"
      }
    ],
    "1.0.11": [
      {
        "comment_text": "",
        "digests": {
          "md5": "453238e86d09126f5feff1a16b23fc92",
          "sha256": "3ff14640f96194dc3fa9b9b5b55da3e00dfec3389f628ef08b99dd4ce83628de"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "453238e86d09126f5feff1a16b23fc92",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 223623,
        "upload_time": "2017-04-26T19:20:03",
        "url": "https://files.pythonhosted.org/packages/dc/46/12eb06369691d1663b202bcd8be11b752dcc74d0ce30e11f5390fef36efa/datapackage-pipelines-1.0.11.tar.gz"
      }
    ],
    "1.0.13": [
      {
        "comment_text": "",
        "digests": {
          "md5": "2b3b38569f4cc899f9c8d70fbeede60c",
          "sha256": "f0221020b33dcca24262544ad96a961ac388966d180c28f449c4e59c3e45c736"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.13.tar.gz",
        "has_sig": false,
        "md5_digest": "2b3b38569f4cc899f9c8d70fbeede60c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 75784,
        "upload_time": "2017-05-02T18:38:15",
        "url": "https://files.pythonhosted.org/packages/bd/1d/1ed97b72a719e24e9c43f0a5bb7214093810b743af4c49bf2f598a7b5bd9/datapackage-pipelines-1.0.13.tar.gz"
      }
    ],
    "1.0.14": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4cab22edd60d79d687cae82aae255c96",
          "sha256": "528d4b4b5bff24bd7b1b796c9364b30d3b43a45026e6a0240eb862ecfe0d1f3f"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.14.tar.gz",
        "has_sig": false,
        "md5_digest": "4cab22edd60d79d687cae82aae255c96",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 75890,
        "upload_time": "2017-05-03T11:12:35",
        "url": "https://files.pythonhosted.org/packages/f9/76/d99859dbc62b10ded2a6f55ac38240d15d876d39cdf0c7b96c2324067a38/datapackage-pipelines-1.0.14.tar.gz"
      }
    ],
    "1.0.15": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b262374c9288ba77bd9ebdb0db32c934",
          "sha256": "cc4709536016b42dda7fab246d8e7f78354cfe60582e6fe0eff3063afe2bd340"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.15.tar.gz",
        "has_sig": false,
        "md5_digest": "b262374c9288ba77bd9ebdb0db32c934",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 151206,
        "upload_time": "2017-05-21T15:24:10",
        "url": "https://files.pythonhosted.org/packages/65/bd/42231fa799557e875fb2561186cd7e5f48c1ec90bb0214ba5847d7800898/datapackage-pipelines-1.0.15.tar.gz"
      }
    ],
    "1.0.16": [
      {
        "comment_text": "",
        "digests": {
          "md5": "c52abd0a12f9fac5b5fe2b64e6bdca5c",
          "sha256": "023219e2991167bf088420d677a802646ef554cc93f1db81086d70ca35c55c47"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.16.tar.gz",
        "has_sig": false,
        "md5_digest": "c52abd0a12f9fac5b5fe2b64e6bdca5c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 151208,
        "upload_time": "2017-05-21T15:43:52",
        "url": "https://files.pythonhosted.org/packages/64/8d/c587c4f35d70f3c5d275ae4beaf4f9b80b0111de0b6afd6f1bcb77630249/datapackage-pipelines-1.0.16.tar.gz"
      }
    ],
    "1.0.17": [
      {
        "comment_text": "",
        "digests": {
          "md5": "fcbc99224fe6949f98311f83c580fcbd",
          "sha256": "30fb486e68be8ce94789e41bb993699e94fab5d6121401e0f576c3485c770a74"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.17.tar.gz",
        "has_sig": false,
        "md5_digest": "fcbc99224fe6949f98311f83c580fcbd",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152065,
        "upload_time": "2017-05-24T07:19:54",
        "url": "https://files.pythonhosted.org/packages/ce/31/947f3452059fe28a027612f66a179388c6a3b7b1a3bc3d1555772ea797e7/datapackage-pipelines-1.0.17.tar.gz"
      }
    ],
    "1.0.18": [
      {
        "comment_text": "",
        "digests": {
          "md5": "edc17a70d7a9da2cd68115e45cda3f88",
          "sha256": "64a28b19bd7a8e2f7e81d5d8935bf88fe0f36b7b1bed4f6b98ca867cff5b379e"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.18.tar.gz",
        "has_sig": false,
        "md5_digest": "edc17a70d7a9da2cd68115e45cda3f88",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152182,
        "upload_time": "2017-05-24T07:25:49",
        "url": "https://files.pythonhosted.org/packages/26/4b/409686ec55137d18529254c375e9f7f9012cbb934a7aa9f54078fce52e04/datapackage-pipelines-1.0.18.tar.gz"
      }
    ],
    "1.0.19": [
      {
        "comment_text": "",
        "digests": {
          "md5": "2ff3bc7aa43ad20778518685b1a678d8",
          "sha256": "6ba69d02fd44b67233458641cf103ed426ec343c601f19a96311ecd512a4d25a"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.19.tar.gz",
        "has_sig": false,
        "md5_digest": "2ff3bc7aa43ad20778518685b1a678d8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152254,
        "upload_time": "2017-05-29T16:48:56",
        "url": "https://files.pythonhosted.org/packages/61/4e/48856ae8a8874d8db7759496ead5f01781c18e7ea93b7682cd23e99e08fd/datapackage-pipelines-1.0.19.tar.gz"
      }
    ],
    "1.0.20": [
      {
        "comment_text": "",
        "digests": {
          "md5": "49fa888838f5ab46fdb87099e1d42e30",
          "sha256": "e03cbae8058b406534841250303092effdcd54f3a9685a129eb70295c6ca052d"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.20.tar.gz",
        "has_sig": false,
        "md5_digest": "49fa888838f5ab46fdb87099e1d42e30",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152265,
        "upload_time": "2017-06-06T11:01:14",
        "url": "https://files.pythonhosted.org/packages/40/3f/c88e4b5d057f1ca2140845ac7ba3f37b10a4775c017bbb57edb4e61ae9bc/datapackage-pipelines-1.0.20.tar.gz"
      }
    ],
    "1.0.21": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f3326aba70fec112349bec4b84f844db",
          "sha256": "4e3955a25ed4c99643821aee6a9d7097c68435c5cc9173859761f7e5f2a3c5df"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.21.tar.gz",
        "has_sig": false,
        "md5_digest": "f3326aba70fec112349bec4b84f844db",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152379,
        "upload_time": "2017-06-12T11:35:06",
        "url": "https://files.pythonhosted.org/packages/7a/97/1cd9e99ac390f432025614081647c0ede72b300d2a0c421cfb353535c91c/datapackage-pipelines-1.0.21.tar.gz"
      }
    ],
    "1.0.22": [
      {
        "comment_text": "",
        "digests": {
          "md5": "dac8f9162baba82ede394159232a62a2",
          "sha256": "defaa09d689c1cdaeff32f64d548176ce7c0f3772d72a7ca1b471429d7df3368"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.22.tar.gz",
        "has_sig": false,
        "md5_digest": "dac8f9162baba82ede394159232a62a2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 152810,
        "upload_time": "2017-06-29T19:19:33",
        "url": "https://files.pythonhosted.org/packages/0e/35/fc9752ad1c65c6b55435544097fa2966071502226bac3ba0e0ea7bd0a138/datapackage-pipelines-1.0.22.tar.gz"
      }
    ],
    "1.0.24": [
      {
        "comment_text": "",
        "digests": {
          "md5": "cf30fe43d64787718ce06a028547c350",
          "sha256": "c418666df68c606a90e78f4534e97750dfbc6648212c55e3430ad0053e813f81"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.0.24-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cf30fe43d64787718ce06a028547c350",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 83574,
        "upload_time": "2017-07-19T07:24:57",
        "url": "https://files.pythonhosted.org/packages/91/0b/d14960b7258693a82e9cfcbe381d08e4492734e329d957c0d20d079827a7/datapackage_pipelines-1.0.24-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "943df7b9db8a3d1e74bcb07b26c901cc",
          "sha256": "e70c32063d73938b26d6520f366bcbe24123a7946a2423dfb9629d384569381d"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.24.tar.gz",
        "has_sig": false,
        "md5_digest": "943df7b9db8a3d1e74bcb07b26c901cc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 153756,
        "upload_time": "2017-07-19T07:23:14",
        "url": "https://files.pythonhosted.org/packages/b2/86/881d8a4ac6b63266150804749c5c722f36670861540b22db13cd0519d107/datapackage-pipelines-1.0.24.tar.gz"
      }
    ],
    "1.0.25": [
      {
        "comment_text": "",
        "digests": {
          "md5": "8bd9dbec151c097ae1aa8620617da153",
          "sha256": "d8b26a6b788e89a5fa62712df17a97141217f0ecc19d80610a2ea359dea317f9"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.0.25-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8bd9dbec151c097ae1aa8620617da153",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 83828,
        "upload_time": "2017-07-20T10:30:27",
        "url": "https://files.pythonhosted.org/packages/b1/c0/9b5a2fbebd35dad8ef3cb797c72c5dc26aeecb09baa6859208c281bc4b19/datapackage_pipelines-1.0.25-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "566c9ea2dda894b34be14911047db0e4",
          "sha256": "648091dfca7d50bdb3b0be947daee910a88afb81362576112b0c17821ad8ecd9"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.25.tar.gz",
        "has_sig": false,
        "md5_digest": "566c9ea2dda894b34be14911047db0e4",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 79037,
        "upload_time": "2017-07-20T10:30:29",
        "url": "https://files.pythonhosted.org/packages/f3/94/225fee20be9cbc3026560d4b75750b1a30ab80ef688f9c7388902c43a071/datapackage-pipelines-1.0.25.tar.gz"
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f10a21c848da65526906e5706f79e1be",
          "sha256": "d81575e79d4fb2d25f24d9e9ef40c472df99fa14a1bc58a652500a6140b983e8"
        },
        "downloads": 15,
        "filename": "datapackage-pipelines-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f10a21c848da65526906e5706f79e1be",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 139695,
        "upload_time": "2017-03-02T06:56:36",
        "url": "https://files.pythonhosted.org/packages/98/5b/30b828a76d9639eb060e022489ef1706a2e7ea778d70abbc33f43fd53d5a/datapackage-pipelines-1.0.3.tar.gz"
      }
    ],
    "1.0.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1e98e28300cd66785f545fdcdc69ba98",
          "sha256": "743b603265f875b1f9d3e107892c001b909fd95583d41a20a4cd29c9c69308a7"
        },
        "downloads": 13,
        "filename": "datapackage-pipelines-1.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "1e98e28300cd66785f545fdcdc69ba98",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 218184,
        "upload_time": "2017-03-27T20:26:29",
        "url": "https://files.pythonhosted.org/packages/74/61/2ecaf8abd68e275b9cc71530e18700fa8a15065d8fdd23b1cce8f5ac48e8/datapackage-pipelines-1.0.4.tar.gz"
      }
    ],
    "1.0.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1740390d5385d4a41e69fe0f70d06771",
          "sha256": "7ab653627f18fdcd6a4f8369ef9aa6867467eae162990d250b174cda8409cf8a"
        },
        "downloads": 19,
        "filename": "datapackage-pipelines-1.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "1740390d5385d4a41e69fe0f70d06771",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 219075,
        "upload_time": "2017-03-31T15:46:01",
        "url": "https://files.pythonhosted.org/packages/16/11/612c6edd74ef58021dc094ca20e23aae482066f414e666107e639a4c5b7b/datapackage-pipelines-1.0.6.tar.gz"
      }
    ],
    "1.0.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "74b7372cecfaf5cd021c574774a1f86c",
          "sha256": "736950b4ce3de325b78613e5ffef69c2e122a52fbc54cc18032da539d627165f"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "74b7372cecfaf5cd021c574774a1f86c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 220803,
        "upload_time": "2017-04-18T07:34:16",
        "url": "https://files.pythonhosted.org/packages/f8/4e/fe52f859c8ecfe8497248636b5bc6badd5e2ed46e45451b7bf7cd11bb400/datapackage-pipelines-1.0.7.tar.gz"
      }
    ],
    "1.0.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "95dd1eac6647529db1fc053ae5190a73",
          "sha256": "05e3e982cbc4aa897bbf4da7fbb9add239c0ad0572928e16c133e98f4c89e02d"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "95dd1eac6647529db1fc053ae5190a73",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 221087,
        "upload_time": "2017-04-20T14:43:08",
        "url": "https://files.pythonhosted.org/packages/d9/ce/a134ce25de40741a7f9da78fd0576d2727c3c02e5567c4b90b4c64c036a7/datapackage-pipelines-1.0.8.tar.gz"
      }
    ],
    "1.0.9": [
      {
        "comment_text": "",
        "digests": {
          "md5": "32c004eca98a1999f718f823ef42a3db",
          "sha256": "2f40ee5751824c0f5a27cf59ef19b11d9d680c0ced506f9f048702c7cfd8465b"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "32c004eca98a1999f718f823ef42a3db",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 221537,
        "upload_time": "2017-04-23T20:35:25",
        "url": "https://files.pythonhosted.org/packages/6e/db/8f621246ffc460e6ed8ec60780033dc65f77030a2f7059e3a9549e35d4cc/datapackage-pipelines-1.0.9.tar.gz"
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4a28a396640a98adc87cf33cc7b9518d",
          "sha256": "8accfb439e0adf8d2eb7facea9f07dddca6f8397b448a5d0139874768fc52f85"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4a28a396640a98adc87cf33cc7b9518d",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 84707,
        "upload_time": "2017-07-21T14:31:53",
        "url": "https://files.pythonhosted.org/packages/f5/4d/cd1f6968040c6d37dabb4e0942794416d9056cafcf1842767ccc29ba80ec/datapackage_pipelines-1.1.0-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "413bdb6bca3c39324ae3d570c599f785",
          "sha256": "a111a06aeab32f2a3c6b57477a4bcac54e35f5a400a9601f24ae6116179f2c26"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "413bdb6bca3c39324ae3d570c599f785",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 79682,
        "upload_time": "2017-07-21T14:31:54",
        "url": "https://files.pythonhosted.org/packages/38/c2/f358e5c05950d84b53cddd84f6bd769be26caae7fdb83a592a4a85163062/datapackage-pipelines-1.1.0.tar.gz"
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "303706e97f348fcf1ff5c60a1447d116",
          "sha256": "ebd5ef24c5a5a1398677bf3be35a6ce4244d33cb976ef36bacdbfed13751e04f"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "303706e97f348fcf1ff5c60a1447d116",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 85233,
        "upload_time": "2017-07-21T17:15:13",
        "url": "https://files.pythonhosted.org/packages/3e/15/1bea0663c264bc894df87f174092dd21753f8121dca57ba65e42727c4755/datapackage_pipelines-1.1.1-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "30ae7d9a7afca71da666136e9392f8b3",
          "sha256": "b7b1ddceaf805014ebcfc6095fe2271d113cc7b61eece9a875e71f0eb48242d7"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "30ae7d9a7afca71da666136e9392f8b3",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 309806,
        "upload_time": "2017-07-21T17:11:55",
        "url": "https://files.pythonhosted.org/packages/4b/fb/6f2feff97083f6123ff21f235450d432235abd17325e0bfaf117c3fb1d94/datapackage-pipelines-1.1.1.tar.gz"
      }
    ],
    "1.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "a1daf06cfb95cabd7311c59f6eaae52e",
          "sha256": "e1f2754785a4f3cd0dc67ce6224be68f600125226efb40a815e0a090b8bc5388"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a1daf06cfb95cabd7311c59f6eaae52e",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 86780,
        "upload_time": "2017-08-01T15:49:36",
        "url": "https://files.pythonhosted.org/packages/97/ac/144d1d67cdd5084d326fe3a0df87d1839dc8fddc640dc3456d5408a52943/datapackage_pipelines-1.1.2-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "6003a4087e713e175bce4007e24ed2b2",
          "sha256": "b83bff43511322494a47899962f488974f922f6c9d8c188d1c9bfb6efdb4763b"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "6003a4087e713e175bce4007e24ed2b2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 82088,
        "upload_time": "2017-08-01T15:49:38",
        "url": "https://files.pythonhosted.org/packages/fe/93/a1fb10b9c4068bba7f6334c6b23d12d7c4e04e4f46a70e41b62d08330854/datapackage-pipelines-1.1.2.tar.gz"
      }
    ],
    "1.1.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "c8fcf833a40d9bb247b6f7569e60851c",
          "sha256": "c8b042a7c88e53d20208530ab8197b96bb80c873a2206dc0264c41ce8c699b2c"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.1.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c8fcf833a40d9bb247b6f7569e60851c",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 86787,
        "upload_time": "2017-08-01T17:30:51",
        "url": "https://files.pythonhosted.org/packages/4e/47/37b1a695b8fedfbbdb31c1975f95dbba2f38cb40dda8691c9a5cb0492856/datapackage_pipelines-1.1.3-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "ce5b88c9dcb368929cb4939664d73364",
          "sha256": "300b430deafef7edd73f961fd4cee025ec25c899fc258037b7485a56fb5a2e84"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "ce5b88c9dcb368929cb4939664d73364",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 311420,
        "upload_time": "2017-08-01T17:28:08",
        "url": "https://files.pythonhosted.org/packages/fd/45/5098c22ef68996dcc83aa78e6ed037a03edac8ad96103540ea4d63654eb7/datapackage-pipelines-1.1.3.tar.gz"
      }
    ],
    "1.1.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4ea0eac25dd855fd8423fae0b5be77e6",
          "sha256": "a5b7fa4475a112f69ff35a79e7c51d7b5a3444fa1e78704250fd85928c47442f"
        },
        "downloads": 0,
        "filename": "datapackage_pipelines-1.1.5-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4ea0eac25dd855fd8423fae0b5be77e6",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 88266,
        "upload_time": "2017-08-07T09:14:49",
        "url": "https://files.pythonhosted.org/packages/b0/ae/453179f295f54545f92d272c8d5b0f358a6eed1c851d6d3e531da531f254/datapackage_pipelines-1.1.5-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "deb612e9cdb0f737b7c38d5855674cce",
          "sha256": "f7e66512128fa7ea2614df56558ffb482211bd81381f5fad807535b5cb658f03"
        },
        "downloads": 0,
        "filename": "datapackage-pipelines-1.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "deb612e9cdb0f737b7c38d5855674cce",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 83425,
        "upload_time": "2017-08-07T09:14:52",
        "url": "https://files.pythonhosted.org/packages/61/b3/d0763d6db0028e3f8e0d97ddbd74db2ee228927a523e26af9e0ba050112d/datapackage-pipelines-1.1.5.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "4ea0eac25dd855fd8423fae0b5be77e6",
        "sha256": "a5b7fa4475a112f69ff35a79e7c51d7b5a3444fa1e78704250fd85928c47442f"
      },
      "downloads": 0,
      "filename": "datapackage_pipelines-1.1.5-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4ea0eac25dd855fd8423fae0b5be77e6",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 88266,
      "upload_time": "2017-08-07T09:14:49",
      "url": "https://files.pythonhosted.org/packages/b0/ae/453179f295f54545f92d272c8d5b0f358a6eed1c851d6d3e531da531f254/datapackage_pipelines-1.1.5-py2.py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "deb612e9cdb0f737b7c38d5855674cce",
        "sha256": "f7e66512128fa7ea2614df56558ffb482211bd81381f5fad807535b5cb658f03"
      },
      "downloads": 0,
      "filename": "datapackage-pipelines-1.1.5.tar.gz",
      "has_sig": false,
      "md5_digest": "deb612e9cdb0f737b7c38d5855674cce",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 83425,
      "upload_time": "2017-08-07T09:14:52",
      "url": "https://files.pythonhosted.org/packages/61/b3/d0763d6db0028e3f8e0d97ddbd74db2ee228927a523e26af9e0ba050112d/datapackage-pipelines-1.1.5.tar.gz"
    }
  ]
}