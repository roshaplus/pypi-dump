{
  "info": {
    "author": "DecisionVis LLC",
    "author_email": "team@decisionvis.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering"
    ],
    "description": "\u03b4MOEA: Multi-Objective Grid Search Algorithm\n============================================\n\n(C) 2017 `DecisionVis LLC <http://www.decisionvis.com>`__\n\nExecutive Summary\n-----------------\n\n\u03b4MOEA is an optimization library that helps people make better decisions\nusing computer models of a problem domain. \u03b4MOEA searches the model\ninputs for combinations that produce optimal model outputs with respect\nto multiple objectives. We call \u03b4MOEA a \"Grid Search\" algorithm because\nit samples the model inputs on a grid rather than attempting to optimize\ncontinuous values.\n\nThis version of \u03b4MOEA is written in Python.\n\nThe Python version of \u03b4MOEA carries the 3-clause BSD license. See\n``COPYING.md`` for details.\n\nGetting Started With \u03b4MOEA\n--------------------------\n\n1. Read the rest of this document for an overview of what \u03b4MOEA does.\n2. Read ```doc/getting_started.md`` <doc/getting_started.md>`__ for a\n   quick guide to downloading and using \u03b4MOEA.\n3. Refer to ```doc/api.md`` <doc/api.md>`__ for documentation on \u03b4MOEA's\n   library functions.\n4. Examine the examples in ```examples/`` <examples>`__ for more ways to\n   use \u03b4MOEA.\n5. Read the documents under ```doc/technotes/`` <doc/technotes>`__ for\n   the design rationale.\n\nComputer Models\n---------------\n\nComputer models of a problem domain express value judgments about it. If\nnothing else, what the author has chosen to model is a statement about\nwhat is important. Deciding to use optimization, however, often implies\nan extra layer of value judgment on top of the domain model. We call\nthis extra layer the \"optimization model\" to distinguish it from the\ndomain model. We call the inputs to the optimization model \"decisions\"\nand the outputs \"objectives\", \"constraints\", and \"tagalongs\".\n\n.. figure:: doc/img/model.svg\n   :alt: Figure 1: Optimization model and domain model.\n\n   Figure 1: Optimization model and domain model.\n\nFigure 1 shows how an optimization model wraps a domain model.\n\n-  *Decisions* are translated by the optimization model into inputs for\n   the domain model.\n-  Some domain model inputs may be held constant and not subjected to\n   optimization.\n-  Some domain model outputs are translated into *objectives* for\n   optimization. Objectives are numbers that \u03b4MOEA will attempt to\n   minimize or maximize.\n-  Other domain model outputs are translated into *constraints* for\n   optimization. Constraints are numbers that \u03b4MOEA will preemptively\n   try to drive to zero, before considering the objectives.\n-  Still other domain model outputs are captured as *tagalongs*.\n   Tagalongs have no role in optimization but may be of use for\n   decision-making. In addition, tagalongs may preserve domain model\n   inputs and outputs so that old domain model evaluations may be reused\n   with a new optimization model.\n-  Some domain model outputs may be ignored and discarded by the\n   optimization model.\n\nMulti-Objective Optimization\n----------------------------\n\nMulti-Objective Optimization differs from conventional\n(single-objective) optimization in that it seeks to approximate a\n\"Pareto Set\" representing the tradeoffs among multiple objectives,\nrather than to approximate a single optimal value. Figure 2 illustrates\nthe difference between the progress of single-objective and\nmulti-objective optimization.\n\n.. figure:: doc/img/multiobjective.svg\n   :alt: Figure 2: Single objective versus multi-objective optimization.\n\n   Figure 2: Single objective versus multi-objective optimization.\n\nUnder single-objective optimization, the objective value is continually\nimproved over time (minimized in this example). This is the top plot in\nFigure 2. Multi-objective optimization, on the other hand, attempts to\noptimize two or more objectives at once (minimizing two objectives, in\nthis example.) The bottom left plot shows both objectives over time.\nRather than a single value, both objectives develop a range of values\nthat trade off against each other. The final tradeoff is shown in the\nbottom right plot.\n\n.. figure:: doc/img/animated_optimization.gif\n   :alt: Figure 3: Animated optimization\n\n   Figure 3: Animated optimization\n\nFigure 3 is an animated version of Figure 2 that illustrates the\nprogress of the optimization runs over time, as both optimization runs\nconverge towards approximations of the optimal values.\n\nWhy \u03b4MOEA?\n----------\n\nOptimization is a powerful tool for understanding a problem. It pushes a\ndomain model to its limits and identifies gaps in our conception of a\nproblem. Multi-objective optimization is particularly powerful: compared\nto single-objective optimization, adding even one more objective allows\nyou to make much more nuanced decisions and avoid blowing past the point\nof diminishing returns. (See Figures 2 and 3.)\n\nAlso, compared to not doing optimization at all, and simply sampling the\nentire decision space on a grid, \u03b4MOEA saves a vast amount of computer\ntime. As an optimization algorithm, \u03b4MOEA focuses its sampling on the\n*interesting* part of the decision space, where interest is defined by\nthe user in terms of objectives and constraints.\n\nCompared with other multi-objective optimization algorithms, \u03b4MOEA\nscales well to large numbers of objectives (it has been tested up to 20\nobjectives) and makes more efficient use of expensive computational\nresources. Furthermore, \u03b4MOEA has been designed to integrate with\nexisting parallel evaluation approaches -- it is readily parallelized\nbut does not impose any one approach on its users. \u03b4MOEA will work with\nanything from Python's ``multiprocessing`` library, to MPI, to homegrown\n\u00d8MQ job queues.\n\nIts design also makes \u03b4MOEA easy to use as a library rather than as an\napplication. Where most optimization routines want to take control of\nyour model, \u03b4MOEA decouples sampling and evaluation to put the user in\ncontrol. This makes it possible to embed \u03b4MOEA in model code rather than\nshoehorning model code into an optimization program.\n\nFinally, because \u03b4MOEA is already grid-oriented, it will avoid a great\ndeal of unnecessary sampling on mixed-integer problems compared to other\nMOEAs.\n\nAbout The Name\n--------------\n\n\u03b4MOEA uses an evolutionary optimization heuristic to improve its Pareto\napproximation. This is the origin of the \"MOEA\" in its name: it is a\nMulti-Objective Evolutionary Algorithm. The \u03b4 alludes to the sampling\ngrid in the decision space, where \u03b4 is the grid spacing. The name \u03b4MOEA\nalso pays homage to Deb et al.'s \u03b5MOEA, an influential algorithm that\napplies a grid on the objective space rather than the decision space.\n\nPython Compatibility\n--------------------\n\n\u03b4MOEA is compatible with both Python 2.7.13 (or later) and Python 3.6\n(or later). It is likely compatible with earlier versions of Python 2.7\nand Python 3, but that has not been tested. Furthermore, while\nperformance between Python versions is equivalent, results are similar\nrather than identical due to differences in their random number\ngenerators.\n\nIn addition, \u03b4MOEA has no library dependencies beyond the Python\nstandard library, so it should work on any platform and with any\ninterpreter. (Reports of incompatibility are encouraged.)\n\nOpen Source\n-----------\n\n\u03b4MOEA is open source because we believe in multi-objective optimization\nand we want people to use it. As a business, DecisionVis LLC has found\nthat licensing MOEAs gets in the way of consulting relationships and\nproduces minimal revenue. The degree to which MOEAs need to integrate\nwith domain models and parallelization environments also makes\nclosed-source releases unreasonably expensive to support and forces us\nto spend time fighting uninteresting integration problems. So we decided\nto develop and release an open source MOEA to let us work with our\ncustomers on interesting problems instead.\n\nWhat Next?\n----------\n\nRefer to ```doc/getting_started.md`` <doc/getting_started.md>`__ for an\noverview of how to get and use \u03b4MOEA.\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://bitbucket.org/decisionvis/dmoea-python",
    "keywords": "\u03b4MOEA,MOEA,grid search,optimization,multi-objective",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "deltamoea",
    "platform": "",
    "project_url": "https://pypi.org/project/deltamoea/",
    "release_url": "https://pypi.org/project/deltamoea/1.4/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "Multi-Objective Grid Search Algorithm",
    "version": "1.4"
  },
  "releases": {
    "1.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1bbd7bfe9de3e61b801ba680d669ade5",
          "sha256": "256fffe53b151afde2a797ca7c3877a88915a9f82636cc0a75255f3a70b303ac"
        },
        "downloads": 0,
        "filename": "deltamoea-1.4-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1bbd7bfe9de3e61b801ba680d669ade5",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 48114,
        "upload_time": "2017-07-13T17:33:25",
        "url": "https://files.pythonhosted.org/packages/40/2a/a8e654058318f0dbf65a8cee8ca5af6311bc515a85c14dbe1c06ab15763d/deltamoea-1.4-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "8d2196e4201de449d62b9c77159c2385",
          "sha256": "42a4544d34a4c314afbf41d0a3f7c7d10a8d73c23a3f165363ad08d01926a722"
        },
        "downloads": 0,
        "filename": "deltamoea-1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "8d2196e4201de449d62b9c77159c2385",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21530,
        "upload_time": "2017-07-13T17:33:27",
        "url": "https://files.pythonhosted.org/packages/3b/f2/46bd26029f006b3d10dddc49f3e211ed21f9cd7a48aa52d77f0f5ade7127/deltamoea-1.4.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "1bbd7bfe9de3e61b801ba680d669ade5",
        "sha256": "256fffe53b151afde2a797ca7c3877a88915a9f82636cc0a75255f3a70b303ac"
      },
      "downloads": 0,
      "filename": "deltamoea-1.4-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1bbd7bfe9de3e61b801ba680d669ade5",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 48114,
      "upload_time": "2017-07-13T17:33:25",
      "url": "https://files.pythonhosted.org/packages/40/2a/a8e654058318f0dbf65a8cee8ca5af6311bc515a85c14dbe1c06ab15763d/deltamoea-1.4-py2.py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "8d2196e4201de449d62b9c77159c2385",
        "sha256": "42a4544d34a4c314afbf41d0a3f7c7d10a8d73c23a3f165363ad08d01926a722"
      },
      "downloads": 0,
      "filename": "deltamoea-1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "8d2196e4201de449d62b9c77159c2385",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 21530,
      "upload_time": "2017-07-13T17:33:27",
      "url": "https://files.pythonhosted.org/packages/3b/f2/46bd26029f006b3d10dddc49f3e211ed21f9cd7a48aa52d77f0f5ade7127/deltamoea-1.4.tar.gz"
    }
  ]
}