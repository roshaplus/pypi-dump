{
  "info": {
    "author": "Nuncjo",
    "author_email": "zoreander@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "Delver\n========================\n\nProgrammatic web browser/crawler in Python. **Alternative to Mechanize, RoboBrowser, MechanicalSoup**\nand others. Strict power of Request and Lxml. Some features and methods usefull in scraping \"out of the box\".\n\n- [Basic examples](#basic-examples)\n    - [Form submit](#form-submit)\n    - [Find links narrowed by filters](#find-links-narrowed-by-filters)\n    - [Download file](#download-file)\n    - [Download files list in parallel](#download-files-list-in-parallel)\n    - [Xpath selectors](#xpath-selectors)\n    - [Css selectors](#css-selectors)\n    - [Xpath result with filters](#xpath-result-with-filters)\n- [Use examples](#use-examples)\n    - [Scraping Steam Specials using XPath](#scraping-steam-specials-using-xpath)\n    - [Simple tables scraping out of the box](#simple-tables-scraping-out-of-the-box)\n    - [User login](#user-login)\n    - [One Punch Man Downloader](#one-punch-man-downloader)\n\n- - -\n\n## Basic examples\n\n\n## Form submit\n\n```python\n\n        >>> from delver import Crawler\n        >>> c = Crawler()\n        >>> response = c.open('https://httpbin.org/forms/post')\n        >>> forms = c.forms()\n\n        # Filling up fields values:\n        >>> form = forms[0]\n        >>> form.fields = {\n        ...    'custname': 'Ruben Rybnik',\n        ...    'custemail': 'ruben.rybnik@fakemail.com',\n        ...    'size': 'medium',\n        ...    'topping': ['bacon', 'cheese'],\n        ...    'custtel': '+48606505888'\n        ... }\n        >>> submit_result = c.submit(form)\n        >>> submit_result.status_code\n        200\n\n        # Checking if form post ended with success:\n        >>> c.submit_check(\n        ...    form,\n        ...    phrase=\"Ruben Rybnik\",\n        ...    url='https://httpbin.org/forms/post',\n        ...    status_codes=[200]\n        ... )\n        True\n```\n\n## Find links narrowed by filters\n\n```python\n\n        >>> c = Crawler()\n        >>> c.open('https://httpbin.org/links/10/0')\n        <Response [200]>\n\n        # Links can be filtered by some html tags and filters\n        # like: id, text, title and class:\n        >>> links = c.links(\n        ...     tags = ('style', 'link', 'script', 'a'),\n        ...     filters = {\n        ...         'text': '7'\n        ...     },\n        ...     match='NOT_EQUAL'\n        ... )\n        >>> len(links)\n        8\n```\n\n## Download file\n\n```python\n\n        >>> import os\n\n        >>> c = Crawler()\n        >>> local_file_path = c.download(\n        ...     local_path='test',\n        ...     url='https://httpbin.org/image/png',\n        ...     name='test.png'\n        ... )\n        >>> os.path.isfile(local_file_path)\n        True\n```\n\n## Download files list in parallel\n\n```python\n\n        >>> c = Crawler()\n        >>> c.open('https://xkcd.com/')\n        <Response [200]>\n        >>> full_images_urls = [c.join_url(src) for src in c.images()]\n        >>> downloaded_files = c.download_files('test', files=full_images_urls)\n        >>> len(full_images_urls) == len(downloaded_files)\n        True\n```\n\n## Xpath selectors\n\n```python\n\n        c = Crawler()\n        c.open('https://httpbin.org/html')\n        p_text = c.xpath('//p/text()')\n```\n\n## Css selectors\n\n```python\n\n        c = Crawler()\n        c.open('https://httpbin.org/html')\n        p_text = c.css('div')\n```\n\n## Xpath result with filters\n\n```python\n\n        c = Crawler()\n        c.open('https://www.w3schools.com/')\n        filtered_results = c.xpath('//p').filter(filters={'class': 'w3-xlarge'})\n```\n\n## Using retries\n\n```python\n\n        c = Crawler()\n        # sets max_retries to 2 means that after there will be max two attempts to open url\n        # if first attempt will fail, wait 1 second and try again, second attempt wait 2 seconds\n        # and then try again\n        c.max_retries = 2\n        c.open('http://www.delver.cg/404')\n```\n\n## Use examples\n\n\n## Scraping Steam Specials using XPath\n\n```python\n\n    from pprint import pprint\n    from delver import Crawler\n\n    c = Crawler(absolute_links=True)\n    c.logging = True\n    c.useragent = \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n    c.random_timeout = (0, 5)\n    c.open('http://store.steampowered.com/search/?specials=1')\n    titles, discounts, final_prices = [], [], []\n\n\n    while c.links(filters={\n        'class': 'pagebtn',\n        'text': '>'\n    }):\n        c.open(c.current_results[0])\n        titles.extend(\n            c.xpath(\"//div/span[@class='title']/text()\")\n        )\n        discounts.extend(\n            c.xpath(\"//div[contains(@class, 'search_discount')]/span/text()\")\n        )\n        final_prices.extend(\n            c.xpath(\"//div[contains(@class, 'discounted')]//text()[2]\").strip()\n        )\n\n    all_results = {\n        row[0]: {\n            'discount': row[1],\n            'final_price': row[2]\n        } for row in zip(titles, discounts, final_prices)}\n    pprint(all_results)\n```\n\n## Simple tables scraping out of the box\n\n```python\n\n    from pprint import pprint\n    from delver import Crawler\n\n    c = Crawler(absolute_links=True)\n    c.logging = True\n    c.useragent = \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n    c.open(\"http://www.boxofficemojo.com/daily/\")\n    pprint(c.tables())\n```\n\n## User login\n\n```python\n\n\n    from delver import Crawler\n\n    c = Crawler()\n    c.useragent = (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n        \"Chrome/60.0.3112.90 Safari/537.36\"\n    )\n    c.random_timeout = (0, 5)\n    c.open('http://testing-ground.scraping.pro/login')\n    forms = c.forms()\n    if forms:\n        login_form = forms[0]\n        login_form.fields = {\n            'usr': 'admin',\n            'pwd': '12345'\n        }\n        c.submit(login_form)\n        success_check = c.submit_check(\n            login_form,\n            phrase='WELCOME :)',\n            status_codes=[200]\n        )\n        print(success_check)\n```\n\n## One Punch Man Downloader\n\n```python\n\n    import os\n    from delver import Crawler\n\n    class OnePunchManDownloader:\n        \"\"\"Downloads One Punch Man free manga chapers to local directories.\n        Uses one main thread for scraper with random timeout.\n        Uses 20 threads just for image downloads.\n        \"\"\"\n        def __init__(self):\n            self._target_directory = 'one_punch_man'\n            self._start_url = \"http://m.mangafox.me/manga/onepunch_man_one/\"\n            self.crawler = Crawler()\n            self.crawler.random_timeout = (0, 5)\n            self.crawler.useragent = \"Googlebot-Image/1.0\"\n\n        def run(self):\n            self.crawler.open(self._start_url)\n            for link in self.crawler.links(filters={'text': 'Ch '}, match='IN'):\n                self.download_images(link)\n\n        def download_images(self, link):\n            target_path = '{}/{}'.format(self._target_directory, link.split('/')[-2])\n            full_chapter_url = link.replace('/manga/', '/roll_manga/')\n            self.crawler.open(full_chapter_url)\n            images = self.crawler.xpath(\"//img[@class='reader-page']/@data-original\")\n            os.makedirs(target_path, exist_ok=True)\n            self.crawler.download_files(target_path, files=images, workers=20)\n\n\n    downloader = OnePunchManDownloader()\n    downloader.run()\n```\n\n\n=======\nHistory\n=======\n\n0.1.1 (2017-09-25)\n------------------\n\n* First release on PyPI.\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/nuncjo/delver",
    "keywords": "delver",
    "license": "MIT license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "delver",
    "platform": "",
    "project_url": "https://pypi.org/project/delver/",
    "release_url": "https://pypi.org/project/delver/0.1.2/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "Modern user friendly web automation and scraping library.",
    "version": "0.1.2"
  },
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "51cec95b6c0f85e581de55b0b4041f34",
          "sha256": "1d678922a95dd27ec571dea23ca0ef9cabfaf093a3b823eb34e739cf2f809b99"
        },
        "downloads": 0,
        "filename": "delver-0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "51cec95b6c0f85e581de55b0b4041f34",
        "packagetype": "bdist_wheel",
        "python_version": "3.6",
        "size": 25180,
        "upload_time": "2017-10-02T21:39:03",
        "url": "https://files.pythonhosted.org/packages/92/8d/d310ecf387436a121e4b462b7778f2e240fa6cd046bcaef49c06b6beaa70/delver-0.1.2-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "e8354497d45a30c09b2efc322599a0c2",
          "sha256": "76488b47b14f0acdbe13da84358d300221a4840d5e87bb99d919789f230a700d"
        },
        "downloads": 0,
        "filename": "delver-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e8354497d45a30c09b2efc322599a0c2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 32736,
        "upload_time": "2017-10-02T21:38:34",
        "url": "https://files.pythonhosted.org/packages/98/3f/56fd5a67373105699c90f157e1468a57d387f38c1c126a5758860e446c69/delver-0.1.2.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "51cec95b6c0f85e581de55b0b4041f34",
        "sha256": "1d678922a95dd27ec571dea23ca0ef9cabfaf093a3b823eb34e739cf2f809b99"
      },
      "downloads": 0,
      "filename": "delver-0.1.2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "51cec95b6c0f85e581de55b0b4041f34",
      "packagetype": "bdist_wheel",
      "python_version": "3.6",
      "size": 25180,
      "upload_time": "2017-10-02T21:39:03",
      "url": "https://files.pythonhosted.org/packages/92/8d/d310ecf387436a121e4b462b7778f2e240fa6cd046bcaef49c06b6beaa70/delver-0.1.2-py2.py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "e8354497d45a30c09b2efc322599a0c2",
        "sha256": "76488b47b14f0acdbe13da84358d300221a4840d5e87bb99d919789f230a700d"
      },
      "downloads": 0,
      "filename": "delver-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "e8354497d45a30c09b2efc322599a0c2",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 32736,
      "upload_time": "2017-10-02T21:38:34",
      "url": "https://files.pythonhosted.org/packages/98/3f/56fd5a67373105699c90f157e1468a57d387f38c1c126a5758860e446c69/delver-0.1.2.tar.gz"
    }
  ]
}