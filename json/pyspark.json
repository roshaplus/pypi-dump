{
  "info": {
    "author": "Spark Developers",
    "author_email": "dev@spark.apache.org",
    "bugtrack_url": "",
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "Apache Spark\r\n============\r\n\r\nSpark is a fast and general cluster computing system for Big Data. It\r\nprovides high-level APIs in Scala, Java, Python, and R, and an optimized\r\nengine that supports general computation graphs for data analysis. It\r\nalso supports a rich set of higher-level tools including Spark SQL for\r\nSQL and DataFrames, MLlib for machine learning, GraphX for graph\r\nprocessing, and Spark Streaming for stream processing.\r\n\r\nhttp://spark.apache.org/\r\n\r\nOnline Documentation\r\n--------------------\r\n\r\nYou can find the latest Spark documentation, including a programming\r\nguide, on the `project web\r\npage <http://spark.apache.org/documentation.html>`__\r\n\r\nPython Packaging\r\n----------------\r\n\r\nThis README file only contains basic information related to pip\r\ninstalled PySpark. This packaging is currently experimental and may\r\nchange in future versions (although we will do our best to keep\r\ncompatibility). Using PySpark requires the Spark JARs, and if you are\r\nbuilding this from source please see the builder instructions at\r\n`\"Building\r\nSpark\" <http://spark.apache.org/docs/latest/building-spark.html>`__.\r\n\r\nThe Python packaging for Spark is not intended to replace all of the\r\nother use cases. This Python packaged version of Spark is suitable for\r\ninteracting with an existing cluster (be it Spark standalone, YARN, or\r\nMesos) - but does not contain the tools required to setup your own\r\nstandalone Spark cluster. You can download the full version of Spark\r\nfrom the `Apache Spark downloads\r\npage <http://spark.apache.org/downloads.html>`__.\r\n\r\n**NOTE:** If you are using this with a Spark standalone cluster you must\r\nensure that the version (including minor version) matches or you may\r\nexperience odd errors.\r\n\r\nPython Requirements\r\n-------------------\r\n\r\nAt its core PySpark depends on Py4J (currently version 0.10.4), but\r\nadditional sub-packages have their own requirements (including numpy and\r\npandas).",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/apache/spark/tree/master/python",
    "keywords": "",
    "license": "http://www.apache.org/licenses/LICENSE-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyspark",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/pyspark/",
    "release_url": "https://pypi.org/project/pyspark/2.1.1/",
    "requires_python": null,
    "summary": "Apache Spark Python API",
    "version": "2.1.1"
  },
  "releases": {
    "2.1.1": []
  },
  "urls": []
}