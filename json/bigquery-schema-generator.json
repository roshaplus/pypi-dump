{
  "info": {
    "author": "Brian T. Park",
    "author_email": "brian@xparks.net",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "BigQuery Schema Generator\n=========================\n\nSummary\n-------\n\nThis script generates the BigQuery schema from the data records on the\nSTDIN. The BigQuery data importer uses only the first 100 lines when the\nschema auto-detection feature is enabled. In contrast, this script uses\nall data records to generate the schema.\n\nUsage:\n\n::\n\n    $ generate-schema < file.data.json > file.schema.json\n\nBackground\n----------\n\nData can be imported into\n`BigQuery <https://cloud.google.com/bigquery/>`__ using the\n`bq <https://cloud.google.com/bigquery/bq-command-line-tool>`__ command\nline tool. It accepts a number of data formats including CSV or\nnewline-delimited JSON. The data can be loaded into an existing table or\na new table can be created during the loading process. The structure of\nthe table is defined by its\n`schema <https://cloud.google.com/bigquery/docs/schemas>`__. The table's\nschema can be defined manually or the schema can be\n`auto-detected <https://cloud.google.com/bigquery/docs/schema-detect#auto-detect>`__.\n\nWhen the auto-detect feature is used, the BigQuery data importer\nexamines only the first 100 records of the input data. In many cases,\nthis is sufficient because the data records were dumped from another\ndatabase and the exact schema of the source table was known. However,\nfor data extracted from a service (e.g. using a REST API) the record\nfields could have been organically added at later dates. In this case,\nthe first 100 records do not contain fields which are present in later\nrecords. The **bq load** auto-detection fails and the data fails to\nload.\n\nThe **bq load** tool does not support the ability to process the entire\ndataset to determine a more accurate schema. This script fills in that\ngap. It processes the entire dataset given in the STDIN and outputs the\nBigQuery schema in JSON format on the STDOUT. This schema file can be\nfed back into the **bq load** tool to create a table that is more\ncompatible with the data fields in the input dataset.\n\nInstallation\n------------\n\nInstall from `PyPI <https://pypi.python.org/pypi>`__ repository using:\n\n::\n\n    $ pip3 install bigquery_schema_generator\n\nUsage\n-----\n\nThe ``generate_schema.py`` script accepts a newline-delimited JSON data\nfile on the STDIN. (CSV is not supported currently.) It scans every\nrecord in the input data file to deduce the table's schema. It prints\nthe JSON formatted schema file on the STDOUT. There are at least 3 ways\nto run this script:\n\n1) If you installed using ``pip3``, then it should have installed a\n   small helper script named ``generate-schema`` in your local ``./bin``\n   directory of your current environment (depending on whether you are\n   using a virtual environment).\n\n::\n\n    $ generate-schema < file.data.json > file.schema.json\n\n2) You can invoke the module directly using:\n\n   ::\n\n       $ python3 -m bigquery_schema_generator.generate_schema < file.data.json > file.schema.json\n\n   This is essentially what the ``generate-schema`` command does.\n\n3) If you retrieved this code from its `GitHub\n   repository <https://github.com/bxparks/bigquery-schema-generator>`__,\n   then you can invoke the Python script directly:\n\n   ::\n\n       $ ./generate_schema.py < file.data.json > file.schema.json\n\nSchema Output\n~~~~~~~~~~~~~\n\nThe resulting schema file can be used in the **bq load** command using\nthe ``--schema`` flag:\n\n::\n\n    $ bq load --source_format NEWLINE_DELIMITED_JSON \\\n            --schema file.schema.json \\\n            mydataset.mytable \\\n            file.data.json\n\nwhere ``mydataset.mytable`` is the target table in BigQuery.\n\nA useful flag for **bq load** is ``--ignore_unknown_values``, which\ncauses **bq load** to ignore fields in the input data which are not\ndefined in the schema. When ``generate_schema.py`` detects an\ninconsistency in the definition of a particular field in the input data,\nit removes the field from the schema definition. Without the\n``--ignore_unknown_values``, the **bq load** fails when the inconsistent\ndata record is read.\n\nAfter the BigQuery table is loaded, the schema can be retrieved using:\n\n::\n\n    $ bq show --schema mydataset.mytable | python -m json.tool\n\n(The ``python -m json.tool`` command will pretty-print the JSON\nformatted schema file.) This schema file should be identical to\n``file.schema.json``.\n\nOptions\n~~~~~~~\n\nThe ``generate_schema.py`` script supports a handful of command line\nflags:\n\n-  ``--help`` Prints the usage with the list of supported flags.\n-  ``--keep_nulls`` Print the schema for null values, empty arrays or\n   empty records.\n-  ``--debugging_interval lines`` Number of lines between heartbeat\n   debugging messages. Default 1000.\n-  ``--debugging_map`` Print the metadata schema map for debugging\n   purposes\n\nHelp\n^^^^\n\nPrint the built-in help strings:\n\n::\n\n    $ generate-schema --help\n\nNull Values\n^^^^^^^^^^^\n\nNormally when the input data file contains a field which has a null,\nempty array or empty record as its value, the field is suppressed in the\nschema file. This flag enables this field to be included in the schema\nfile. In other words, for the data file:\n\n::\n\n    { \"s\": null, \"a\": [], \"m\": {} }\n\nthe schema would normally be:\n\n::\n\n    []\n\nWith the ``keep_nulls``, the resulting schema file will be:\n\n::\n\n    [\n      {\n        \"mode\": \"REPEATED\",\n        \"type\": \"STRING\",\n        \"name\": \"a\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"fields\": [\n          {\n            \"mode\": \"NULLABLE\",\n            \"type\": \"STRING\",\n            \"name\": \"__unknown__\"\n          }\n        ],\n        \"type\": \"RECORD\",\n        \"name\": \"d\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"type\": \"STRING\",\n        \"name\": \"s\"\n      }\n    ]\n\nExample:\n\n::\n\n    $ generate-schema --keep_nulls < file.data.json > file.schema.json\n\nDebugging Interval\n^^^^^^^^^^^^^^^^^^\n\nBy default, the ``generate_schema.py`` script prints a short progress\nmessage every 1000 lines of input data. This interval can be changed\nusing the ``--debugging_interval`` flag.\n\n::\n\n    $ generate-schema --debugging_interval 1000 < file.data.json > file.schema.json\n\nDebugging Map\n^^^^^^^^^^^^^\n\nInstead of printing out the BigQuery schema, the ``--debugging_map``\nprints out the bookkeeping metadata map which is used internally to keep\ntrack of the various fields and theirs types that was inferred using the\ndata file. This flag is intended to be used for debugging.\n\n::\n\n    $ generate-schema --debugging_map < file.data.json > file.schema.json\n\nExamples\n--------\n\nHere is an example of a single JSON data record on the STDIN:\n\n::\n\n    $ generate-schema\n    { \"s\": \"string\", \"b\": true, \"i\": 1, \"x\": 3.1, \"t\": \"2017-05-22T17:10:00-07:00\" }\n    ^D\n    INFO:root:Processed 1 lines\n    [\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"b\",\n        \"type\": \"BOOLEAN\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"i\",\n        \"type\": \"INTEGER\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"s\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"t\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"x\",\n        \"type\": \"FLOAT\"\n      }\n    ]\n\nIn most cases, the data file will be stored in a file:\n\n::\n\n    cat > file.data.json\n    { \"a\": [1, 2] }\n    { \"i\": 3 }\n    ^D\n\n    $ generate-schema < file.data.json > file.schema.json\n    INFO:root:Processed 2 lines\n\n    $ cat file.schema.json\n    [\n      {\n        \"mode\": \"REPEATED\",\n        \"name\": \"a\",\n        \"type\": \"INTEGER\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"i\",\n        \"type\": \"INTEGER\"\n      }\n    ]\n\nSystem Requirements\n-------------------\n\nThis project was developed on Ubuntu 17.04 using Python 3.5. It is\nlikely compatible with other Python environments but I have not yet\nverified those.\n\nAuthor\n------\n\nCreated by Brian T. Park (brian@xparks.net).\n\nLicense\n-------\n\nApache License 2.0",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/bxparks/bigquery-schema-generator",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bigquery-schema-generator",
    "platform": "",
    "project_url": "https://pypi.org/project/bigquery-schema-generator/",
    "release_url": "https://pypi.org/project/bigquery-schema-generator/0.1.2/",
    "requires_dist": [],
    "requires_python": "~=3.5",
    "summary": "BigQuery schema generator",
    "version": "0.1.2"
  },
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "24cf9cc9d0c5f9f043cb304612367bc2",
          "sha256": "f72de9d303d89997b7d83f1ecc046ff7d089a76f8ce864e55d80336c1c296eba"
        },
        "downloads": -1,
        "filename": "bigquery-schema-generator-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "24cf9cc9d0c5f9f043cb304612367bc2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8142,
        "upload_time": "2018-01-02T21:23:28",
        "url": "https://files.pythonhosted.org/packages/e1/db/d9a03a7e36a708489a655effd59a3cb5fcc13f8dd2aba74a319d65d7e03a/bigquery-schema-generator-0.1.tar.gz"
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4c1ab0d67aa70ab53cbe32fc757113fc",
          "sha256": "12174eac7b354136d0aede4845b557116c4a7d6d64d4276f1377219a085ccd0c"
        },
        "downloads": -1,
        "filename": "bigquery-schema-generator-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4c1ab0d67aa70ab53cbe32fc757113fc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8614,
        "upload_time": "2018-01-03T22:39:39",
        "url": "https://files.pythonhosted.org/packages/34/1b/cae35741da8bbd894f78cef64157b091be6477475cf8d78785031f5e1a20/bigquery-schema-generator-0.1.1.tar.gz"
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0ce0f074da4741cb365a3c3dba2bc4a0",
          "sha256": "4def9de61d384654948a31d80a757fea205038933a36575e773b9fa4b30a34ec"
        },
        "downloads": -1,
        "filename": "bigquery-schema-generator-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "0ce0f074da4741cb365a3c3dba2bc4a0",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8955,
        "upload_time": "2018-01-04T22:34:23",
        "url": "https://files.pythonhosted.org/packages/d7/82/976a46999464e60371ba72cdb89bd8609e071104208e9196d6e2ae9a4b09/bigquery-schema-generator-0.1.2.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "0ce0f074da4741cb365a3c3dba2bc4a0",
        "sha256": "4def9de61d384654948a31d80a757fea205038933a36575e773b9fa4b30a34ec"
      },
      "downloads": -1,
      "filename": "bigquery-schema-generator-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "0ce0f074da4741cb365a3c3dba2bc4a0",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 8955,
      "upload_time": "2018-01-04T22:34:23",
      "url": "https://files.pythonhosted.org/packages/d7/82/976a46999464e60371ba72cdb89bd8609e071104208e9196d6e2ae9a4b09/bigquery-schema-generator-0.1.2.tar.gz"
    }
  ]
}