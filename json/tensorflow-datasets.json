{
  "info": {
    "author": "Stefan Webb",
    "author_email": "stefan.webb@eng.ox.ac.uk",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.5",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "tensorflow-datasets\n===================\nThis library contains scripts to download common datasets, convert to the ``.tfrecords`` format, and use in your TensorFlow graph with a queue runner. \n\nThe TensorFlow authors recommend `using queues to read data\n<https://www.tensorflow.org/performance/performance_guide#utilize_queues_for_reading_data>`_ for maximal performance, and strongly recommend against feeding in data from Python variables. This also permits a uniform data input interface. We have created a library to automate getting common datasets into TensorFlow in the right format for queue runners so that you can easily do this with a single function call.\n\nInstallation\n------------\n**Developer Install**\n\n.. code-block:: bash\n\n  git clone https://github.com/stefanwebb/tensorflow-datasets.git\n  cd tensorflow-datasets\n  python setup.py develop\n\n**User Install**\n\n.. code-block:: bash\n\n  pip install --user tensorflow-datasets\n\nInstructions\n------------\n1. As above, either install a developer version (which allows you to make changes to the library), or perform an easy Pip package installation\n2. Optionally, set the environment variable ``TF_DATA`` to a folder where you have read and write permissions. The data will be downloaded, processed, and accessed from here by default.\n3. Include the package in your program, and create a data node in your TensorFlow graph. You need to start the queue runners otherwise your program will hang. `Here\n<https://github.com/stefanwebb/tensorflow-datasets/blob/master/examples/minimal.py>`_ is a minimal example.\n\nDatasets\n--------\nEach supported dataset has its own module under the package folder containing a settings dictionary and a function to process the raw data into a NumPy array. You can browse them `here\n<https://github.com/stefanwebb/tensorflow-datasets/tree/master/tensorflow_datasets>`_.\n\nThe method ``tf_data.inputs()`` creates the node in your TensorFlow graph to read a minibatch of the data, and downloads and processes the dataset when it detects the required ``.tfrecords`` is missing. You can use a data folder other than that from the environment variable ``TF_DATA`` using the path option.\n\nTODO\n----\n1. Detailed .rst description in each datasets folder\n2. The CelebA and Frey Faces datasets\n3. More transformations such as one-hot encodings, random crops, etc. and instructions on these\n4. Common datasets for sequential NNs\n5. A complementary package that abstracts model creation and running, combining inputs, model, inference, and optimization classes. Implementation of common models\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/stefanwebb/tensorflow-datasets",
    "keywords": "tensorflow dataset library mnist svhn omniglot cifar-10 cifar-100",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tensorflow-datasets",
    "platform": "",
    "project_url": "https://pypi.org/project/tensorflow-datasets/",
    "release_url": "https://pypi.org/project/tensorflow-datasets/0.1.0/",
    "requires_python": "",
    "summary": "Scripts to download common datasets, convert to the .tfrecords format, and use in your TensorFlow graph with a queue runner",
    "version": "0.1.0"
  },
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0a188913b8e6f2a2ce5126c195bf806b",
          "sha256": "eebcf85f74dcb1a5c873f7fd01833b74746fc4df3f576718b8d5867afaed46c5"
        },
        "downloads": 43,
        "filename": "tensorflow_datasets-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0a188913b8e6f2a2ce5126c195bf806b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 25125,
        "upload_time": "2017-03-13T21:15:33",
        "url": "https://files.pythonhosted.org/packages/f0/2a/8c35a879abe68643c706f6e9c63057c03143b39c9f46e09184237dea4a39/tensorflow_datasets-0.1.0-py2.py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "0a188913b8e6f2a2ce5126c195bf806b",
        "sha256": "eebcf85f74dcb1a5c873f7fd01833b74746fc4df3f576718b8d5867afaed46c5"
      },
      "downloads": 43,
      "filename": "tensorflow_datasets-0.1.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0a188913b8e6f2a2ce5126c195bf806b",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 25125,
      "upload_time": "2017-03-13T21:15:33",
      "url": "https://files.pythonhosted.org/packages/f0/2a/8c35a879abe68643c706f6e9c63057c03143b39c9f46e09184237dea4a39/tensorflow_datasets-0.1.0-py2.py3-none-any.whl"
    }
  ]
}