{
  "info": {
    "author": "Lukasz Janyst",
    "author_email": "xyz@jany.st",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "Environment :: No Input/Output (Daemon)",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Topic :: Internet :: WWW/HTTP"
    ],
    "description": "\n=========\nScrapy Do\n=========\n\n.. image:: https://api.travis-ci.org/ljanyst/scrapy-do.svg?branch=master\n   :target: https://travis-ci.org/ljanyst/scrapy-do\n\n.. image:: https://coveralls.io/repos/github/ljanyst/scrapy-do/badge.svg?branch=master\n   :target: https://coveralls.io/github/ljanyst/scrapy-do?branch=master\n\n.. image:: https://img.shields.io/pypi/v/scrapy-do.svg\n   :target: https://pypi.python.org/pypi/scrapy-do\n   :alt: PyPI Version\n\n\nScrapy Do is a daemon that provides a convenient way to run `Scrapy\n<https://scrapy.org/>`_ spiders. It can either do it once - immediately; or it\ncan run them periodically, at specified time intervals. It's been inspired by\n`scrapyd <https://github.com/scrapy/scrapyd>`_ but written from scratch. For\nthe time being, it only comes with a REST API and a command line client. Version\n0.3.0 will have an interactive web interface.\n\n * Homepage: `https://jany.st/scrapy-do.html <https://jany.st/scrapy-do.html>`_\n * Documentation: `https://scrapy-do.readthedocs.io/en/latest/ <https://scrapy-do.readthedocs.io/en/latest/>`_\n\n-----------\nQuick Start\n-----------\n\n* Install ``scrapy-do`` using ``pip``:\n\n  .. code-block:: console\n\n       $ pip install scrapy-do\n\n* Start the daemon in the foreground:\n\n  .. code-block:: console\n\n       $ scrapy-do -n scrapy-do\n\n* Open another terminal window and store the server's URL in the client's\n  configuration file so that you don't have to type it all the time:\n\n  .. code-block:: console\n\n       $ cat > ~/.scrapy-do.cfg << EOF\n       > [scrapy-do]\n       > url=http://localhost:7654\n       > EOF\n\n\n* Download the Scrapy's Quotesbot example and push the code to the server:\n\n  .. code-block:: console\n\n       $ git clone https://github.com/scrapy/quotesbot.git\n       $ cd quotesbot\n       $ scrapy-do-cl push-project\n       +----------------+\n       | spiders        |\n       |----------------|\n       | toscrape-css   |\n       | toscrape-xpath |\n       +----------------+\n\n* Schedule some jobs:\n\n  .. code-block:: console\n\n       $ scrapy-do-cl schedule-job --project quotesbot \\\n           --spider toscrape-css --when 'every 5 to 15 minutes'\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | 0a3db618-d8e1-48dc-a557-4e8d705d599c |\n       +--------------------------------------+\n\n       $ scrapy-do-cl schedule-job --project quotesbot --spider toscrape-css\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | b3a61347-92ef-4095-bb68-0702270a52b8 |\n       +--------------------------------------+\n\n* See what's going on:\n\n  .. code-block:: console\n\n       $ scrapy-do-cl list-jobs\n       +--------------------------------------+-----------+--------------+-----------+-----------------------+---------+----------------------------+------------+\n       | identifier                           | project   | spider       | status    | schedule              | actor   | timestamp                  | duration   |\n       |--------------------------------------+-----------+--------------+-----------+-----------------------+---------+----------------------------+------------|\n       | b3a61347-92ef-4095-bb68-0702270a52b8 | quotesbot | toscrape-css | RUNNING   | now                   | USER    | 2018-01-27 08:32:19.781720 |            |\n       | 0a3db618-d8e1-48dc-a557-4e8d705d599c | quotesbot | toscrape-css | SCHEDULED | every 5 to 15 minutes | USER    | 2018-01-27 08:29:24.749770 |            |\n       +--------------------------------------+-----------+--------------+-----------+-----------------------+---------+----------------------------+------------+\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://jany.st/scrapy-do.html",
    "keywords": "",
    "license": "BSD License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-do",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-do/",
    "release_url": "https://pypi.org/project/scrapy-do/0.2.0/",
    "requires_dist": [
      "requests",
      "tabulate",
      "pem",
      "schedule",
      "python-dateutil",
      "psutil",
      "pyOpenSSL",
      "twisted",
      "scrapy"
    ],
    "requires_python": "",
    "summary": "A Spider Runner for Scrapy",
    "version": "0.2.0"
  },
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b34748d95f5503b9a1f731e8c3b2295b",
          "sha256": "d90388c240648c391b1dc4d911d158d735a487f7c3c2c82d6b3d08db353afecd"
        },
        "downloads": -1,
        "filename": "scrapy_do-0.1.0-py3-none-any.whl",
        "has_sig": true,
        "md5_digest": "b34748d95f5503b9a1f731e8c3b2295b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 20685,
        "upload_time": "2017-12-11T17:40:11",
        "url": "https://files.pythonhosted.org/packages/f3/3f/0a9fdcbbde07dacaccaa13f23c311e1d58808b2637b9b89d870093f79681/scrapy_do-0.1.0-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "0e6b6e12f968af9161b414c5dd710649",
          "sha256": "b63188f4450a304f9e78fd1899ce2053f0ca9bb631226c42f18105c27039f791"
        },
        "downloads": -1,
        "filename": "scrapy-do-0.1.0.tar.gz",
        "has_sig": true,
        "md5_digest": "0e6b6e12f968af9161b414c5dd710649",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15531,
        "upload_time": "2017-12-11T17:40:14",
        "url": "https://files.pythonhosted.org/packages/4c/40/3337dc90bc3a35fdc54c4d590375f7589dc7b88e2c495615bc7b0f342bb3/scrapy-do-0.1.0.tar.gz"
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "5e180cd2a352befea28275d61d3a84ef",
          "sha256": "8580bb7421a777ee3b6103e77aa808ada9c67c222b9f9e1d7bc6b0e307ac5fb8"
        },
        "downloads": -1,
        "filename": "scrapy_do-0.2.0-py3-none-any.whl",
        "has_sig": true,
        "md5_digest": "5e180cd2a352befea28275d61d3a84ef",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 29200,
        "upload_time": "2018-01-27T10:14:23",
        "url": "https://files.pythonhosted.org/packages/5b/78/bd7f2adcdabddc939218d1cc8fa54b081697f3826048743b698fb2481496/scrapy_do-0.2.0-py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "5e180cd2a352befea28275d61d3a84ef",
        "sha256": "8580bb7421a777ee3b6103e77aa808ada9c67c222b9f9e1d7bc6b0e307ac5fb8"
      },
      "downloads": -1,
      "filename": "scrapy_do-0.2.0-py3-none-any.whl",
      "has_sig": true,
      "md5_digest": "5e180cd2a352befea28275d61d3a84ef",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 29200,
      "upload_time": "2018-01-27T10:14:23",
      "url": "https://files.pythonhosted.org/packages/5b/78/bd7f2adcdabddc939218d1cc8fa54b081697f3826048743b698fb2481496/scrapy_do-0.2.0-py3-none-any.whl"
    }
  ]
}