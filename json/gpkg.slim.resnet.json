{
  "info": {
    "author": "Guild AI",
    "author_email": "packages@guild.ai",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "\nModels\n######\n\nslim-resnet-50\n##############\n\n*ResNet-50 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-50*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-50*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-101\n###############\n\n*ResNet-101 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-101*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-101*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-152\n###############\n\n*ResNet-152 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-152*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-152*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-200\n###############\n\n*ResNet-200 classifier in TF-Slim*\n\nOperations\n==========\n\ntrain\n^^^^^\n\n*Train ResNet-200*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-50-v2\n#################\n\n*ResNet-50-v2 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-50-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-50-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-101-v2\n##################\n\n*ResNet-101-v2 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-101-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-101-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-152-v2\n##################\n\n*ResNet-152-v2 classifier in TF-Slim*\n\nOperations\n==========\n\nfine-tune\n^^^^^^^^^\n\n*Fine-tune ResNet-152-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\ntrain\n^^^^^\n\n*Train ResNet-152-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nslim-resnet-200-v2\n##################\n\n*ResNet-200-v2 classifier in TF-Slim*\n\nOperations\n==========\n\ntrain\n^^^^^\n\n*Train ResNet-200-v2*\n\nFlags\n-----\n\n**batch-size**\n  *Number of samples in each batch (default is 32)*\n\n**dataset**\n  *Dataset to train with (cifar10, mnist, flowers)*\n\n**learning-rate**\n  *Initial learning rate (default is 0.01)*\n\n**learning-rate-decay-type**\n  *How the learning rate is decayed (default is 'exponential')*\n\n**log-every-n-steps**\n  *Steps between status updates (default is 10)*\n\n**max-steps**\n  *Maximum number of training steps (default is 1000)*\n\n**optimizer**\n  *Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd,\n  rmsprop) (default is 'rmsprop')*\n\n**save-model-secs**\n  *Seconds between model saves (default is 60)*\n\n**save-summaries-secs**\n  *Seconds between summary saves (default is 60)*\n\n**weight-decay**\n  *Weight decay on the model weights (default is 4e-05)*\n\n\nReferences\n##########\n\nModelfile: https://github.com/guildai/index/tree/master/slim/resnet/MODELS\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/guildai/index/tree/master/slim/resnet",
    "keywords": "resnet images model",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gpkg.slim.resnet",
    "platform": "",
    "project_url": "https://pypi.org/project/gpkg.slim.resnet/",
    "release_url": "https://pypi.org/project/gpkg.slim.resnet/0.1.0.dev2/",
    "requires_dist": [
      "gpkg.slim.datasets"
    ],
    "requires_python": "",
    "summary": "TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)",
    "version": "0.1.0.dev2"
  },
  "releases": {
    "0.1.0.dev2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "66e56b7eb3d24fecb36a27c5f57fe308",
          "sha256": "7d6493177d1fb154a466949c20eb6256ea412ab5308ae09ffff549b841f1c85d"
        },
        "downloads": -1,
        "filename": "gpkg.slim.resnet-0.1.0.dev2-py2.py3-none-any.whl",
        "has_sig": true,
        "md5_digest": "66e56b7eb3d24fecb36a27c5f57fe308",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 6975,
        "upload_time": "2017-11-27T18:54:31",
        "url": "https://files.pythonhosted.org/packages/3c/d2/d033ba2ef13afa234d7929c6a5ff997246347e3edfbfeccc3959c6c47bbf/gpkg.slim.resnet-0.1.0.dev2-py2.py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "66e56b7eb3d24fecb36a27c5f57fe308",
        "sha256": "7d6493177d1fb154a466949c20eb6256ea412ab5308ae09ffff549b841f1c85d"
      },
      "downloads": -1,
      "filename": "gpkg.slim.resnet-0.1.0.dev2-py2.py3-none-any.whl",
      "has_sig": true,
      "md5_digest": "66e56b7eb3d24fecb36a27c5f57fe308",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 6975,
      "upload_time": "2017-11-27T18:54:31",
      "url": "https://files.pythonhosted.org/packages/3c/d2/d033ba2ef13afa234d7929c6a5ff997246347e3edfbfeccc3959c6c47bbf/gpkg.slim.resnet-0.1.0.dev2-py2.py3-none-any.whl"
    }
  ]
}