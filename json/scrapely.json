{
  "info": {
    "author": "Scrapy project",
    "author_email": "info@scrapy.org",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Topic :: Internet :: WWW/HTTP",
      "Topic :: Text Processing :: Markup :: HTML"
    ],
    "description": "========\nScrapely\n========\n\n.. image:: https://api.travis-ci.org/scrapy/scrapely.svg?branch=master\n    :target: https://travis-ci.org/scrapy/scrapely\n\nScrapely is a library for extracting structured data from HTML pages. Given\nsome example web pages and the data to be extracted, scrapely constructs a\nparser for all similar pages.\n\nOverview\n========\n\nScrapinghub wrote a nice `blog post`_ explaining how scrapely works and how it's used in Portia_.\n\n.. _blog post: https://blog.scrapinghub.com/2016/07/07/scrapely-the-brains-behind-portia-spiders/\n.. _Portia: http://portia.readthedocs.io/\n\nInstallation\n============\n\nScrapely works in Python 2.7 or 3.3+.\nIt requires numpy and w3lib Python packages.\n\nTo install scrapely on any platform use::\n\n    pip install scrapely\n\nIf you're using Ubuntu (9.10 or above), you can install scrapely from the\nScrapy Ubuntu repos. Just add the Ubuntu repos as described here:\nhttp://doc.scrapy.org/en/latest/topics/ubuntu.html\n\nAnd then install scrapely with::\n\n    aptitude install python-scrapely\n\nUsage (API)\n===========\n\nScrapely has a powerful API, including a template format that can be edited\nexternally, that you can use to build very capable scrapers.\n\nWhat follows is a quick example of the simplest possible usage, that you can\nrun in a Python shell.\n\nStart by importing and instantiating the Scraper class::\n\n    >>> from scrapely import Scraper\n    >>> s = Scraper()\n\nThen, proceed to train the scraper by adding some page and the data you expect\nto scrape from there (note that all keys and values in the data you pass must\nbe strings)::\n\n    >>> url1 = 'http://pypi.python.org/pypi/w3lib/1.1'\n    >>> data = {'name': 'w3lib 1.1', 'author': 'Scrapy project', 'description': 'Library of web-related functions'}\n    >>> s.train(url1, data)\n\nFinally, tell the scraper to scrape any other similar page and it will return\nthe results::\n\n    >>> url2 = 'http://pypi.python.org/pypi/Django/1.3'\n    >>> s.scrape(url2)\n    [{u'author': [u'Django Software Foundation &lt;foundation at djangoproject com&gt;'],\n      u'description': [u'A high-level Python Web framework that encourages rapid development and clean, pragmatic design.'],\n      u'name': [u'Django 1.3']}]\n\nThat's it! No xpaths, regular expressions, or hacky python code.\n\nUsage (command line tool)\n=========================\n\nThere is also a simple script to create and manage Scrapely scrapers.\n\nIt supports a command-line interface, and an interactive prompt. All commands\nsupported on interactive prompt are also supported in the command-line\ninterface.\n\nTo enter the interactive prompt type the following without arguments::\n\n    python -m scrapely.tool myscraper.json\n\nExample::\n\n    $ python -m scrapely.tool myscraper.json\n    scrapely> help\n\n    Documented commands (type help <topic>):\n    ========================================\n    a  al  s  ta  td  tl\n\n    scrapely>\n\nTo create a scraper and add a template::\n\n    scrapely> ta http://pypi.python.org/pypi/w3lib/1.1\n    [0] http://pypi.python.org/pypi/w3lib/1.1\n\nThis is equivalent as typing the following in one command::\n\n    python -m scrapely.tool myscraper.json ta http://pypi.python.org/pypi/w3lib/1.1\n\nTo list available templates from a scraper::\n\n    scrapely> tl\n    [0] http://pypi.python.org/pypi/w3lib/1.1\n\nTo add a new annotation, you usually test the selection criteria first::\n\n    scrapely> t 0 w3lib 1.1\n    [0] u'<h1>w3lib 1.1</h1>'\n    [1] u'<title>Python Package Index : w3lib 1.1</title>'\n\nYou can also quote the text, if you need to specify an arbitrary number of\nspaces, for example::\n\n    scrapely> t 0 \"w3lib 1.1\"\n\nYou can refine by position. To take the one in position [0]::\n\n    scrapely> a 0 w3lib 1.1 -n 0\n    [0] u'<h1>w3lib 1.1</h1>'\n\nTo annotate some fields on the template::\n\n    scrapely> a 0 w3lib 1.1 -n 0 -f name\n    [new] (name) u'<h1>w3lib 1.1</h1>'\n    scrapely> a 0 Scrapy project -n 0 -f author\n    [new] u'<span>Scrapy project</span>'\n\nTo list annotations on a template::\n\n    scrapely> al 0\n    [0-0] (name) u'<h1>w3lib 1.1</h1>'\n    [0-1] (author) u'<span>Scrapy project</span>'\n\nTo scrape another similar page with the already added templates::\n\n    scrapely> s http://pypi.python.org/pypi/Django/1.3\n    [{u'author': [u'Django Software Foundation'], u'name': [u'Django 1.3']}]\n\n\nTests\n=====\n\n`tox`_ is the preferred way to run tests. Just run: ``tox`` from the root\ndirectory.\n\nSupport\n=======\n\n* Mailing list: https://groups.google.com/forum/#!forum/scrapely\n* IRC: `scrapy@freenode`_\n\nScrapely is created and maintained by the Scrapy group, so you can get help\nthrough the usual support channels described in the `Scrapy community`_ page.\n\nArchitecture\n============\n\nUnlike most scraping libraries, Scrapely doesn't work with DOM trees or xpaths\nso it doesn't depend on libraries such as lxml or libxml2. Instead, it uses\nan internal pure-python parser, which can accept poorly formed HTML. The HTML is\nconverted into an array of token ids, which is used for matching the items to\nbe extracted.\n\nScrapely extraction is based upon the Instance Based Learning algorithm [1]_\nand the matched items are combined into complex objects (it supports nested and\nrepeated objects), using a tree of parsers, inspired by A Hierarchical\nApproach to Wrapper Induction [2]_.\n\n.. [1] `Yanhong Zhai , Bing Liu, Extracting Web Data Using Instance-Based Learning, World Wide Web, v.10 n.2, p.113-132, June 2007 <http://portal.acm.org/citation.cfm?id=1265174>`_\n\n.. [2] `Ion Muslea , Steve Minton , Craig Knoblock, A hierarchical approach to wrapper induction, Proceedings of the third annual conference on Autonomous Agents, p.190-197, April 1999, Seattle, Washington, United States <http://portal.acm.org/citation.cfm?id=301191>`_\n\nKnown Issues\n============\n\nThe training implementation is currently very simple and is only provided for\nreferences purposes, to make it easier to test Scrapely and play with it. On\nthe other hand, the extraction code is reliable and production-ready. So, if\nyou want to use Scrapely in production, you should use train() with caution and\nmake sure it annotates the area of the page you intended.\n\nAlternatively, you can use the Scrapely command line tool to annotate pages,\nwhich provides more manual control for higher accuracy.\n\nHow does Scrapely relate to `Scrapy`_?\n======================================\n\nDespite the similarity in their names, Scrapely and `Scrapy`_ are quite\ndifferent things. The only similarity they share is that they both depend on\n`w3lib`_, and they are both maintained by the same group of developers (which\nis why both are hosted on the `same Github account`_).\n\nScrapy is an application framework for building web crawlers, while Scrapely is\na library for extracting structured data from HTML pages. If anything, Scrapely\nis more similar to `BeautifulSoup`_ or `lxml`_ than Scrapy.\n\nScrapely doesn't depend on Scrapy nor the other way around. In fact, it is\nquite common to use Scrapy without Scrapely, and viceversa.\n\nIf you are looking for a complete crawler-scraper solution, there is (at least)\none project called `Slybot`_ that integrates both, but you can definitely use\nScrapely on other web crawlers since it's just a library.\n\nScrapy has a builtin extraction mechanism called `selectors`_ which (unlike\nScrapely) is based on XPaths.\n\n\nLicense\n=======\n\nScrapely library is licensed under the BSD license.\n\n.. _Scrapy: http://scrapy.org/\n.. _w3lib: https://github.com/scrapy/w3lib\n.. _BeautifulSoup: http://www.crummy.com/software/BeautifulSoup/\n.. _lxml: http://lxml.de/\n.. _same Github account: https://github.com/scrapy\n.. _slybot: https://github.com/scrapy/slybot\n.. _selectors: http://doc.scrapy.org/en/latest/topics/selectors.html\n.. _nose: http://readthedocs.org/docs/nose/en/latest/\n.. _scrapy@freenode: http://webchat.freenode.net/?channels=scrapy\n.. _Scrapy community: http://scrapy.org/community/\n.. _tox: https://pypi.python.org/pypi/tox\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/scrapy/scrapely",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapely",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/scrapely/",
    "release_url": "https://pypi.org/project/scrapely/0.13.4/",
    "requires_python": "",
    "summary": "A pure-python HTML screen-scraping library",
    "version": "0.13.4"
  },
  "releases": {
    "0.10": [
      {
        "comment_text": "",
        "digests": {
          "md5": "867e6d65272b8044ea3b14c16d273954",
          "sha256": "db48b220b20ae30cb5eba1996f42fa86c70fbd0da79063d7c9f66583d8e1a940"
        },
        "downloads": 4545,
        "filename": "scrapely-0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "867e6d65272b8044ea3b14c16d273954",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 26039,
        "upload_time": "2014-01-14T02:46:12",
        "url": "https://files.pythonhosted.org/packages/76/47/5230651436bbb1cad48ae1896c33df06025e50180e82877ce18519a60b75/scrapely-0.10.tar.gz"
      }
    ],
    "0.11.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0ea2357c6ceba768addd7e03ca7d9172",
          "sha256": "e2c36bf25e4db6286e9a08339f61969a07dc72209acc4b9950ea0a6be4acc0d7"
        },
        "downloads": 2108,
        "filename": "scrapely-0.11.0-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "0ea2357c6ceba768addd7e03ca7d9172",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 31840,
        "upload_time": "2014-08-01T21:52:32",
        "url": "https://files.pythonhosted.org/packages/2c/1a/c89ba5e74703727bfc63a34579eb5efe7b4f488ee0e522a4c264cb92537e/scrapely-0.11.0-py2-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "bd08fd66f4384c9894fc2b1e89ca94a6",
          "sha256": "6d7518c4acb270cf6116a1ccc65a2ff3eb311e6b65bdb3fd71dff98d09a7b17e"
        },
        "downloads": 1668,
        "filename": "scrapely-0.11.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bd08fd66f4384c9894fc2b1e89ca94a6",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 29160,
        "upload_time": "2014-08-01T21:52:30",
        "url": "https://files.pythonhosted.org/packages/ee/a8/a6b8886fff2846049f6cd49febcccab6825feb5d0d330bae5257ffa29080/scrapely-0.11.0.tar.gz"
      }
    ],
    "0.12.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1badf106492088c2b852f7f5c1107f98",
          "sha256": "4c3de5dece9a81db8b479b2e496161f4d949519ab4993fcd7e20500da873c2b1"
        },
        "downloads": 7610,
        "filename": "scrapely-0.12.0-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "1badf106492088c2b852f7f5c1107f98",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 31865,
        "upload_time": "2015-01-26T16:37:47",
        "url": "https://files.pythonhosted.org/packages/2f/62/911cbced57f1e7a6fc7db7da8238c679b62ff42f01c24b26400830642418/scrapely-0.12.0-py2-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "bb0916fb85a40580ad14485f7f26f0ad",
          "sha256": "ba16099e90d448b063d119832b09836f632d9a6328bcce176aa89655e174e75f"
        },
        "downloads": 1688,
        "filename": "scrapely-0.12.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bb0916fb85a40580ad14485f7f26f0ad",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 29293,
        "upload_time": "2015-01-26T16:37:45",
        "url": "https://files.pythonhosted.org/packages/44/ac/5da9c0e7d3cac2729b4f279129ea909e6ed8cbf7f72b613d83d64eb00b8b/scrapely-0.12.0.tar.gz"
      }
    ],
    "0.13.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "50b718516d8910fe8f97fb0747336d3f",
          "sha256": "ef0d7b3e7e1f00ac117532b9a0bade71f56aa3e4c2d80787ffae83f1ad2e87a2"
        },
        "downloads": 66,
        "filename": "scrapely-0.13.0.tar.gz",
        "has_sig": false,
        "md5_digest": "50b718516d8910fe8f97fb0747336d3f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134454,
        "upload_time": "2016-12-21T09:55:14",
        "url": "https://files.pythonhosted.org/packages/0e/47/c9dab3832707dce62ac39599dda7e8679e3221c70be9c932d44c27727551/scrapely-0.13.0.tar.gz"
      }
    ],
    "0.13.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "7a271ec0cd432aa37d23936ef51d026c",
          "sha256": "de065beb28733cc46bc3e560d302bc8c445056bdfbeabcfc46786e06dc3a8672"
        },
        "downloads": 68,
        "filename": "scrapely-0.13.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7a271ec0cd432aa37d23936ef51d026c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134462,
        "upload_time": "2016-12-21T12:02:06",
        "url": "https://files.pythonhosted.org/packages/45/d9/96056009bf88726a959da52139b0639ca007a8f87246668bc874d4aef8c7/scrapely-0.13.1.tar.gz"
      }
    ],
    "0.13.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "12fcedb8dc3376c49fbc0da719004002",
          "sha256": "f6b5839342e0e874f3d5a636ad48c10312a682de3e0a4153f2768627c28ea751"
        },
        "downloads": 173,
        "filename": "scrapely-0.13.2.tar.gz",
        "has_sig": false,
        "md5_digest": "12fcedb8dc3376c49fbc0da719004002",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134471,
        "upload_time": "2016-12-21T12:51:45",
        "url": "https://files.pythonhosted.org/packages/fd/95/225193197af0b54522107126ae1069254e5c47f8145ddaca5fb67359e37c/scrapely-0.13.2.tar.gz"
      }
    ],
    "0.13.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "02a55f668a7b7a98a7713590d18aaddf",
          "sha256": "2292f53990915f87271e2e090c93b86531e23022342b55ea22e1d83195a4302d"
        },
        "downloads": 37,
        "filename": "scrapely-0.13.3.tar.gz",
        "has_sig": false,
        "md5_digest": "02a55f668a7b7a98a7713590d18aaddf",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134547,
        "upload_time": "2017-01-27T13:38:21",
        "url": "https://files.pythonhosted.org/packages/01/f0/e0c9d858dfe0fe31ed13346c2d0e3598e5683b6289f6e3e9aaddce461156/scrapely-0.13.3.tar.gz"
      }
    ],
    "0.13.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1ba71685e0cbcbe0ebd902c050e25e1d",
          "sha256": "d4baf3bdb9e48e3217da3de24dfa964f29e03c3c970220bf95af62d40b1719a0"
        },
        "downloads": 0,
        "filename": "scrapely-0.13.4.tar.gz",
        "has_sig": false,
        "md5_digest": "1ba71685e0cbcbe0ebd902c050e25e1d",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 134756,
        "upload_time": "2017-05-26T13:05:34",
        "url": "https://files.pythonhosted.org/packages/5e/8b/dcf53699a4645f39e200956e712180300ec52d2a16a28a51c98e96e76548/scrapely-0.13.4.tar.gz"
      }
    ],
    "0.9": [
      {
        "comment_text": "",
        "digests": {
          "md5": "05e45f03f4751329067c843b2bb7b2bc",
          "sha256": "c8d5fd285ab4c42ab34ec94426bb7200d65e64fc00f8dfe3c69a50e168e0d48f"
        },
        "downloads": 5271,
        "filename": "scrapely-0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "05e45f03f4751329067c843b2bb7b2bc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 24577,
        "upload_time": "2011-04-19T03:39:37",
        "url": "https://files.pythonhosted.org/packages/1d/16/892d94655015dc5e0dd1f0013e78b5c510a6da6b1f1d92b7b4ec7d43cdbd/scrapely-0.9.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "1ba71685e0cbcbe0ebd902c050e25e1d",
        "sha256": "d4baf3bdb9e48e3217da3de24dfa964f29e03c3c970220bf95af62d40b1719a0"
      },
      "downloads": 0,
      "filename": "scrapely-0.13.4.tar.gz",
      "has_sig": false,
      "md5_digest": "1ba71685e0cbcbe0ebd902c050e25e1d",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 134756,
      "upload_time": "2017-05-26T13:05:34",
      "url": "https://files.pythonhosted.org/packages/5e/8b/dcf53699a4645f39e200956e712180300ec52d2a16a28a51c98e96e76548/scrapely-0.13.4.tar.gz"
    }
  ]
}