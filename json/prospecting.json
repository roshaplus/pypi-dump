{
  "info": {
    "author": "Reid Bradley",
    "author_email": "reidbradley.dev@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.5"
    ],
    "description": "![status](https://img.shields.io/badge/status-wip-lightgrey.svg) ![license](https://img.shields.io/badge/license-MIT-blue.svg)\n\n<!-- \n![docker pulls](https://img.shields.io/docker/pulls/jupyter/base-notebook.svg) ![docker stars](https://img.shields.io/docker/stars/jupyter/base-notebook.svg) [![](https://images.microbadger.com/badges/image/jupyter/base-notebook.svg)](https://microbadger.com/images/jupyter/base-notebook \"jupyter/base-notebook image metadata\")\n-->\n\n# &#x2692; Prospecting &#x2692; <!-- &#9874; -->\n\nThis project started as an effort to predict a 'prospect score' for each business in a list of current and (predominantly) potential customers. \n\nWhile the initial goal was to provide a list to help prioritize sales opportunities (ex. rank order prospects by state), I also had some ideas about tying in Google Sheets to help with my typical ML workflow (data profiling > clean/transform > performance reporting > delivery of final predictions > revisiting column treatments >> etc). OAuth 2.0 is used for Google API authentication when using the `SheetsApi` and `DriveApi` classes, and the usual Sheets sharing options exist if you want to invite collaborators.\n\nI'll be updating this README and documentation in general...In the interim - as an example of how Google Sheets is used, the following table outlines the spreadsheets and tabs which I have found to be useful. While I cannot share my original prospecting dataset, I used an old [Innocentive challenge dataset](https://github.com/reidbradley/prospecting/blob/master/data/README.md) as an example.\n\n| spreadsheet | sheet | note\n| --- | --- | ---\n| [**projectname_metadata**](https://docs.google.com/spreadsheets/d/17R9V5tefzFzMXBi2i9SOybhqwzF7PSlse9OO99BfDxQ/) | _metadata_ | Control logic for column processing treatments; used by Python to inform how each column is processed. The functions in `process.py` rely on information from this tab.\n|  | _raw_descr_ | Descriptive information about raw data (`df_raw`)\n|  | _clean_descr_ | Descriptive information about cleaned dataset (`df_clean`)\n| --- | --- | ---\n| [**projectname_model_reporting**](https://docs.google.com/spreadsheets/d/1dG5lQfqthqshz45Rs94VLSSWmSrS60b1iw7cT4Rqevs/) | _session_report_ | Summarizes model performance, plan to make this the main performance tab. A \"session\" represents an instance of a \"ModelSession\" class instance which is used to share access to train/test sets.\n|  | _cv_results_ | If GridSearchCV is used, the `GridSearchCV.cv_results_` reports are saved here (shows performance by fold for each parameter set evaluated)\n|  | _model_types_ | A simple lookup table, used by Python script as a reference when building the report for the `session_report` tab\n|  | _\\_plots_ | <a href=\"https://docs.google.com/spreadsheets/d/1dG5lQfqthqshz45Rs94VLSSWmSrS60b1iw7cT4Rqevs/pubchart?oid=1358454056&format=interactive\"><img src=\"https://docs.google.com/spreadsheets/d/1dG5lQfqthqshz45Rs94VLSSWmSrS60b1iw7cT4Rqevs/pubchart?oid=1358454056&format=image\" alt=\"performance report\" height=\"115px\"></a>&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1dG5lQfqthqshz45Rs94VLSSWmSrS60b1iw7cT4Rqevs/pubchart?oid=6448021&format=interactive\"><img src=\"https://docs.google.com/spreadsheets/d/1dG5lQfqthqshz45Rs94VLSSWmSrS60b1iw7cT4Rqevs/pubchart?oid=6448021&format=image\" alt=\"performance report subset\" height=\"115px\"></a>\n| --- | --- | ---\n| **projectname_predictions** | _predictions_ | Final predictions, with probabilities\n|  | _lookupmaster_ | A lookup table with master list of prospects / entities of interest, or misc information to join with predictions\n|  | _README_ | Intend to use as an FYI tab, to provide overview of health of predictions made (ex. highlight number of correct/incorrect predictions, etc)\n\n\n\n<!--\n- **prospecting_metadata**\n - _metadata_ - Control logic for column processing treatments; used by Python to inform how each column is processed\n - _raw_descr_ - Descriptive information about raw data (`df_raw`)\n - _clean_descr_ - Descriptive information about cleaned dataset (`df_clean`)\n\n- **prospecting_model_reporting** (data model WIP)\n - _session_report_ - Summarizes model performance, plan to make this the main performance tab. A \"session\" represents an instance of a \"ModelSession\" class I created to allow sharing of train/test sets.\n - _cv_results_ - If GridSearchCV is used, the `GridSearchCV.cv_results_` reporting is saved here (shows k-fold performance for each parameter set evaluated)\n - _model_types_ - A simple lookup table, used as a reference when building the report for the `session_report` tab\n\n- **prospecting_predictions**\n - _predictions_ - Final predictions, with probabilities, by firm\n - _prospects_ - Master list of prospects\n - _README_ - Intend to use as an FYI tab, and to provide overview of health of predictions made (ex. highlight number of correct/incorrect predictions, etc)\n-->\n\n## Overview\n\n* The project directory contains:\n```\n        .\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac .dockerignore\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac .gitattributes              # For CRLF correction\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac .gitignore\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac credentials/                # Not necessarily best practice, but convenient\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac certs/\n        \u00e2\u201d\u201a       \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac data/\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac tmp/                    # Logs saved here\n        \u00e2\u201d\u201a       \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u201a       \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac joblib/             # Used by scikit learn when running in Docker container\n        \u00e2\u201d\u201a           \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac Dockerfile                  # See README_detail.md for more info\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac LICENSE.md\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac jupyter_notebook_config.py  # See README_detail.md for more info\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac mplimporthook.py            # Used by Dockerfile\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac notebooks/                  # Jupyter Notebooks\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac prospecting\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac __init__.py\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac env.py                  # Check here for environment variables required\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac utils.py\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac api.py                  # Google Sheets and Google Drive API classes\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac process.py              # Data cleaning functions, relies on info in metadata tab\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac model.py\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac report.py\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac errors.py\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac version.py\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac README.md\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac requirements_nonconda.txt   # Used by Dockerfile\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac scripts\n        \u00e2\u201d\u201a\u00c2\u00a0\u00c2\u00a0 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac hash_jupyter_pw.py      # Create hashed password to use with Docker container\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac start-notebook.sh           # Used by Dockerfile\n        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac start.sh                    # Used by Dockerfile\n        \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac start-singleuser.sh         # Used by Dockerfile\n```\n&#x2692; &#x2692; &#x2692;",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/reidbradley/prospecting",
    "keywords": "predictive analytics",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "prospecting",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/prospecting/",
    "release_url": "https://pypi.org/project/prospecting/0.1.2/",
    "requires_python": "",
    "summary": "predict lead score for list of prospects",
    "version": "0.1.2"
  },
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "49ec3372b57b3744f1d87339db724e8d",
          "sha256": "6a3b9a0cfb2693ccbd46363e337dafa12286af5b1a97139f5c789da4a9bb1796"
        },
        "downloads": 14,
        "filename": "prospecting-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "49ec3372b57b3744f1d87339db724e8d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 23056,
        "upload_time": "2017-02-13T18:19:12",
        "url": "https://files.pythonhosted.org/packages/6d/02/1f2617131a856aaf5a78e8f818fd93f8c44d400b3edd6984cfb657795348/prospecting-0.1.2-py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "49ec3372b57b3744f1d87339db724e8d",
        "sha256": "6a3b9a0cfb2693ccbd46363e337dafa12286af5b1a97139f5c789da4a9bb1796"
      },
      "downloads": 14,
      "filename": "prospecting-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "49ec3372b57b3744f1d87339db724e8d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 23056,
      "upload_time": "2017-02-13T18:19:12",
      "url": "https://files.pythonhosted.org/packages/6d/02/1f2617131a856aaf5a78e8f818fd93f8c44d400b3edd6984cfb657795348/prospecting-0.1.2-py3-none-any.whl"
    }
  ]
}