{
  "info": {
    "author": "Andreu Casas",
    "author_email": "acasas2@uw.edu",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "`rlda`: Robust Latent Dirichlet Allocation models \n-------------------------\n\nThis python module provides a set of functions to fit multiple LDA models to a \ntext corpus and then search for the robust topics present in multiple models.\n\nIn natural language processing LDA models are used to classify text into topics. However, the substance of\ntopics often varies depending on model specification (e.g. number of *k* topics), making them\nquite unstable (see Chuang_ 2015). This `python` module implements a method \nproposed by Wilkerson and Casas (2016) to add a level of robustness when using\nunsupervised topic models. You can find the replication material for the Wilkerson and Casas (2016) paper in this_ GitHub repository.\n\nPlease cite as:\n\nWilkerson, John and Andreu Casas. 2016. \"Large-scale Computerized Text\nAnalysis in Political Science: Opportunities and Challenges.\" *Annual Review\nof Political Science*, AA:x-x. (Forthcoming)\n\nInstallation\n-------------------------\n``pip install rlda``\n\nExample: studying the topic of one-minute floor speeches\n--------------------------------------------------------\n\n>>> import rlda\n>>> import random \n\nLoading all one-minute floor speeches from House representatives of the 113th Congress (n = 9,704). This dataset already comes with the module\n\n>>> sample_data = rlda.speeches_data\n\nEach observation or speech is a `dictionary` with the following keys: bioguide_ide, speech, date, party, id, captiolwords_url.\n\n.. image:: images/observation_example.png\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: center\n\nCreate a list conatining only the speeches. Using only a sample of 1,000 random speeches for this example so that it runs faster.\n\n>>> speeches = [d['speech'] for d in sample_data]\n>>> random.seed(1)\n>>> rand_vector = random.sample(xrange(len(speeches)), 1000)\n>>> sample = speeches[:100]\n\nCreate an object of class RLDA so that you can implement all functions in this module\n\n>>> robust_model = rlda.RLDA()\n\nPre-process the sample of speeches. These are the default settings, but you can choose your pre-processing parameters:\n\n   - Parsing speeches into words (features)\n   - Removing punctuation\n   - Removing stopwords (the default list, <stopw>, is the english stopwords list from the `nltk` module)\n   - Removing words shorter than 3 characters\n   - Stemming remaining words (Porter Stemmer)\n\n>>> clean_speeches = rlda.pre_processing(sample, remove_punct = True,\n                        remove_stopwords = True, stopwords_list = stopw,\n                        remove_words_shorter_than = 3, steming = True)\n\nConstruct a Term Document Matrix (TDM) from the speeches text\n\n>>> robust_model.get_tdm(clean_speeches)\n\nSpecify in a list the number of topics (k) of the LDA models you want to estimate. For example, 3 LDA models, one with 45 topics, one with 50, and one with 55\n\n>>> k_list = [45, 50, 55]\n\nSpecify the number of iterations when estimating the LDA models (e.g. 300)\n\n>>> n_iter = 300\n\nFit the multiple LDA models \n\n>>> robust_model.fit_models(k_list = k_list, n_iter = n_iter)\n\nGet the feature-topic-probabilty vectors for each topic, and also the top(e.g. 50) keywords for each topic\n\n>>> robust_model.get_all_ftp(features_top_n = 50)\n\nYou can explore now the top keywords of a topic in the console by using this funciton and specifying the topic label: \"k-t\" where k = the number of topics of that topic, and t = the topic number. For example, \"45-1\" is the first topic of the topic-model with 45 topics...\n\n>>> robust_model.show_top_kws('45-1')\n\n.. image:: images/topic_kws_example.png\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: center\n\n... or you can also save the top keywords for each model's topics in a separate \"csv\" file and explore them in a spreadsheet like Excel\n\n>>> robust_model.save_top_kws()\n\n.. image:: images/tm_45_1_example.png\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: center\n\nSave the classifications made by each lda model. Run this function to create a directory named \"classifications\" that will have as many \"csv\" files as topic-models you run. The \"csv\" files will have 2 variables: \"top_topic\", the topic of each document, \"text\", the text of the document\n\n>>> robust_model.save_models_classificiations()\n\nClustering topics to get more robust meta-topics\n--------------------------------------------------------\n\nCreate a cosine similarity matrix. Dimensions = TxT, where T = (number topics from all topic models). In this example the dimensions of the cosine matrix will be 150x150\n\n>>> robust_model.get_cosine_matrix()\n\nClustering the topics into N clusters, e.g. 50 clusters, using Spectral_ Clustering. \n\n>>> clusters = robust_model.cluster_topics(clusters_n = 50)\n\n... still editing! To be continued...\n\n\n\n\n.. _Chuang: http://www.aclweb.org/anthology/N15-1018  \n.. _Spectral: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html\n.. _this: https://github.com/CasAndreu/wilkerson_casas_2016_TAD\n",
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "http://github.com/CasAndreu/rlda",
    "keywords": null,
    "license": "MIT",
    "maintainer": null,
    "maintainer_email": null,
    "name": "rlda",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/rlda/",
    "release_url": "https://pypi.org/project/rlda/0.5/",
    "requires_python": null,
    "summary": "A module to use robust lda topics for the study of text",
    "version": "0.5"
  },
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "3930c2356be0ca728fa667f08e0d2233",
          "sha256": "4825597fda93572961590af89ba96351cb848f5593e1b834826d0428889036f5"
        },
        "downloads": 351,
        "filename": "rlda-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3930c2356be0ca728fa667f08e0d2233",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 1239,
        "upload_time": "2016-07-26T22:55:32",
        "url": "https://files.pythonhosted.org/packages/98/11/ac4752c80e881e5dd2828393a472fd03d51b262248dc6d0f625c0dda7a5b/rlda-0.1.tar.gz"
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "37cff5156d86d8a3a7fe5e6ae554ce49",
          "sha256": "82396b26fb4bfa2b172241577d0a2ac3bbd87e2db8cb8d32dea9145717896537"
        },
        "downloads": 111,
        "filename": "rlda-0.2.zip",
        "has_sig": false,
        "md5_digest": "37cff5156d86d8a3a7fe5e6ae554ce49",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 6636986,
        "upload_time": "2016-09-12T19:34:14",
        "url": "https://files.pythonhosted.org/packages/d4/b9/dfc27f48a9f19a4681dacb8f99ee47881ed05b678961495cd082de67e632/rlda-0.2.zip"
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "74330b55d7dcefc2dde31a011ccf5f82",
          "sha256": "cb9380ecf940eaec790bd85095972db10c2b128f97b00621826ff0a8f2fc479d"
        },
        "downloads": 65,
        "filename": "rlda-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "74330b55d7dcefc2dde31a011ccf5f82",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 6661691,
        "upload_time": "2016-09-16T06:28:44",
        "url": "https://files.pythonhosted.org/packages/ff/68/e17e0733b9e48cbb669a2430c51284267761d345e1d8379f713c2333a776/rlda-0.3.tar.gz"
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "36b4a1a7ef3d4d1b1bfac5a816330523",
          "sha256": "7460519ea8939f3932bef22d0d193b42d030609b89cb135480ac28d449791b86"
        },
        "downloads": 63,
        "filename": "rlda-0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "36b4a1a7ef3d4d1b1bfac5a816330523",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 7123975,
        "upload_time": "2016-09-17T02:21:37",
        "url": "https://files.pythonhosted.org/packages/c5/d3/36a881e23888f5cc93a6ca367f26c7c32e94e741c86e6e941b15618b1cd3/rlda-0.4.tar.gz"
      }
    ],
    "0.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "dea1075dd430e4c0c4997b38cb2fc010",
          "sha256": "f796cca86d1423411f118a413760941c106517e3972ee9589c8bb19d87536de4"
        },
        "downloads": 81,
        "filename": "rlda-0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "dea1075dd430e4c0c4997b38cb2fc010",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 7124077,
        "upload_time": "2016-09-17T02:23:36",
        "url": "https://files.pythonhosted.org/packages/69/d7/a914f33d13da5254e799728b8fe8240b15d734531a44288cb21deee7cd21/rlda-0.5.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "dea1075dd430e4c0c4997b38cb2fc010",
        "sha256": "f796cca86d1423411f118a413760941c106517e3972ee9589c8bb19d87536de4"
      },
      "downloads": 81,
      "filename": "rlda-0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "dea1075dd430e4c0c4997b38cb2fc010",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 7124077,
      "upload_time": "2016-09-17T02:23:36",
      "url": "https://files.pythonhosted.org/packages/69/d7/a914f33d13da5254e799728b8fe8240b15d734531a44288cb21deee7cd21/rlda-0.5.tar.gz"
    }
  ]
}