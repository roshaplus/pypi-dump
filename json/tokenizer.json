{
  "info": {
    "author": "Vilhj\u00e1lmur \u00deorsteinsson",
    "author_email": "vt@extrada.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: Icelandic",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: Text Processing :: Linguistic",
      "Topic :: Utilities"
    ],
    "description": "-----------------------------------------\nTokenizer: A tokenizer for Icelandic text\n-----------------------------------------\n\nOverview\n--------\n\nTokenization is a necessary first step in many natural language processing tasks,\nsuch as word counting, parsing, spell checking, corpus generation, and\nstatistical analysis of text.\n\nTokenizer is a compact pure-Python (2 and 3) module for tokenizing Icelandic text. It converts\nPython text strings to streams of token objects, where each token object is a separate\nword, punctuation sign, number/amount, date, e-mail, URL/URI, etc. It also segments\nthe token stream into sentences, considering corner cases such as abbreviations\nand dates in the middle of sentences.\n\nThe package contains a dictionary of common Icelandic abbreviations, in the file\n``src/tokenizer/Abbrev.conf``.\n\nTokenizer is derived from a corresponding module in the `Greynir project <https://greynir.is>`_\n(GitHub repository `here <https://github.com/vthorsteinsson/Reynir>`_), by the same author.\nNote that Tokenizer is licensed under the MIT license while Greynir is licensed under GPLv3.\n\nTo install::\n\n\tpip install tokenizer\n\nTo use (for Python 3, you can omit the ``u\"\"`` string prefix)::\n\n\tfrom tokenizer import tokenize, TOK\n\n\ttext = (u\"M\u00e1linu var v\u00edsa\u00f0 til stj\u00f3rnskipunar- og eftirlitsnefndar \"\n\t\tu\"skv. 3. gr. XVII. kafla laga nr. 10/2007 \u00feann 3. jan\u00faar 2010.\")\n\n\tfor token in tokenize(text):\n\n\t\tprint(u\"{0}: '{1}' {2}\".format(\n\t\t\tTOK.descr[token.kind],\n\t\t\ttoken.txt or \"-\",\n\t\t\ttoken.val or \"\"))\n\nOutput::\n\n\tS_BEGIN: '-' (0, None)\n\tWORD: 'M\u00e1linu'\n\tWORD: 'var'\n\tWORD: 'v\u00edsa\u00f0'\n\tWORD: 'til'\n\tWORD: 'stj\u00f3rnskipunar- og eftirlitsnefndar'\n\tWORD: 'skv.' samkv\u00e6mt\n\tORDINAL: '3.' 3\n\tWORD: 'gr.' grein\n\tORDINAL: 'XVII.' 17\n\tWORD: 'kafla'\n\tWORD: 'laga'\n\tWORD: 'nr.' n\u00famer\n\tNUMBER: '10' (10, None, None)\n\tPUNCTUATION: '/' 4\n\tYEAR: '2007' 2007\n\tWORD: '\u00feann'\n\tDATE: '3. jan\u00faar 2010' (2010, 1, 3)\n\tPUNCTUATION: '.' 3\n\tS_END: '-'\n\nNote the following:\n\n\t- Sentences are delimited by ``TOK.S_BEGIN`` and ``TOK.S_END`` tokens.\n\t- Composite words, such as *stj\u00f3rnskipunar- og eftirlitsnefndar*, are coalesced into one token.\n\t- Well-known abbreviations are recognized and their full expansion is available in the ``token.val`` field.\n\t- Ordinal numbers (*3., XVII.*) are recognized and their value (*3, 17*) is available in the ``token.val`` field.\n\t- Dates, years and times are recognized and the respective year, month, day, hour, minute and second\n\t  values are included as a tuple in ``token.val``.\n\t- Numbers, both integer and real, are recognized and their value is available in the ``token.val`` field.\n\n\nThe ``tokenize()`` function\n---------------------------\n\nTo tokenize a text string, call ``tokenizer.tokenize(text)``. This function returns a\nPython *generator* of token objects. Each token object is a simple ``namedtuple`` with three\nfields: ``(kind, txt, val)`` (see below).\n\nThe ``tokenizer.tokenize()`` function is typically called in a ``for`` loop::\n\n\tfor token in tokenizer.tokenize(mystring):\n\t\tkind, txt, val = token\n\t\tif kind == tokenizer.TOK.WORD:\n\t\t\t# Do something with word tokens\n\t\t\tpass\n\t\telse:\n\t\t\t# Do something else\n\t\t\tpass\n\nAlternatively, create a token list from the returned generator::\n\n\ttoken_list = list(tokenizer.tokenize(mystring))\n\n\nThe token object\n----------------\n\nEach token is represented by a ``namedtuple`` with three fields: ``(kind, txt, val)``.\n\nThe ``kind`` field contains one of the following integer constants, defined within the ``TOK``\nclass::\n\n    PUNCTUATION = 1\n    TIME = 2\n    DATE = 3\n    YEAR = 4\n    NUMBER = 5\n    WORD = 6\n    TELNO = 7\n    PERCENT = 8\n    URL = 9\n    ORDINAL = 10\n    TIMESTAMP = 11\n    CURRENCY = 12\t# Not used\n    AMOUNT = 13\n    PERSON = 14\t\t# Not used\n    EMAIL = 15\n    ENTITY = 16\t\t# Not used\n    UNKNOWN = 17\n\n    S_BEGIN = 11001\t# Sentence begin\n    S_END = 11002 \t# Sentence end\n\nTo obtain a descriptive text for a token kind, use ``TOK.descr[token.kind]`` (see example above).\n\nThe ``txt`` field contains the original source text for the token.\n\nIn the case of abbreviations that end a sentence, the final period '.' is a separate token,\nand it is consequently omitted from the abbreviation token's ``txt`` field. A sentence ending\nin *o.s.frv.* will thus end with two tokens, the next-to-last one being the tuple\n``(TOK.WORD, \"o.s.frv\", \"og svo framvegis\")`` - note the omitted period in the ``txt`` field -\nand the last one being ``(TOK.PUNCTUATION, \".\", 3)`` (the 3 is explained below).\n\nThe ``val`` field contains auxiliary information, corresponding to the token kind, as follows:\n\n- For ``TOK.PUNCTUATION``, the ``val`` field specifies the whitespace normally found around\n  the symbol in question::\n\n\tTP_LEFT = 1   # Whitespace to the left\n\tTP_CENTER = 2 # Whitespace to the left and right\n\tTP_RIGHT = 3  # Whitespace to the right\n\tTP_NONE = 4   # No whitespace\n\n- For ``TOK.TIME``, the ``val`` field contains an ``(hour, minute, second)`` tuple.\n- For ``TOK.DATE``, the ``val`` field contains a ``(year, month, day)`` tuple (all 1-based).\n- For ``TOK.YEAR``, the ``val`` field contains the year as an integer.\n- For ``TOK.NUMBER``, the ``val`` field contains a tuple ``(number, None, None)``.\n  (The two empty fields are included for compatibility with Greynir.)\n- For ``TOK.WORD``, the ``val`` field contains the full expansion of an abbreviation,\n  or ``None`` if the word is not abbreviated.\n- For ``TOK.PERCENT``, the ``val`` field contains a tuple of ``(percentage, None, None)``.\n- For ``TOK.ORDINAL``, the ``val`` field contains the ordinal value as an integer.\n- For ``TOK.TIMESTAMP``, the ``val`` field contains a ``(year, month, day, hour, minute, second)`` tuple.\n- For ``TOK.AMOUNT``, the ``val`` field contains an ``(amount, currency, None, None)`` tuple. The\n  amount is a float, and the currency is an ISO currency code, i.e. \"USD\" for dollars ($ sign) or\n  \"EUR\" for euros (\u20ac sign). (The two empty fields are included for compatibility with Greynir.)\n\n\nThe ``correct_spaces()`` function\n---------------------------------\n\nTokenizer also contains the utility function ``tokenizer.correct_spaces(text)``. This function\nreturns a string after splitting it up and re-joining\nit with correct whitespace around punctuation tokens. Example::\n\n\t>>> tokenizer.correct_spaces(\"Fr\u00e9tt \\n  dagsins:J\u00f3n\\t ,Fri\u00f0geir og P\u00e1ll ! 100  /  2  =   50\")\n\t'Fr\u00e9tt dagsins: J\u00f3n, Fri\u00f0geir og P\u00e1ll! 100/2 = 50'\n\n\nThe ``Abbrev.conf`` file\n------------------------\n\nAbbreviations recognized by Tokenizer are defined in the ``Abbrev.conf`` file, found in the\n``src/tokenizer/`` directory. This is a text file with abbreviations, their definitions and\nexplanatory comments. The file is loaded into memory during the first call to\n``tokenizer.tokenize()`` within a process.",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/vthorsteinsson/Tokenizer",
    "keywords": "nlp,tokenizer,icelandic",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokenizer",
    "platform": "",
    "project_url": "https://pypi.org/project/tokenizer/",
    "release_url": "https://pypi.org/project/tokenizer/0.1.3/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "A tokenizer for Icelandic text",
    "version": "0.1.3"
  },
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "51f8450a721544630aef1e6fb6a25b5c",
          "sha256": "e24eed67a141272d0cb88dfd3fa730082ca1aa119a38c9df57b87fda4fdc30f8"
        },
        "downloads": 0,
        "filename": "tokenizer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "51f8450a721544630aef1e6fb6a25b5c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 49706,
        "upload_time": "2017-10-01T16:08:19",
        "url": "https://files.pythonhosted.org/packages/61/50/b35a23d91503fa7ba757129c1bc04d37cfb7e0ead2b19729307f2a6dafb2/tokenizer-0.1.1.tar.gz"
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "bc8ad18886c03b320edff4280e1144f8",
          "sha256": "427b2aa6acedb3c8e89d78503ef4f22a9545cb455e1b48700084121d625d86e4"
        },
        "downloads": 0,
        "filename": "tokenizer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bc8ad18886c03b320edff4280e1144f8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 49676,
        "upload_time": "2017-10-01T16:30:42",
        "url": "https://files.pythonhosted.org/packages/43/92/bf087046d1ee49258c35448cc34608660a705ce5e01c0924b02518da3ed6/tokenizer-0.1.2.tar.gz"
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "fb16da94ce5a387c80d2cb00a2cd53eb",
          "sha256": "9c4da27fcb68f7c5959923f78765f4b775fe337d5d955dfeb74a58214c11d86c"
        },
        "downloads": 0,
        "filename": "tokenizer-0.1.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fb16da94ce5a387c80d2cb00a2cd53eb",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 58150,
        "upload_time": "2017-10-01T18:41:08",
        "url": "https://files.pythonhosted.org/packages/37/5e/35ddaa5e3fac34d0a440efd09cd7237fa94ef48047e7b4eca067e8ad51e2/tokenizer-0.1.3-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "0f2d98d55110d35ca63b45ba4b853878",
          "sha256": "eb1cf95c6f4fcfa87bcb8addc2acf5955b284647398b5fd216e3bcfdf94be4d8"
        },
        "downloads": 0,
        "filename": "tokenizer-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "0f2d98d55110d35ca63b45ba4b853878",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 50373,
        "upload_time": "2017-10-01T18:27:19",
        "url": "https://files.pythonhosted.org/packages/2d/8e/38a33f2e6ed3f39eab6d6d0074ac292386afb431ffc09fd9327069451425/tokenizer-0.1.3.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "fb16da94ce5a387c80d2cb00a2cd53eb",
        "sha256": "9c4da27fcb68f7c5959923f78765f4b775fe337d5d955dfeb74a58214c11d86c"
      },
      "downloads": 0,
      "filename": "tokenizer-0.1.3-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fb16da94ce5a387c80d2cb00a2cd53eb",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 58150,
      "upload_time": "2017-10-01T18:41:08",
      "url": "https://files.pythonhosted.org/packages/37/5e/35ddaa5e3fac34d0a440efd09cd7237fa94ef48047e7b4eca067e8ad51e2/tokenizer-0.1.3-py2.py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "0f2d98d55110d35ca63b45ba4b853878",
        "sha256": "eb1cf95c6f4fcfa87bcb8addc2acf5955b284647398b5fd216e3bcfdf94be4d8"
      },
      "downloads": 0,
      "filename": "tokenizer-0.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "0f2d98d55110d35ca63b45ba4b853878",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 50373,
      "upload_time": "2017-10-01T18:27:19",
      "url": "https://files.pythonhosted.org/packages/2d/8e/38a33f2e6ed3f39eab6d6d0074ac292386afb431ffc09fd9327069451425/tokenizer-0.1.3.tar.gz"
    }
  ]
}