{
  "info": {
    "author": "Cameron Simpson",
    "author_email": "cs@cskk.id.au",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "An assortment of lexcial and tokenisation functions useful for writing recursive descent parsers, of which I have several.\n\nGenerally the get_* functions accept a source string and an offset (often optional, default 0) and return a token and the new offset, raising ValueError on failed tokenisation.\n\n* as_lines(chunks, partials=None): parse text chunks, yield complete individual lines\n\n* get_chars(s, offset, gochars): collect adjacent characters from `gochars`\n\n* get_delimited(s, offset, delim): collect text up to the first ocurrence of the character `delim`.\n\n* get_envvar(s, offset=0, environ=None, default=None, specials=None): parse an environment variable reference such as $foo\n\n* get_identifier(s, offset=0, alpha=ascii_letters, number=digits, extras='_'): parse an identifier\n\n* get_nonwhite(s, offset=0): collect nonwhitespace characters\n\n* get_other_chars(s, offset=0, stopchars=None): collect adjacent characters not from `stopchars`\n\n* get_qstr(s, offset=0, q='\"', environ=None, default=None, env_specials=None): collect a quoted string, honouring slosh escapes and optionally expanding environment variable references\n\n* get_sloshed_text(s, delim, offset=0, slosh='\\\\', mapper=slosh_mapper, specials=None): collect some slosh escaped text with optional special tokens (such as '$' introducing '$foo')\n\n* get_tokens(s, offset, getters): collect a sequence of tokens specified in `getters`\n\n* match_tokens(s, offset, getters): wrapper for get_tokens which catches ValueError and returns None instead\n\n* get_uc_identifier(s, offset=0, number=digits, extras='_'): collect an UPPERCASE identifier\n\n* get_white(s, offset=0): collect whitespace characters\n\n* isUC_(s): test if a string looks like an upper case identifier\n\n* htmlify(s,nbsp=False): transcribe text in HTML safe form, using &lt; for \"<\", etc\n\n* htmlquote(s): transcribe text as HTML quoted string suitable for HTML tag attribute values\n\n* jsquote(s): transcribe text as JSON quoted string; essentially like htmlquote without its htmlify step\n\n* parseUC_sAttr(attr): parse FOO or FOOs (or FOOes) and return (FOO, is_plural)\n\n* slosh_mapper(c, charmap=SLOSH_CHARMAP): return a string to replace \\c; the default charmap matches Python slosh escapes\n\n* texthexify(bs, shiftin='[', shiftout=']', whitelist=None): a function like binascii.hexlify but also supporting embedded \"printable text\" subsequences for compactness and human readbility in the result; initial use case was for transcription of binary data with frequent text, specificly directory entry data\n\n* untexthexify(s, shiftin='[', shiftout=']'): the inverse of texthexify()\n\n* unctrl(s,tabsize=8): transcribe text removing control characters",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://bitbucket.org/cameron_simpson/css/commits/all",
    "keywords": "python2,python3",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cs.lex",
    "platform": "",
    "project_url": "https://pypi.org/project/cs.lex/",
    "release_url": "https://pypi.org/project/cs.lex/20170904/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "lexical analysis, tokenisers",
    "version": "20170904"
  },
  "releases": {
    "20150118": [
      {
        "comment_text": "",
        "digests": {
          "md5": "532ce38899726a27d8c0910f751e5e08",
          "sha256": "fbd1f673a03a36361e0b126241b9e12c18e79d844b6a64ae248d2b2986932500"
        },
        "downloads": 1752,
        "filename": "cs.lex-20150118.tar.gz",
        "has_sig": false,
        "md5_digest": "532ce38899726a27d8c0910f751e5e08",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8005,
        "upload_time": "2015-01-18T05:58:10",
        "url": "https://files.pythonhosted.org/packages/8d/3f/519781144aa6b9b603696365283c7c240458f4e3286013cf9bcc6c00ef00/cs.lex-20150118.tar.gz"
      }
    ],
    "20150120": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b8ed5aede3eb8b527acd72417413ef31",
          "sha256": "84efcc0725cf99fe953148c49c7d0e8c7e283c244f0ea039f6decfac54c98235"
        },
        "downloads": 1500,
        "filename": "cs.lex-20150120.tar.gz",
        "has_sig": false,
        "md5_digest": "b8ed5aede3eb8b527acd72417413ef31",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8142,
        "upload_time": "2015-01-19T23:36:47",
        "url": "https://files.pythonhosted.org/packages/b2/5f/83512d8dcd5e5d92bca9a048f2749863fa0f3f2964b4e0c18de761b72f47/cs.lex-20150120.tar.gz"
      }
    ],
    "20160828": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b22839b534cc0deb9d98dd9cef5b2166",
          "sha256": "b93062cb06598d82335df57268fc27ca49420f07f87c04a3a58019170d6d5a48"
        },
        "downloads": 364,
        "filename": "cs.lex-20160828.tar.gz",
        "has_sig": false,
        "md5_digest": "b22839b534cc0deb9d98dd9cef5b2166",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8951,
        "upload_time": "2016-08-28T06:07:18",
        "url": "https://files.pythonhosted.org/packages/b7/a5/1ddfcf4304d3f40897bce3701053b8521b85d82dc4dfe815321672f184ef/cs.lex-20160828.tar.gz"
      }
    ],
    "20170904": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4d318811bdb1f9a761491a75d029b547",
          "sha256": "3987b53546cfbab46c188bf8b83fa70b4fa311e1fd8256714e636abacd2254a2"
        },
        "downloads": 0,
        "filename": "cs.lex-20170904.tar.gz",
        "has_sig": false,
        "md5_digest": "4d318811bdb1f9a761491a75d029b547",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8278,
        "upload_time": "2017-09-04T01:07:12",
        "url": "https://files.pythonhosted.org/packages/c5/7c/a952ec94ddaf052b22f4fb2e274c60716cfe6e3727c966f5ee0affc37f1c/cs.lex-20170904.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "4d318811bdb1f9a761491a75d029b547",
        "sha256": "3987b53546cfbab46c188bf8b83fa70b4fa311e1fd8256714e636abacd2254a2"
      },
      "downloads": 0,
      "filename": "cs.lex-20170904.tar.gz",
      "has_sig": false,
      "md5_digest": "4d318811bdb1f9a761491a75d029b547",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 8278,
      "upload_time": "2017-09-04T01:07:12",
      "url": "https://files.pythonhosted.org/packages/c5/7c/a952ec94ddaf052b22f4fb2e274c60716cfe6e3727c966f5ee0affc37f1c/cs.lex-20170904.tar.gz"
    }
  ]
}