{
  "info": {
    "author": "Jacob Bouffard, James McClain",
    "author_email": "jbouffard@azavea.com, jmcclain@azavea.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: GIS",
      "Topic :: Scientific/Engineering :: Information Analysis"
    ],
    "description": "GeoPySpark\n***********\n.. image:: https://travis-ci.org/locationtech-labs/geopyspark.svg?branch=master\n   :target: https://travis-ci.org/locationtech-labs/geopyspark\n\n.. image:: https://readthedocs.org/projects/geopyspark/badge/?version=latest\n   :target: https://geopyspark.readthedocs.io/en/latest/?badge=latest\n\n``GeoPySpark`` provides Python bindings for working with geospatial data using `PySpark <http://spark.apache.org/docs/latest/api/python/pyspark.html>`_\nIt will provide interfaces into GeoTrellis and GeoMesa LocationTech frameworks.\nIt is currently under development, and has just entered alpha.\n\nCurrently, only functionality from GeoTrellis has been supported. GeoMesa\nLocationTech frameworks will be added at a later date.\n\nA Quick Example\n----------------\n\nHere is a quick example of GeoPySpark. In the following code, we take NLCD data\nof the state of Pennslyvania from 2011, and do a polygonal summary of an area\nof interest to find the min and max classifcations values of that area.\n\nIf you wish to follow along with this example, you will need to download the\nNLCD data and the geojson that represents the area of interest. Running these\ntwo commands will download these files for you:\n\n.. code:: console\n\n   curl -o /tmp/NLCD2011_LC_Pennsylvannia.zip https://s3-us-west-2.amazonaws.com/prd-tnm/StagedProducts/NLCD/2011/landcover/states/NLCD2011_LC_Pennsylvania.zip?ORIG=513_SBDDG\n   unzip /tmp/NLCD2011_LC_Pennsylvannia.zip\n   curl -o /tmp/area_of_interest.geojson https://s3.amazonaws.com/geopyspark-test/area_of_interest.json\n\n.. code:: python\n\n  import json\n  from functools import partial\n\n  from geopyspark.geopycontext import GeoPyContext\n  from geopyspark.geotrellis.constants import SPATIAL, ZOOM\n  from geopyspark.geotrellis.geotiff_rdd import get\n  from geopyspark.geotrellis.catalog import write\n\n  from shapely.geometry import Polygon, shape\n  from shapely.ops import transform\n  import pyproj\n\n\n  # Create the GeoPyContext\n  geopysc = GeoPyContext(appName=\"example\", master=\"local[*]\")\n\n  # Read in the NLCD tif that has been saved locally.\n  # This tif represents the state of Pennsylvania.\n  raster_rdd = get(geopysc=geopysc, rdd_type=SPATIAL,\n  uri='/tmp/NLCD2011_LC_Pennsylvania.tif',\n  options={'numPartitions': 100})\n\n  tiled_rdd = raster_rdd.to_tiled_layer()\n\n  # Reproject the reclassified TiledRasterRDD so that it is in WebMercator\n  reprojected_rdd = tiled_rdd.reproject(3857, scheme=ZOOM).cache().repartition(150)\n\n  # We will do a polygonal summary of the north-west region of Philadelphia.\n  with open('/tmp/area_of_interest.json') as f:\n      txt = json.load(f)\n\n  geom = shape(txt['features'][0]['geometry'])\n\n  # We need to reporject the geometry to WebMercator so that it will intersect with\n  # the TiledRasterRDD.\n  project = partial(\n      pyproj.transform,\n      pyproj.Proj(init='epsg:4326'),\n      pyproj.Proj(init='epsg:3857'))\n\n  area_of_interest = transform(project, geom)\n\n  # Find the min and max of the values within the area of interest polygon.\n  min_val = reprojected_rdd.polygonal_min(geometry=area_of_interest, data_type=int)\n  max_val = reprojected_rdd.polygonal_max(geometry=area_of_interest, data_type=int)\n\n  print('The min value of the area of interest is:', min_val)\n  print('The max value of the area of interest is:', max_val)\n\n  # We will now pyramid the relcassified TiledRasterRDD so that we can use it in a TMS server later.\n  pyramided_rdd = reprojected_rdd.pyramid(start_zoom=1, end_zoom=12)\n\n  # Save each layer of the pyramid locally so that it can be accessed at a later time.\n  for pyramid in pyramided_rdd:\n      write('file:///tmp/nld-2011', 'pa', pyramid)\n\n\nContact and Support\n--------------------\n\nIf you need help, have questions, or like to talk to the developers (let us\nknow what you're working on!) you contact us at:\n\n * `Gitter <https://gitter.im/geotrellis/geotrellis>`_\n * `Mailing list <https://locationtech.org/mailman/listinfo/geotrellis-user>`_\n\nAs you may have noticed from the above links, those are links to the GeoTrellis\ngitter channel and mailing list. This is because this project is currently an\noffshoot of GeoTrellis, and we will be using their mailing list and gitter\nchannel as a means of contact. However, we will form our own if there is a need\nfor it.\n\nSetup\n------\n\nGeoPySpark Requirements\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n============ ============\nRequirement  Version\n============ ============\nJava         >=1.8\nScala        2.11.8\nPython       3.3 - 3.5\nHadoop       >=2.0.1\n============ ============\n\nJava 8 and Scala 2.11 are needed for GeoPySpark to work; as they are required by\nGeoTrellis. In addition, Spark needs to be installed and configured with the\nenvironment variable, ``SPARK_HOME`` set.\n\nYou can test to see if Spark is installed properly by running the following in\nthe terminal:\n\n.. code:: console\n\n   > echo $SPARK_HOME\n   /usr/local/bin/spark\n\nIf the return is a path leading to your Spark folder, then it means that Spark\nhas been configured correctly.\n\nHow to Install\n^^^^^^^^^^^^^^^\n\nBefore installing, check the above table to make sure that the\nrequirements are met.\n\nInstalling From Pip\n~~~~~~~~~~~~~~~~~~~~\n\nTo install via ``pip`` open the terminal and run the following:\n\n.. code:: console\n\n   pip install geopyspark\n   geopyspark install-jar -p [path/to/install/jar]\n\nWhere the first command installs the python code from PyPi and the second\ndownloads the backend, jar file. If no path is given when downloading the jar,\nthen it will be downloaded to wherever GeoPySpark was installed at.\n\nWhat's With That Weird Pip Install?\n====================================\n\n\"What's with that weird pip install?\", you may be asking yourself. The reason\nfor its unusualness is due to how GeoPySpark functions. Because this library\nis a python binding for a Scala project, we need to be able to access the\nScala backend. To do this, we plug into PySpark which acts as a bridge between\nPython and Scala. However, in order to achieve this the Scala code needs to be\nassembled into a jar file. This poses a problem due to its size (117.7 MB at\nv0.1.0-RC!). To get around the size constraints of PyPi, we thus utilized this\nmethod of distribution where the jar must be downloaded in a serperate command\nwhen using ``pip install``.\n\nNote:\n  Installing from source or for development does not require the seperate\n  download of the jar.\n\nInstalling From Source\n~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you would rather install from source, clone the GeoPySpark repo and enter it.\n\n.. code:: console\n\n   git clone https://github.com/locationtech-labs/geopyspark.git\n   cd geopyspark\n\nInstalling For Users\n=====================\n\n.. code:: console\n\n   make install\n\nThis will assemble the backend-end ``jar`` that contains the Scala code,\nmove it to the ``jars`` sub-package, and then runs the ``setup.py`` script.\n\nNote:\n  If you have altered the global behavior of ``sbt`` this install may\n  not work the way it was intended.\n\nInstalling For Developers\n===========================\n\n.. code:: console\n\n   make build\n   pip install -e .\n\n``make build`` will assemble the back-end ``jar`` and move it the ``jars``\nsub-package. The second command will install GeoPySpark in \"editable\" mode.\nMeaning any changes to the source files will also appear in your system\ninstallation.\n\nInstalling to a Virtual Environment\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA third option is to install GeoPySpark in a virtual environment. To get things\nstarted, enter the envrionemnt and run the following:\n\n.. code:: console\n\n   git clone https://github.com/locationtech-labs/geopyspark.git\n   cd geopyspark\n   export PYTHONPATH=$VIRTUAL_ENV/lib/<your python version>/site-packages\n\nReplace ``<your python version`` with whatever Python version\n``virtualenvwrapper`` is set to. Installation in a virtual environment can be\na bit weird with GeoPySpark. This is why you need to export the\n``PYTHONPATH`` before installing to ensure that it performs correctly.\n\nInstalling For Users\n=====================\n\n.. code:: console\n\n   make virtual-install\n\nInstalling For Developers\n===========================\n\n.. code:: console\n\n   make build\n   pip install -e .\n\n\nDeveloping GeoPySpark With GeoNotebook\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n`GeoNotebook <https://github.com/OpenGeoscience/geonotebook>`_ is a Jupyter\nnotebook extension that specializes in working with geospatial data. GeoPySpark\ncan be used with this notebook; which allows for a more interactive experience\nwhen using the library. For this section, we will be installing both tools in a\nvirtual environment. It is recomended that you start with a new environment\nbefore following this guide.\n\nBecause there's already documentation on how to install GeoPySpark in a virtual\nenvironment, we won't go over it here. As for GeoNotebook, it also has a section\non `installtion <https://github.com/OpenGeoscience/geonotebook#make-a-virtualenv-install-jupyternotebook-install-geonotebook>`_\nso that will not be covered here either.\n\nOnce you've setup both GeoPySpark and GeoNotebook, all that needs to be done\nis go to where you want to save/have saved your notebooks and execute this\ncommand:\n\n.. code:: console\n\n   jupyter notebook\n\nThis will open up the jupyter hub and will allow you to work on your notebooks.\n\nIt is also possible to develop with both GeoPySpark and GeoNotebook in editable mode.\nTo do so you will need to re-install and re-register GeoNotebook with Jupyter.\n\n.. code:: console\n\n   pip uninstall geonotebook\n   git clone --branch feature/geotrellis https://github.com/geotrellis/geonotebook ~/geonotebook\n   pip install -r ~/geonotebook/prerequirements.txt\n   pip install -r ~/geonotebook/requirements.txt\n   pip install -e ~/geonotebook\n   jupyter serverextension enable --py geonotebook\n   jupyter nbextension enable --py geonotebook\n   make notebook\n\nThe default ``Geonotebook (Python 3)`` kernel will require the following environment variables to be defined:\n\n.. code:: console\n\n   export PYSPARK_PYTHON=\"/usr/local/bin/python3\"\n   export SPARK_HOME=\"/usr/local/apache-spark/2.1.1/libexec\"\n   export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip\"\n\nMake sure to define them to values that are correct for your system.\nThe ``make notebook`` command also makes used of ``PYSPARK_SUBMIT_ARGS`` variable defined in the ``Makefile``.\n\nGeoNotebook/GeoTrellis integration in currently in active development and not part of GeoNotebook master.\nThe latest development is on a ``feature/geotrellis`` branch at ``<https://github.com/geotrellis/geonotebook>``.\n\nSide Note For Developers\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn optional (but recomended!) step for developers is to place these\ntwo lines of code at the top of your notebooks.\n\n.. code:: console\n\n   %load_ext autoreload\n   %autoreload 2\n\nThis will make it so that you don't have to leave the notebook for your changes\nto take affect. Rather, you just have to reimport the module and it will be\nupdated. However, there are a few caveats when using ``autoreload`` that can be\nread `here <http://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats>`_.\n\nUsing ``pip install -e`` in conjunction with ``autoreload`` should cover any\nchanges made, though, and will make the development experience much less\npainful.\n\nGeoPySpark Script\n-----------------\n\nWhen GeoPySpark is installed, it comes with a script which can be accessed\nfrom anywhere on you computer. These are the commands that can be ran via the\nscript:\n\n.. code:: console\n\n   geopyspark install-jar -p, --path [download/path] //downloads the jar file\n   geopyspark jar-path //returns the relative path of the jar file\n   geopyspark jar-path -a, --absolute //returns the absolute path of the jar file\n\nThe first command is only needed when installing GeoPySpark through ``pip``;\nand it **must** be ran before using GeoPySpark. If no path is selected, then\nthe jar will be installed wherever GeoPySpark was installed.\n\nThe second and third commands are for getting the location of the jar file.\nThese can be used regardless of installation method. However, if installed\nthrough ``pip``, then the jar must be downloaded first or these commands\nwill not work.\n\nMake Targets\n^^^^^^^^^^^^\n\n - **install** - install GeoPySpark python package locally\n - **wheel** - build python GeoPySpark wheel for distribution\n - **pyspark** - start pyspark shell with project jars\n - **build** - builds the backend jar and moves it to the jars sub-package\n - **docker-build** - build docker image for Jupyter with GeoPySpark\n - **clean** - remove the wheel, the backend jar file, and clean the\n   geotrellis-backend directory\n - **cleaner** - the same as **clean**, but also erase all .pyc\n   files and delete binary artifacts in the docker directory\n\nDocker Container\n^^^^^^^^^^^^^^^^\n\nTo build the docker container, type the following in a terminal:\n\n.. code:: console\n\n   make docker-build\n\nIf you encounter problems, typing ``make cleaner`` before typing\n``make docker-build`` could help.\n\nTo run the container, type:\n\n.. code:: console\n\n   docker run -it --rm -p 8000:8000 quay.io/geodocker/jupyter-geopyspark:6\n\nUninstalling\n------------\n\nTo uninstall GeoPySpark, run the following in the terminal:\n\n.. code:: console\n\n   pip uninstall geopyspark\n   rm .local/bin/geopyspark\n\nContributing\n------------\n\nAny kind of feedback and contributions to GeoPySpark is always welcomed.\nA CLA is required for contribution, see `Contributing <docs/contributing.rst>`_ for more\ninformation.\n\n\n",
    "docs_url": null,
    "download_url": "http://github.com/locationtech-labs/geopyspark",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "",
    "keywords": "",
    "license": "LICENSE",
    "maintainer": "",
    "maintainer_email": "",
    "name": "geopyspark",
    "platform": "",
    "project_url": "https://pypi.org/project/geopyspark/",
    "release_url": "https://pypi.org/project/geopyspark/0.1.0/",
    "requires_dist": [
      "shapely (>=1.6b3)",
      "numpy (>=1.8)",
      "fastavro (>=0.13.0)",
      "bitstring (>=3.1.5)"
    ],
    "requires_python": "",
    "summary": "Python bindings for GeoTrellis and GeoMesa",
    "version": "0.1.0"
  },
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "50ca605618ff793899de93c30b7093a3",
          "sha256": "0c36c510b4838c8b94143c925869ebbeec4fbaf358fc40361fbde6e3ff437764"
        },
        "downloads": 0,
        "filename": "geopyspark-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "50ca605618ff793899de93c30b7093a3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 72122,
        "upload_time": "2017-05-24T22:00:40",
        "url": "https://files.pythonhosted.org/packages/49/fd/9ea6934fd215a2a3a49e980163b1a3c76c0ce7bfa629b79d2b3a07e65845/geopyspark-0.1.0-py3-none-any.whl"
      }
    ],
    "0.2.0rc1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "06d3a918abb0cf1697a16a18f4f3668b",
          "sha256": "a87e8a45cea7ea59d469c4bdd21d5b45cdc0e7e03f0f8b0ef3c7c3e75269d6b5"
        },
        "downloads": 0,
        "filename": "geopyspark-0.2.0rc1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "06d3a918abb0cf1697a16a18f4f3668b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 101022,
        "upload_time": "2017-07-25T16:48:05",
        "url": "https://files.pythonhosted.org/packages/42/d8/6d55d5fa19f5e79e54b67f093e1f5b075fdbec30d16af9a944bbeb23c1fc/geopyspark-0.2.0rc1-py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "50ca605618ff793899de93c30b7093a3",
        "sha256": "0c36c510b4838c8b94143c925869ebbeec4fbaf358fc40361fbde6e3ff437764"
      },
      "downloads": 0,
      "filename": "geopyspark-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "50ca605618ff793899de93c30b7093a3",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 72122,
      "upload_time": "2017-05-24T22:00:40",
      "url": "https://files.pythonhosted.org/packages/49/fd/9ea6934fd215a2a3a49e980163b1a3c76c0ce7bfa629b79d2b3a07e65845/geopyspark-0.1.0-py3-none-any.whl"
    }
  ]
}