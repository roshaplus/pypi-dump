{
  "info": {
    "author": "Peter Organisciak",
    "author_email": "organisciak@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: University of Illinois/NCSA Open Source License",
      "Natural Language :: English",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.2",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "\nHTRC-Features |Build Status| |PyPI version| |Anaconda-Server Badge|\n===================================================================\n\nTools for working with the `HTRC Extracted Features\ndataset <https://sharc.hathitrust.org/features>`__, a dataset of\npage-level text features extracted from 13.7 million digitized works.\n\nThis library provides a ``FeatureReader`` for parsing files, which are\nhandled as ``Volume`` objects with collections of ``Page`` objects.\nVolumes provide access to metadata (e.g. language), volume-wide feature\ninformation (e.g. token counts), and access to Pages. Pages allow you to\neasily parse page-level features, particularly token lists.\n\nThis library makes heavy use of `Pandas <pandas.pydata.org>`__,\nreturning many data representations as DataFrames. This is the leading\nway of dealing with structured data in Python, so this library doesn't\ntry to reinvent the wheel. Since refactoring around Pandas, the primary\nbenefit of using the HTRC Feature Reader is performance: reading the\njson structures and parsing them is generally faster than custom code.\nYou also get convenient access to common information, such as\ncase-folded token counts or part-of-page specific character counts.\nDetails of the public methods provided by this library can be found in\nthe `HTRC Feature Reader\ndocs <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html>`__.\n\n**Table of Contents**: `Installation <#Installation>`__ \\|\n`Usage <#Usage>`__ \\| `Additional Notes <#Additional-Notes>`__\n\n**Links**: `HTRC Feature Reader\nDocumentation <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html>`__\n\\| `HTRC Extracted Features\nDataset <https://sharc.hathitrust.org/features>`__\n\n**Citation**: Peter Organisciak and Boris Capitanu, \"Text Mining in\nPython through the HTRC Feature Reader,\" *Programming Historian*, (22\nNovember 2016),\nhttp://programminghistorian.org/lessons/text-mining-with-extracted-features.\n\nInstallation\n------------\n\nTo install,\n\n.. code:: bash\n\n        pip install htrc-feature-reader\n\nThat's it! This library is written for Python 2.7 and 3.0+. For Python\nbeginners, you'll need\n`pip <https://pip.pypa.io/en/stable/installing/>`__.\n\nAlternately, if you are using\n`Anaconda <https://www.continuum.io/downloads>`__, you can install with\n\n.. code:: bash\n\n        conda install -c htrc htrc-feature-reader\n\nThe ``conda`` approach is recommended, because it makes sure that some\nof the hard-to-install dependencies are properly installed.\n\nGiven the nature of data analysis, using iPython with Jupyter notebooks\nfor preparing your scripts interactively is a recommended convenience.\nMost basically, it can be installed with\n``pip install ipython[notebook]`` and run with ``ipython notebook`` from\nthe command line, which starts a session that you can access through\nyour browser. If this doesn't work, consult the `iPython\ndocumentation <http://ipython.readthedocs.org/>`__.\n\nOptional: `installing the development\nversion <#Installing-the-development-version>`__.\n\n.. |Build Status| image:: https://travis-ci.org/htrc/htrc-feature-reader.svg?branch=master\n   :target: https://travis-ci.org/htrc/htrc-feature-reader\n.. |PyPI version| image:: https://badge.fury.io/py/htrc-feature-reader.svg\n   :target: https://badge.fury.io/py/htrc-feature-reader\n.. |Anaconda-Server Badge| image:: https://anaconda.org/htrc/htrc-feature-reader/badges/installer/conda.svg\n   :target: https://anaconda.org/htrc/htrc-feature-reader\n\nUsage\n-----\n\n*Note: for new Python users, a more in-depth lesson is published by\nProgramming Historian: `Text Mining in Python through the HTRC Feature\nReader <http://programminghistorian.org/lessons/text-mining-with-extracted-features>`__.\nThat lesson is also the official citation associated the HTRC Feature\nReader library.*\n\nReading feature files\n~~~~~~~~~~~~~~~~~~~~~\n\nThe easiest way to start using this library is to use the\n`FeatureReader <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.FeatureReader>`__\ninterface, which takes a list of paths.\n\n.. code:: python\n\n    import glob\n    import pandas as pd\n    from htrc_features import FeatureReader\n    paths = glob.glob('data/PZ-volumes/*.json.bz2')\n    # Here we're loading five paths, for brevity\n    fr = FeatureReader(paths[:5])\n    for vol in fr.volumes():\n        print(\"%s - %s\" % (vol.id, vol.title))\n\n\n.. parsed-literal::\n\n    hvd.32044010273894 - The ballet dancer, and On guard,\n    njp.32101068970662 - Seven years, and other tales / by Julia Kavanagh.\n    nyp.33433074811310 - June / by Edith Barnard Delano ; with illustrations.\n    nyp.33433075749246 - You never know your luck; being the story of a matrimonial deserter, by Gilbert Parker ... illustrated by W.L. Jacobs.\n    mdp.39015028036104 - Russian short stories, ed. for school use,\n\n\nIterating on ``FeatureReader`` returns ``Volume`` objects. This is\nsimply an easy way to access ``feature_reader.volumes()``. Wherever\npossible, this library tries not to hold things in memory, so most of\nthe time you want to iterate rather than casting to a list. In addition\nto memory issues, since each volume needs to be read from a file and\ninitialized, it will be slow. *Woe to whomever tries\n``list(FeatureReader.volumes())``*.\n\nThe method for creating a path list with 'glob' is just one way to do\nso. For large sets, it's better to just have a text file of your paths,\nand read it line by line.\n\nThe feature reader also has a useful method,\n``multiprocessing(map_func)``, for chunking a running functions across\nmultiple processes. This is an advanced feature, but extremely helpful\nfor any large-scale processing.\n\nIn addition to iterating on ``feature_reader.volumes()``, there is a\nconvenient function to grab the first volume in a feature reader. This\nhelps in testing code, and is what we'll do to continue this\nintroduction:\n\n.. code:: python\n\n    vol = fr.first()\n    vol\n\n\n\n\n.. parsed-literal::\n\n    <htrc_features.feature_reader.Volume at 0x1d2ffc52240>\n\n\n\nVolume\n~~~~~~\n\nA\n`Volume <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume>`__\ncontains information about the current work and access to the pages of\nthe work. All the metadata fields from the HTRC JSON file are accessible\nas properties of the volume object, including *title*, *language*,\n*imprint*, *oclc*, *pubDate*, and *genre*. The main identifier *id* and\n*pageCount* are also accessible, and you can find the URL for the Full\nView of the text in the HathiTrust Digital Library - if it exists - with\n``vol.handle_url``.\n\n.. code:: python\n\n    \"Volume %s is a %s page text written in %s. You can doublecheck at %s\" % (vol.id, vol.page_count,\n                                                                              vol.language, vol.handle_url)\n\n\n\n\n.. parsed-literal::\n\n    'Volume hvd.32044010273894 is a 284 page text written in eng. You can doublecheck at http://hdl.handle.net/2027/hvd.32044010273894'\n\n\n\nAs a convenience, ``Volume.year`` returns ``Volume.pub_date``:\n\n.. code:: python\n\n    \"%s == %s\" % (vol.pub_date, vol.year)\n\n\n\n\n.. parsed-literal::\n\n    '1901 == 1901'\n\n\n\n``Volume`` objects have an page genrator method for pages, through\n``Volume.pages()``. Iterating through pages using this generator only\nkeeps one page at a time in memory, and again it is preferable to\nreading all the pages into the list at once. Unlike volumes, your\ncomputer can probably hold all the pages of a single volume in memory,\nso it is not dire if you try to read them into a list.\n\nLike with the ``FeatureReader``, you can also access the page generator\nby iterating directly on the object (i.e. ``for page in vol``). Python\nbeginners may find that using ``vol.pages()`` is more clear as to what\nis happening.\n\n.. code:: python\n\n    # Let's skip ahead some pages\n    i = 0\n    for page in vol:\n        # Same as `for page in vol.pages()`\n        i += 1\n        if i >= 16:\n            break\n    print(page)\n\n\n.. parsed-literal::\n\n    <page 00000016 of volume hvd.32044010273894>\n\n\nIf you want to pass arguments to page initialization, such as changing\nthe page's default section from 'body' to 'group' (which returns\nheader+footer+body), it can be done with\n``for page in vol.pages(default_section='group')``.\n\nFinally, if the minimal metadata included with the extracted feature\nfiles is insufficient, you can fetch the HTRC's metadata record from the\nSolr Proxy with ``vol.metadata``. Remember that this calls the HTRC\nservers for each volume, so can add considerable overhead.\n\n.. code:: python\n\n    for vol in fr.volumes():\n        print(vol.metadata['published'][0])\n\n\n.. parsed-literal::\n\n    New York, and London, Harper & brothers, 1901\n    London : Hurst and Blackett, 1860\n    Boston ; New York : Houghton Mifflin Company, 1916 (Cambridge : The Riverside Press)\n    New York, George H. Doran Company [1914]\n    Chicago, New York, Scott, Foresman and company [c1919]\n\n\n.. code:: python\n\n    print(\"METADATA FIELDS: \" + \", \".join(vol.metadata.keys()))\n\n\n.. parsed-literal::\n\n    METADATA FIELDS: _version_, htrc_charCount, title, htrc_volumePageCountBin, publishDate, title_a, mainauthor, author_only, oclc, authorSort, country_of_pub, author, htrc_gender, language, ht_id, publisher, author_top, publishDateRange, htrc_pageCount, title_top, callnosort, publication_place, topic, htsource, htrc_wordCount, title_ab, callnumber, fullrecord, htrc_volumeWordCountBin, format, lccn, genre, htrc_genderMale, topic_subject, topicStr, geographic, published, sdrnum, id\n\n\n*At large-scales, using ``vol.metadata`` is an impolite and inefficient\namount of server pinging; there are better ways to query the API than\none volume at a time. Read about the `HTRC Solr\nProxy <https://wiki.htrc.illinois.edu/display/COM/Solr+Proxy+API+User+Guide>`__.*\n\nAnother source of bibliographic metadata is the HathiTrust Bib API. You\ncan access this information through the URL returned with\n``vol.ht_bib_url``:\n\n.. code:: python\n\n    vol.ht_bib_url\n\n\n\n\n.. parsed-literal::\n\n    'http://catalog.hathitrust.org/api/volumes/full/htid/mdp.39015028036104.json'\n\n\n\nVolumes also have direct access to volume-wide info of features stored\nin pages. For example, you can get a list of words per page through\n`Volume.tokens\\_per\\_page() <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume.tokens_per_page>`__.\nWe'll discuss these features `below <#Volume-stats-collecting>`__, after\nlooking first at Pages.\n\nPages\n-----\n\nA page contains the meat of the HTRC's extracted features, including\ninformation for:\n\n-  Part of speech tagged token counts, through ``Page.tokenlist()``\n-  Counts of the characters occurred at the start and end of physical\n   lines, though ``Page.lineCounts()``\n-  Sentence counts, line counts (referring to the physical line on the\n   page)\n-  And more, seen in the docs for\n   `Page <http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Page>`__\n\n.. code:: python\n\n    print(\"The body has %s lines, %s empty lines, and %s sentences\" % (page.line_count(),\n                                                                       page.empty_line_count(),\n                                                                       page.sentence_count()))\n\n\n.. parsed-literal::\n\n    The body has 30 lines, 0 empty lines, and 9 sentences\n\n\nSince the HTRC provides information by header/body/footer, most methods\ntake a ``section=`` argument. If not specified, this defaults to\n``\"body\"``, or whatever argument is supplied to\n``Page.default_section``.\n\n.. code:: python\n\n    print(\"%s tokens in the default section, %s\" % (page.token_count(), page.default_section))\n    print(\"%s tokens in the header\" % (page.token_count(section='header')))\n    print(\"%s tokens in the footer\" % (page.token_count(section='footer')))\n\n\n.. parsed-literal::\n\n    294 tokens in the default section, body\n    3 tokens in the header\n    0 tokens in the footer\n\n\nThere are also two special arguments that can be given to ``section``:\n``\"all\"`` and \"``group``\". 'all' returns information for each section\nseparately, when appropriate, while 'group' returns information for all\nheader, body, and footer combined.\n\n.. code:: python\n\n    print(\"%s tokens on the full page\" % (page.token_count(section='group')))\n    assert(page.token_count(section='group') == (page.token_count(section='header') +\n                                                 page.token_count(section='body') + \n                                                 page.token_count(section='footer')))\n\n\n.. parsed-literal::\n\n    297 tokens on the full page\n\n\nNote that for the most part, the properties of the ``Page`` and\n``Volume`` objects aligns with the names in the HTRC Extracted Features\nschema, except they are converted to follow `Python naming\nconventions <https://google.github.io/styleguide/pyguide.html?showone=Naming#Naming>`__:\nconverting the ``CamelCase`` of the schema to\n``lowercase_with_underscores``. E.g. ``beginLineChars`` from the HTRC\ndata is accessible as ``Page.begin_line_chars``.\n\nThe fun stuff: playing with token counts and character counts\n-------------------------------------------------------------\n\nToken counts are returned by ``Page.tokenlist()``. By default,\npart-of-speech tagged, case-sensitive counts are returned for the body.\n\nThe token count information is returned as a DataFrame with a MultiIndex\n(page, section, token, and part of speech) and one column (count).\n\n.. code:: python\n\n    print(page.tokenlist()[:3])\n\n\n.. parsed-literal::\n\n                               count\n    page section token    pos       \n    16   body    !        .        1\n                 '        ''       1\n                 'Flowers NNS      1\n\n\n``Page.tokenlist()`` can be manipulated in various ways. You can\ncase-fold, for example:\n\n.. code:: python\n\n    df = page.tokenlist(case=False)\n    print(df[15:18])\n\n\n.. parsed-literal::\n\n                                count\n    page section lowercase pos       \n    16   body    ancient   JJ       1\n                 and       CC      12\n                 any       DT       1\n\n\nOr, you can combine part of speech counts into a single integer.\n\n.. code:: python\n\n    df = page.tokenlist(pos=False)\n    print(df[15:18])\n\n\n.. parsed-literal::\n\n                           count\n    page section token          \n    16   body    Naples        1\n                 November      1\n                 October       1\n\n\nSection arguments are valid here: 'header', 'body', 'footer', 'all', and\n'group'\n\n.. code:: python\n\n    df = page.tokenlist(section=\"header\", case=False, pos=False)\n    print(df)\n\n\n.. parsed-literal::\n\n                            count\n    page section lowercase       \n    16   header  ballet         1\n                 dancer         1\n                 the            1\n\n\nThe MultiIndex makes it easy to slice the results, and it is althogether\nmore memory-efficient. If you are new to Pandas DataFrames, you might\nfind it easier to learn by converting the index to columns.\n\n.. code:: python\n\n    df = page.tokenlist()\n    # Slicing on Multiindex: get all Signular or Mass Nouns (NN)\n    idx = pd.IndexSlice\n    nouns = df.loc[idx[:,:,:,'NN'],]\n    print(nouns[:3])\n    print(\"With index reset: \")\n    print(nouns.reset_index()[:2])\n\n\n.. parsed-literal::\n\n                                   count\n    page section token        pos       \n    16   body    benefactress NN       1\n                 bitterness   NN       1\n                 case         NN       1\n    With index reset: \n       page section         token pos  count\n    0    16    body  benefactress  NN      1\n    1    16    body    bitterness  NN      1\n\n\nIf you prefer not to use Pandas, you can always convert the object, with\nmethods like ``to_dict`` and ``to_csv``).\n\n.. code:: python\n\n    df[:3].to_dict()\n\n\n\n\n.. parsed-literal::\n\n    {'count': {(16, 'body', '!', '.'): 1,\n      (16, 'body', \"'\", \"''\"): 1,\n      (16, 'body', \"'Flowers\", 'NNS'): 1}}\n\n\n\nTo get just the unique tokens, ``Page.tokens`` provides them as a list.\n\n.. code:: python\n\n    page.tokens()[:7]\n\n\n\n\n.. parsed-literal::\n\n    ['!', \"'\", \"'Flowers\", \"'s\", ',', '.', '6']\n\n\n\nIn addition to token lists, you can also access\n``Page.begin_line_chars`` and ``Section.end_line_chars``, which are\nDataFrames of character counts that occur at the start or end of a line.\n\nVolume stats collecting\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThe Volume object has a number of methods for collecting information\nfrom all its pages.\n\n``Volume.tokenlist()`` works identically the page tokenlist method,\nexcept it returns information for the full volume:\n\n.. code:: python\n\n    # Print case-insensitive occurrances of the word `she`\n    all_vol_token_counts = vol.tokenlist(pos=False, case=False)\n    print(all_vol_token_counts.loc[idx[:,'body', 'she'],][:3])\n\n\n.. parsed-literal::\n\n                            count\n    page section lowercase       \n    38   body    she            1\n    39   body    she            1\n    42   body    she            1\n\n\nNote that a Volume-wide tokenlist is not crunched until you need it,\nthen it will stay cached in case you need it. If you try to access\n``Page.tokenlist()`` *after* accessing ``Volume.tokenlist()``, the Page\nobject will return that page from the Volume's cached representation,\nrather than preparing it itself.\n\n``Volume.tokens()``, and ``Volume.tokens_per_page()`` give easy access\nto the full vocabulary of the volume, and the token counts per page.\n\n.. code:: python\n\n    vol.tokens()[:10]\n\n\n\n\n.. parsed-literal::\n\n    ['\"', '.', ':', 'Fred', 'Newton', 'Scott', 'gift', 'i', 'ii', 'iiiiISI']\n\n\n\nIf you prefer a DataFrame structured like a term-document matrix (where\npages are the 'documents'), ``vol.term_page_freqs()`` will return it.\n\nBy default, this returns a page-frequency rather than term-frequency,\nwhich is to say it counts ``1`` when a term occurs on a page, regardless\nof how much it occurs on that page. For a term frequency, pass\n``page_freq=False``.\n\n.. code:: python\n\n    a = vol.term_page_freqs()\n    print(a.loc[10:11,['the','and','is','he', 'she']])\n    a = vol.term_page_freqs(page_freq=False)\n    print(a.loc[10:11,['the','and','is', 'he', 'she']])\n\n\n.. parsed-literal::\n\n    token  the  and   is   he  she\n    page                          \n    10     0.0  1.0  0.0  0.0  0.0\n    11     1.0  1.0  1.0  0.0  0.0\n    token   the  and   is   he  she\n    page                           \n    10      0.0  1.0  0.0  0.0  0.0\n    11     22.0  7.0  4.0  0.0  0.0\n\n\nVolume.term\\_page\\_freqs provides a wide DataFrame resembling a matrix,\nwhere terms are listed as columns, pages are listed as rows, and the\nvalues correspond to the term frequency (or page page frequency with\n``page_freq=true``). Volume.term\\_volume\\_freqs() simply sums these.\n\nMultiprocessing\n~~~~~~~~~~~~~~~\n\nFor faster processing, you can write a mapping function for acting on\nvolumes, then pass it to ``FeatureReader.multiprocessing``. This sends\nout the function to a different process per volume, spawning\n(CPU\\_CORES-1) processes at a time. The map function receives the\nfeature\\_reader and a volume path as a tuple, and needs to initialize\nthe volume.\n\nHere's a simple example that returns the term counts for each volume\n(take note of the first two lines of the function):\n\n.. code:: python\n\n    def printTokenList(args):\n        fr, path = args\n        vol = fr.create_volume(path)\n        return ('tokens', vol.tokens)\n\n    fr  = FeatureReader(paths)\n    all_tokens = []\n    mapper = fr.multiprocessing(printTokenList)\n    for key, result in mapper:\n        all_tokens = all_tokens + result\n    set(all_tokens)\n\nSome rules: results must be serializeable, and the map\\_func must be\naccessible from **main** (basically: no dynamic functions: they should\nbe written plainly in your script).\n\nThe results are collected and returned together, so you don't want a\nfeature reader with all 4.8 million files, because the results will be\ntoo much memory (depending on how big your result is). Instead, it\neasier to initialize feature readers for smaller batches.\n\nGNU Parallel\n^^^^^^^^^^^^\n\nAs an alternative to multiprocessing in Python, my preference is to have\nsimpler Python scripts and to use GNU Parallel on the command line. To\ndo this, you can set up your Python script to take variable length\narguments of feature file paths, and to print to stdout.\n\nThis psuedo-code shows how that you'd use parallel, where the number of\nparallel processes is 90% the number of cores, and 50 paths are sent to\nthe script at a time (if you send too little at a time, the\ninitialization time of the script can add up).\n\n.. code:: bash\n\n    find feature-files/ -name '*json.bz2' | parallel --eta --jobs 90% -n 50 python your_script.py >output.txt\n\nAdditional Notes\n----------------\n\nInstalling the development version\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n    git clone https://github.com/htrc/htrc-feature-reader.git\n    cd htrc-feature-reader\n    python setup.py install\n\nIterating through the JSON files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you need to do fast, highly customized processing without\ninstantiating Volumes, FeatureReader has a convenient generator for\ngetting the raw JSON as a Python dict: ``fr.jsons()``. This simply does\nthe file reading, optional decompression, and JSON parsing.\n\nDownloading files within the library\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``utils`` includes an Rsyncing utility, ``download_file``. This requires\nRsync to be installed on your system.\n\n**Usage:**\n\nDownload one file to the current directory:\n\n::\n\n    utils.download_file(htids='nyp.33433042068894')\n\nDownload multiple files to the current directory:\n\n::\n\n    ids = ['nyp.33433042068894', 'nyp.33433074943592', 'nyp.33433074943600']\n    utils.download_file(htids=ids)\n\nDownload file to ``/tmp``:\n\n::\n\n    utils.download_file(htids='nyp.33433042068894', outdir='/tmp')\n\nDownload file to current directory, keeping pairtree directory\nstructure, i.e.\n``./nyp/pairtree_root/33/43/30/42/06/88/94/33433042068894/nyp.33433042068894.json.bz2``:\n\n``utils.download_file(htids='nyp.33433042068894', keep_dirs=True)``\n\nGetting the Rsync URL\n~~~~~~~~~~~~~~~~~~~~~\n\nIf you have a HathiTrust Volume ID and want to be able to download the\nfeatures for a specific book, ``hrtc_features.utils`` contains an\n`id\\_to\\_rsync <http://htrc.github.io/htrc-feature-reader/htrc_features/utils.m.html#htrc_features.utils.id_to_rsync>`__\nfunction. This uses the `pairtree <http://pythonhosted.org/Pairtree/>`__\nlibrary but has a fallback written with that library is not installed,\nsince it isn't compatible with Python 3.\n\n.. code:: python\n\n    from htrc_features import utils\n    utils.id_to_rsync('miun.adx6300.0001.001')\n\n\n\n\n.. parsed-literal::\n\n    'miun/pairtree_root/ad/x6/30/0,/00/01/,0/01/adx6300,0001,001/miun.adx6300,0001,001.json.bz2'\n\n\n\nSee the `ID to Rsync notebook <examples/ID_to_Rsync_Link.ipynb>`__ for\nmore information on this format and on Rsyncing lists of urls.\n\nThere is also a command line utility installed with the HTRC Feature\nReader:\n\n.. code:: bash\n\n    $ htid2rsync miun.adx6300.0001.001\n    miun/pairtree_root/ad/x6/30/0,/00/01/,0/01/adx6300,0001,001/miun.adx6300,0001,001.json.bz2\n\nAdvanced Features\n~~~~~~~~~~~~~~~~~\n\nIn the beta Extracted Features release, schema 2.0, a few features were\nseparated out to an advanced files. However, *this designation is no\nlonger present starting with schema 3.0*, meaning information like\n``beginLineChars``, ``endLineChars``, and ``capAlphaSeq`` are always\navailable:\n\n.. code:: python\n\n    # What is the longest sequence of capital letter on each page?\n    vol.cap_alpha_seqs()[:10]\n\n\n\n\n.. parsed-literal::\n\n    [0, 1, 0, 0, 0, 0, 0, 0, 4, 1]\n\n\n\n.. code:: python\n\n    end_line_chars = vol.end_line_chars()\n    print(end_line_chars.head())\n\n\n.. parsed-literal::\n\n                             count\n    page section place char       \n    2    body    end   -         1\n                       :         1\n                       I         1\n                       f         1\n                       t         1\n\n\n.. code:: python\n\n    # Find pages that have lines ending with \"!\"\n    idx = pd.IndexSlice\n    print(end_line_chars.loc[idx[:,:,:,'!'],].head())\n\n\n.. parsed-literal::\n\n                             count\n    page section place char       \n    45   body    end   !         1\n    75   body    end   !         1\n    77   body    end   !         1\n    91   body    end   !         1\n    92   body    end   !         1\n\n\nTesting\n~~~~~~~\n\nThis library is meant to be compatible with Python 3.2+ and Python 2.7+.\nTests are written for py.test and can be run with ``setup.py test``, or\ndirectly with ``python -m py.test -v``.\n\nIf you find a bug, leave an issue on the issue tracker, or contact Peter\nOrganisciak at ``organisciak+htrc@gmail.com``.\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/organisciak/htrc-feature-reader",
    "keywords": "hathitrust text-mining text-analysis features",
    "license": "NCSA",
    "maintainer": "",
    "maintainer_email": "",
    "name": "htrc-feature-reader",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/htrc-feature-reader/",
    "release_url": "https://pypi.org/project/htrc-feature-reader/1.92/",
    "requires_python": "",
    "summary": "Library for working with the HTRC Extracted Features dataset",
    "version": "1.92"
  },
  "releases": {
    "1.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f4424fbe3f0b80915d6a30faa4d8ca2e",
          "sha256": "fa245a8a986350985d9cefddfe90e8f31ba30a515a7f67f92c21cb16be29b79b"
        },
        "downloads": 765,
        "filename": "htrc-feature-reader-1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f4424fbe3f0b80915d6a30faa4d8ca2e",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8526,
        "upload_time": "2015-12-23T20:24:17",
        "url": "https://files.pythonhosted.org/packages/1c/49/5164365a4c21d773a860a1a79e06b3aa0f1da92eb4d81038a3666723ad32/htrc-feature-reader-1.3.tar.gz"
      }
    ],
    "1.41": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4a1a335ae8640c09ba27c28beede28dc",
          "sha256": "ab7fce29cdf694ee1a131056c8c154941209b04ec8869823024a80b31b8ef24d"
        },
        "downloads": 365,
        "filename": "htrc-feature-reader-1.41.tar.gz",
        "has_sig": false,
        "md5_digest": "4a1a335ae8640c09ba27c28beede28dc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8703,
        "upload_time": "2016-01-12T19:05:11",
        "url": "https://files.pythonhosted.org/packages/8c/58/f8a4ed6f981051e7b6738cd45ab85503cd0ecacca9b49938d5162faaaa40/htrc-feature-reader-1.41.tar.gz"
      }
    ],
    "1.42": [
      {
        "comment_text": "",
        "digests": {
          "md5": "74c87f4b6788d8a76e228acb21944814",
          "sha256": "6a2dfbd3f1083bab08958fce952056e850220346bb55c585df5fc3ca7dc58c1a"
        },
        "downloads": 126,
        "filename": "htrc_feature_reader-1.42-py2.7.egg",
        "has_sig": false,
        "md5_digest": "74c87f4b6788d8a76e228acb21944814",
        "packagetype": "bdist_egg",
        "python_version": "2.7",
        "size": 19312,
        "upload_time": "2016-03-30T18:20:35",
        "url": "https://files.pythonhosted.org/packages/3b/e6/69a6f3efb5c5bc0c268355e385d6688d47783169e347417cf459799f4871/htrc_feature_reader-1.42-py2.7.egg"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "f9382eebead4bc3e7dddbe61d880c68e",
          "sha256": "1d5386d23a08390939001be4a02849b9832a5aa40094f0f74bb071f32052b1d0"
        },
        "downloads": 124,
        "filename": "htrc_feature_reader-1.42-py3.4.egg",
        "has_sig": false,
        "md5_digest": "f9382eebead4bc3e7dddbe61d880c68e",
        "packagetype": "bdist_egg",
        "python_version": "3.4",
        "size": 19615,
        "upload_time": "2016-03-30T18:20:47",
        "url": "https://files.pythonhosted.org/packages/b2/e6/fc22ed2e91dbaecfc552b011c8ae18d98e026c8ee98c3a96a5fb501f7764/htrc_feature_reader-1.42-py3.4.egg"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "143d5e3b7e07b612d36d96f19bd49c42",
          "sha256": "2ad0f0a674f25c9f97deae0495728aace0f79a813d060fa74957da22aa8c934f"
        },
        "downloads": 126,
        "filename": "htrc-feature-reader-1.42.tar.gz",
        "has_sig": false,
        "md5_digest": "143d5e3b7e07b612d36d96f19bd49c42",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 8235,
        "upload_time": "2016-02-23T22:39:37",
        "url": "https://files.pythonhosted.org/packages/ec/1e/f5717e065a1d365c2d66138d4aa1aec134eb8a779527ec8c2b7762f83815/htrc-feature-reader-1.42.tar.gz"
      }
    ],
    "1.43": [
      {
        "comment_text": "",
        "digests": {
          "md5": "d2339e51ce54394dc4a74cc45f84be25",
          "sha256": "d42fafccc4053123bbcde1877a45ed2b51e8c68645a56b3ec760c6cab6d26468"
        },
        "downloads": 151,
        "filename": "htrc-feature-reader-1.43.tar.gz",
        "has_sig": false,
        "md5_digest": "d2339e51ce54394dc4a74cc45f84be25",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15826,
        "upload_time": "2016-03-30T18:22:20",
        "url": "https://files.pythonhosted.org/packages/9e/63/ab651ad3a7b5542536c43a6e6dcd92d89efe13cc58b56b64006a7dc943a7/htrc-feature-reader-1.43.tar.gz"
      }
    ],
    "1.45": [
      {
        "comment_text": "",
        "digests": {
          "md5": "7deaf03e757de09db7cdb6ba91324d90",
          "sha256": "ef09261d85e47b81d3fc2e305358bbbbda13e197d9f27dbd9c52e39b0c3295b2"
        },
        "downloads": 128,
        "filename": "htrc-feature-reader-1.45.tar.gz",
        "has_sig": false,
        "md5_digest": "7deaf03e757de09db7cdb6ba91324d90",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16157,
        "upload_time": "2016-04-19T16:38:16",
        "url": "https://files.pythonhosted.org/packages/8f/58/eb38aabbd69f8e4bd65364cb23ad3146a441cd8625ea45ad86193c4ad38e/htrc-feature-reader-1.45.tar.gz"
      }
    ],
    "1.50": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0f1a573232aae290f4f26f02b5d460b1",
          "sha256": "36143abc3da6f89cf6422eb835cf156d2f2f17218e982ca07949cd55458ccf38"
        },
        "downloads": 153,
        "filename": "htrc-feature-reader-1.50.tar.gz",
        "has_sig": false,
        "md5_digest": "0f1a573232aae290f4f26f02b5d460b1",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16247,
        "upload_time": "2016-06-04T00:10:52",
        "url": "https://files.pythonhosted.org/packages/58/d0/d783e1587ad8721b901170a95b777c4e07155f6b498388bc54d35dc63f28/htrc-feature-reader-1.50.tar.gz"
      }
    ],
    "1.70": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f37f2f5f72b320411231ca8bacf697dc",
          "sha256": "55c0b53c9e396aa1aa8f15e56d130f2c75ac619f8963069bdae2626a833744f7"
        },
        "downloads": 381,
        "filename": "htrc-feature-reader-1.70.tar.gz",
        "has_sig": false,
        "md5_digest": "f37f2f5f72b320411231ca8bacf697dc",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17034,
        "upload_time": "2016-08-24T20:03:38",
        "url": "https://files.pythonhosted.org/packages/98/f6/fb8e470048227651faf3a949734ddfe098f06e70f7198bceffe8687bbf66/htrc-feature-reader-1.70.tar.gz"
      }
    ],
    "1.80": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1995895b7eeab5ce07ed4113c661f2cd",
          "sha256": "1be7de6f1e26748ad6b3e27c78956e055f86707567d85a7ce777e3d53b9e8c8d"
        },
        "downloads": 55,
        "filename": "htrc-feature-reader-1.80.tar.gz",
        "has_sig": false,
        "md5_digest": "1995895b7eeab5ce07ed4113c661f2cd",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17838,
        "upload_time": "2016-11-08T00:27:53",
        "url": "https://files.pythonhosted.org/packages/8f/0b/2c26f144b57d8ccf7a0ddc857509f1de8c25d1237016f4428c21d6c0400d/htrc-feature-reader-1.80.tar.gz"
      }
    ],
    "1.81": [
      {
        "comment_text": "",
        "digests": {
          "md5": "8913668c877723f2d7b6c6eb089dd26a",
          "sha256": "3b9ab1b30ebf37b541dfa582640679fb6a394440859c6092feef9bc302ac1cd6"
        },
        "downloads": 53,
        "filename": "htrc-feature-reader-1.81.tar.gz",
        "has_sig": false,
        "md5_digest": "8913668c877723f2d7b6c6eb089dd26a",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 18097,
        "upload_time": "2016-11-08T17:43:29",
        "url": "https://files.pythonhosted.org/packages/cb/d6/c5618b3417cc4131913479284cb298605f3a5216857cbb9ccfcde5805e12/htrc-feature-reader-1.81.tar.gz"
      }
    ],
    "1.82": [
      {
        "comment_text": "",
        "digests": {
          "md5": "458981943c9585042e621c91a55554c6",
          "sha256": "4e72b20edc8f1734998e5dd3083109f59e8d00d43b0056f95b426f0564e464a4"
        },
        "downloads": 59,
        "filename": "htrc-feature-reader-1.82.tar.gz",
        "has_sig": false,
        "md5_digest": "458981943c9585042e621c91a55554c6",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 18313,
        "upload_time": "2016-12-04T21:13:59",
        "url": "https://files.pythonhosted.org/packages/71/dc/22eef0bd30531dbeec251b801331b82ca09379dac213a496f133252d4a91/htrc-feature-reader-1.82.tar.gz"
      }
    ],
    "1.90": [
      {
        "comment_text": "",
        "digests": {
          "md5": "c2e74301157344ad4a60cff8b6f1283e",
          "sha256": "c96396625376a7eae587b5ec742004a5425584da7e94a8929be21446bd1932da"
        },
        "downloads": 0,
        "filename": "htrc-feature-reader-1.90.tar.gz",
        "has_sig": false,
        "md5_digest": "c2e74301157344ad4a60cff8b6f1283e",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21562,
        "upload_time": "2017-05-10T23:15:33",
        "url": "https://files.pythonhosted.org/packages/58/e4/fca105536c6f8a6f2a7abcbab1f6b3e4f72a33174c094689800434ca3899/htrc-feature-reader-1.90.tar.gz"
      }
    ],
    "1.91": [
      {
        "comment_text": "",
        "digests": {
          "md5": "427f35c61927555f73193c0d85a9c561",
          "sha256": "c9344577c21b00fb55311f3d3164eaae1c92d51a78601b4f3811002a5b0f308f"
        },
        "downloads": 0,
        "filename": "htrc-feature-reader-1.91.tar.gz",
        "has_sig": false,
        "md5_digest": "427f35c61927555f73193c0d85a9c561",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21594,
        "upload_time": "2017-05-10T23:56:57",
        "url": "https://files.pythonhosted.org/packages/1c/e4/befd8b063796ea6efae81097ac8e27ed6fc3759820912b9aa07ea6e6812d/htrc-feature-reader-1.91.tar.gz"
      }
    ],
    "1.92": [
      {
        "comment_text": "",
        "digests": {
          "md5": "8906cd100be953823145e78c5001d21e",
          "sha256": "986eafebce4c07326e7cd1d7e9c6557f93b4c737027d98ad38187f2f2a3a349f"
        },
        "downloads": 0,
        "filename": "htrc-feature-reader-1.92.tar.gz",
        "has_sig": false,
        "md5_digest": "8906cd100be953823145e78c5001d21e",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21595,
        "upload_time": "2017-05-11T19:49:36",
        "url": "https://files.pythonhosted.org/packages/e1/c3/b577f7c7f5b51665132db4488ce5b76257b91486881c22d824366adac405/htrc-feature-reader-1.92.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "8906cd100be953823145e78c5001d21e",
        "sha256": "986eafebce4c07326e7cd1d7e9c6557f93b4c737027d98ad38187f2f2a3a349f"
      },
      "downloads": 0,
      "filename": "htrc-feature-reader-1.92.tar.gz",
      "has_sig": false,
      "md5_digest": "8906cd100be953823145e78c5001d21e",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 21595,
      "upload_time": "2017-05-11T19:49:36",
      "url": "https://files.pythonhosted.org/packages/e1/c3/b577f7c7f5b51665132db4488ce5b76257b91486881c22d824366adac405/htrc-feature-reader-1.92.tar.gz"
    }
  ]
}