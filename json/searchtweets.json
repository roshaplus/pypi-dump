{
  "info": {
    "author": "Fiona Pigott, Jeff Kolb, Josh Montague, Aaron Gonzales",
    "author_email": "agonzales@twitter.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "Python Twitter Search API\n=========================\n\nThis library serves as a python interface to the `Twitter premium and enterprise search APIs <https://developer.twitter.com/en/docs/tweets/search/overview/30-day-search>`_. It provides a command-line utility and a library usable from within python. It comes with tools for assisting in dynamic generation of search rules and for parsing tweets.\n\nPretty docs can be seen `here <https://twitterdev.github.io/search_tweets_api/>`_.\n\n\nFeatures\n========\n\n- Command-line utility is pipeable to other tools (e.g., ``jq``).\n- Automatically handles pagination of results with specifiable limits\n- Delivers a stream of data to the user for low in-memory requirements\n- Handles Enterprise and Premium authentication methods\n- Flexible usage within a python program\n- Compatible with our group's Tweet Parser for rapid extraction of relevant data fields from each tweet payload\n- Supports the Counts API, which can reduce API call usage and provide rapid insights if you only need volumes and not tweet payloads\n\n\n\nInstallation\n============\n\nWe will host the package on PyPi so it's pip-friendly.\n\n.. code:: bash\n\n  pip install searchtweets\n\nOr the development version locally via\n\n.. code:: bash\n\n  git clone https://github.com/twitterdev/search-tweets-python\n  cd search-tweets-python\n  pip install -e .\n\n\n\nUsing the Comand Line Application\n=================================\n\nWe provide a utility, ``search_tweets.py``, in the ``tools`` directory that provides rapid access to tweets.\nPremium customers should use ``--bearer-token``; enterprise customers should use ``--user-name`` and ``--password``.\n\nThe ``--endpoint`` flag will specify the full URL of your connection, e.g.:\n\n\n.. code:: bash\n\n  https://api.twitter.com/1.1/tweets/search/30day/dev.json\n\nYou can find this url in your developer console.\n\nNote that the ``--results-per-call`` flag specifies an argument to the API call ( ``maxResults``, results returned per CALL), not as a hard max to number of results returned from this program. use ``--max-results`` for that for now.\n\n\n\n**Stream json results to stdout without saving**\n\n.. code:: bash\n\n  python search_tweets.py \\\n    --bearer-token <BEARER_TOKEN> \\\n    --endpoint <MY_ENDPOINT> \\\n    --max-results 1000 \\\n    --results-per-call 100 \\\n    --filter-rule \"beyonce has:hashtags\" \\\n    --print-stream\n\n\n**Stream json results to stdout and save to a file**\n\n.. code:: bash\n\n  python search_tweets.py \\\n    --user-name <USERNAME> \\\n    --password <PW> \\\n    --endpoint <MY_ENDPOINT> \\\n    --max-results 1000 \\\n    --results-per-call 100 \\\n    --filter-rule \"beyonce has:hashtags\" \\\n    --filename-prefix beyonce_geo \\\n    --print-stream\n\n\n**Save to file without output**\n\n.. code:: bash\n\n  python search_tweets.py \\\n    --user-name <USERNAME> \\\n    --password <PW> \\\n    --endpoint <MY_ENDPOINT> \\\n    --max-results 100 \\\n    --results-per-call 100 \\\n    --filter-rule \"beyonce has:hashtags\" \\\n    --filename-prefix beyonce_geo \\\n    --no-print-stream\n\n\n\nIt can be far easier to specify your information in a configuration file. An example file can be found in the ``tools/api_config_example.config`` file, but will look something like this:\n\n.. code:: bash\n\n  [credentials]\n  account_name = <account_name>\n  username =  <user_name>\n  password = <password>\n  bearer_token = <token>\n\n  [api_info]\n  endpoint = <endpoint>\n\n  [gnip_search_rules]\n  from_date = 2017-06-01\n  to_date = 2017-09-01\n  results_per_call = 100\n  pt_rule = beyonce has:hashtags\n\n\n  [search_params]\n  max_results = 500\n\n  [output_params]\n  output_file_prefix = beyonce\n\nSoon, we will update this behavior and remove the credentials section from the config file to be handled differently.\n\nWhen using a config file in conjunction with the command-line utility, you need to specify your config file via the ``--config-file`` parameter. Additional command-line arguments will either be *added* to the config file args or **overwrite** the config file args if both are specified and present.\n\n\nExample::\n\n  python search_tweets.py \\\n    --config-file myapiconfig.config \\\n    --no-print-stream\n\n\nUsing the Twitter Search API within Python\n==========================================\n\nWorking with the API within a Python program is straightforward both for\nPremium and Enterprise clients.\n\nOur group's python `tweet parser\nlibrary <https://github.com/twitterdev/tweet_parser>`__ is a\nrequirement.\n\nPrior to starting your program, an easy way to define your secrets will\nbe setting an environment variable. If you are an enterprise client,\nyour authentication will be a (username, password) pair. If you are a\npremium client, you'll need to get a bearer token that will be passed\nwith each call for authentication.\n\nYour credentials should be put into a YAML file that looks like this:\n\n.. code:: yaml\n\n\n    search_tweets_api:\n      endpoint: <FULL_URL_OF_ENDPOINT>\n      account: <ACCOUNT_NAME>\n      username: <USERNAME>\n      password: <PW>\n      bearer_token: <TOKEN>\n\nAnd filling in the keys that are appropriate for your account type.\nPremium users should only have the ``endpoint`` and ``bearer_token``;\nEnterprise customers should have ``account``, ``username``,\n``endpoint``, and ``password``.\n\nOur credential reader expects this file at\n``\"~/.twitter_keys.yaml\"``, but you can pass the relevant location as\nneeded.\n\nThe following cell demonstrates the basic setup that will be referenced\nthroughout your program's session.\n\n.. code:: python\n\n    from searchtweets import ResultStream, gen_rule_payload, load_credentials\n\nEnterprise setup\n----------------\n\nIf you are an enterprise customer, you'll need to authenticate with a\nbasic username/password method. You can specify that here:\n\n.. code:: python\n\n    enterprise_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n                                              account_type=\"enterprise\")\n\nPremium Setup\n-------------\n\nPremium customers will use a bearer token for authentication. Use the\nfollowing cell for setup:\n\n.. code:: python\n\n    premium_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n                                           account_type=\"premium\")\n\nThere is a function that formats search API rules into valid json\nqueries called ``gen_rule_payload``. It has sensible defaults, such as\npulling more tweets per call than the default 100 (but note that a\nsandbox environment can only have a max of 100 here, so if you get\nerrors, please check this) not including dates, and defaulting to hourly\ncounts when using the counts api. Discussing the finer points of\ngenerating search rules is out of scope for these examples; I encourage\nyou to see the docs to learn the nuances within, but for now let's see\nwhat a rule looks like.\n\n.. code:: python\n\n    rule = gen_rule_payload(\"beyonce\", results_per_call=100) # testing with a sandbox account\n    print(rule)\n\n\n.. parsed-literal::\n\n    {\"query\":\"beyonce\",\"maxResults\":100}\n\n\nThis rule will match tweets that have the text ``beyonce`` in them.\n\nFrom this point, there are two ways to interact with the API. There is a\nquick method to collect smaller amounts of tweets to memory that\nrequires less thought and knowledge, and interaction with the\n``ResultStream`` object which will be introduced later.\n\nFast Way\n--------\n\nWe'll use the ``search_args`` variable to power the configuration point\nfor the API. The object also takes a valid PowerTrack rule and has\noptions to cutoff search when hitting limits on both number of tweets\nand API calls.\n\nWe'll be using the ``collect_results`` function, which has three\nparameters.\n\n-  rule: a valid powertrack rule, referenced earlier\n-  max\\_results: as the api handles pagination, it will stop collecting\n   when we get to this number\n-  result\\_stream\\_args: configuration args that we've already\n   specified.\n\nFor the remaining examples, please change the args to either premium or\nenterprise depending on your usage.\n\nLet's see how it goes:\n\n.. code:: python\n\n    from searchtweets import collect_results\n\n.. code:: python\n\n    tweets = collect_results(rule,\n                             max_results=100,\n                             result_stream_args=enterprise_search_args) # change this if you need to\n\nBy default, tweet payloads are lazily parsed into a ``Tweet`` object. An\noverwhelming number of tweet attributes are made available directly, as\nsuch:\n\n.. code:: python\n\n    [print(tweet.all_text) for tweet in tweets[0:10]];\n\n\n.. parsed-literal::\n\n    That deep sigh Beyonc\u00e9 took once she realized she wouldn\u2019t be able to get the earpiece out of her hair before the dance break \ud83d\ude02.  https://t.co/dU1K2KMT7i\n    4 Years ago today, \"BEYONC\u00c9\" by Beyonc\u00e9 was surprise released. It received acclaim from critics,  debuted at #1 and certified 2x Platinum in the US. https://t.co/wB3C7DuX9o\n    me mata la gente que se cree superior por sus gustos de m\u00fasica escuches queen beyonce o el polaco no sos mas ni menos que nadie\n    I\u2019m literally not Beyonc\u00e9 https://t.co/LwIkllCx6P\n    #BEYONC\u00c9 \u2023 \ud835\udc0c\ud835\udc04\ud835\udc00\ud835\udc03\ud835\udc03\ud835\udc05\ud835\udc00\ud835\udc0d \ud835\udc0e\ud835\udc05\ud835\udc08\ud835\udc02\ud835\udc08\ud835\udc00\ud835\udc0b - I Am... \ud835\udc16\ud835\udc0e\ud835\udc11\ud835\udc0b\ud835\udc03 \ud835\udc13\ud835\udc0e\ud835\udc14\ud835\udc11! https://t.co/TyyeDdXKiM\n    Beyonc\u00e9 on how nervous she was to release her self-titled... https://t.co/fru23c6DYC\n    AAAA ansiosa por esse feat da Beyonc\u00e9 com Jorge Ben Jor &lt;3 https://t.co/NkKJhC9JUd\n    I am world tour, the Beyonce experience, revamped hmt. https://t.co/pb07eMyNka\n    Tell me what studio versions of any artists would u like me to do? https://t.co/Z6YWsAJuhU\n    Billboard's best female artists over the last decade:\n    \n    2017: Ariana Grande\n    2016: Adele\n    2015: Taylor Swift\n    2014: Katy Perry\n    2013: Taylor Swift\n    2012: Adele\n    2011: Adele\n    2010: Lady Gaga\n    2009: Taylor Swift\n    2008: Rihanna\n    \n    Beyonce = 0\n    \n    Taylor Swift = 3 \ud83d\udc51\n    Beyonc\u00e9 explaining her intent behind the BEYONC\u00c9 visual album &amp; how she wanted to reinstate the idea of an album release as a significant, exciting event which had lost meaning in the face of hype created around singles. \ud83d\udc51 https://t.co/pK2MB35vYl\n\n\n.. code:: python\n\n    [print(tweet.created_at_datetime) for tweet in tweets[0:10]];\n\n\n.. parsed-literal::\n\n    2017-12-13 21:18:17\n    2017-12-13 21:18:16\n    2017-12-13 21:18:16\n    2017-12-13 21:18:15\n    2017-12-13 21:18:15\n    2017-12-13 21:18:13\n    2017-12-13 21:18:12\n    2017-12-13 21:18:12\n    2017-12-13 21:18:11\n    2017-12-13 21:18:10\n\n\n.. code:: python\n\n    [print(tweet.generator.get(\"name\")) for tweet in tweets[0:10]];\n\n\n.. parsed-literal::\n\n    Twitter for Android\n    Twitter for Android\n    Twitter for Android\n    Twitter for iPhone\n    Meadd\n    Twitter for iPhone\n    Twitter for Android\n    Twitter for iPhone\n    Twitter for iPhone\n    Twitter for Android\n\n\nVoila, we have some tweets. For interactive environments and other cases\nwhere you don't care about collecting your data in a single load or\ndon't need to operate on the stream of tweets or counts directly, I\nrecommend using this convenience function.\n\nWorking with the ResultStream\n-----------------------------\n\nThe ResultStream object will be powered by the ``search_args``, and\ntakes the rules and other configuration parameters, including a hard\nstop on number of pages to limit your API call usage.\n\n.. code:: python\n\n    rs = ResultStream(rule_payload=rule,\n                      max_results=500,\n                      max_pages=1,\n                      **premium_search_args)\n    \n    print(rs)\n\n\n.. parsed-literal::\n\n    ResultStream: \n    \t{\n        \"username\":null,\n        \"endpoint\":\"https:\\/\\/api.twitter.com\\/1.1\\/tweets\\/search\\/30day\\/dev.json\",\n        \"rule_payload\":{\n            \"query\":\"beyonce\",\n            \"maxResults\":100\n        },\n        \"tweetify\":true,\n        \"max_results\":500\n    }\n\n\nThere is a function, ``.stream``, that seamlessly handles requests and\npagination for a given query. It returns a generator, and to grab our\n500 tweets that mention ``beyonce`` we can do this:\n\n.. code:: python\n\n    tweets = list(rs.stream())\n\nTweets are lazily parsed using our Tweet Parser, so tweet data is very\neasily extractable.\n\n.. code:: python\n\n    # using unidecode to prevent emoji/accents printing \n    [print(tweet.all_text) for tweet in tweets[0:10]];\n\n\n.. parsed-literal::\n\n    Everyone: *still dragging Jay for cheating*\n    \n    Beyonc\u00e9: https://t.co/2z1ltlMQiJ\n    Beyonc\u00e9 changed the game w/ that digital drop 4 years ago today! \ud83c\udf89\n    \n    \u2022 #1 debut on Billboard\n    \u2022 Sold 617K in the US / over 828K WW in only 3 days\n    \u2022 Fastest-selling album on iTunes of all time\n    \u2022 Reached #1 in 118 countries\n    \u2022 Widespread acclaim; hailed as her magnum opus https://t.co/lDCdVs6em3\n    Beyonc\u00e9 \ud83d\udd25 #444Tour https://t.co/sCvZzjLwqx\n    Se presentan casos de feminismo pop basado en sugerencias de artistas famosos en turno, Emma Watson, Beyonc\u00e9.\n    Beyonce. Are you kidding me with this?! #Supreme #love #everything\n    Dear Beyonc\u00e9, https://t.co/5visfVK2LR\n    At this time 4 years ago today, Beyonc\u00e9 released her self-titled album BEYONC\u00c9 exclusively on the iTunes Store without any prior announcement. The album remains the ONLY album in history to reach #1 in 118 countries &amp; the fastest-selling album in the history of the iTunes Store. https://t.co/ZZb4QyQYf0\n    4 years ago today, Beyonc\u00e9 released her self-titled visual album \"BEYONC\u00c9\" and shook up the music world forever. \ud83d\ude4c\ud83c\udfff https://t.co/aGtUSq9R3u\n    Everyone: *still dragging Jay for cheating*\n    \n    Beyonc\u00e9: https://t.co/2z1ltlMQiJ\n    And Beyonce hasn't had a solo #1 hit since the Bush administration soooo... https://t.co/WCd7ni8DwN\n\n\nCounts API\n----------\n\nWe can also use the counts api to get counts of tweets that match our\nrule. Each request will return up to *30* results, and each count\nrequest can be done on a minutely, hourly, or daily basis. The\nunderlying ``ResultStream`` object will handle converting your endpoint\nto the count endpoint, and you have to specify the ``count_bucket``\nargument when making a rule to use it.\n\nThe process is very similar to grabbing tweets, but has some minor\ndifferneces.\n\n**Caveat - premium sandbox environments do NOT have access to the counts\nAPI.**\n\n.. code:: python\n\n    count_rule = gen_rule_payload(\"beyonce\", count_bucket=\"day\")\n    \n    counts = collect_results(count_rule, result_stream_args=enterprise_search_args)\n\nOur results are pretty straightforward and can be rapidly used.\n\n.. code:: python\n\n    counts\n\n\n\n\n.. parsed-literal::\n\n    [{'count': 85660, 'timePeriod': '201712130000'},\n     {'count': 95231, 'timePeriod': '201712120000'},\n     {'count': 114540, 'timePeriod': '201712110000'},\n     {'count': 165964, 'timePeriod': '201712100000'},\n     {'count': 102022, 'timePeriod': '201712090000'},\n     {'count': 87630, 'timePeriod': '201712080000'},\n     {'count': 195794, 'timePeriod': '201712070000'},\n     {'count': 209629, 'timePeriod': '201712060000'},\n     {'count': 88742, 'timePeriod': '201712050000'},\n     {'count': 96795, 'timePeriod': '201712040000'},\n     {'count': 177595, 'timePeriod': '201712030000'},\n     {'count': 120102, 'timePeriod': '201712020000'},\n     {'count': 186759, 'timePeriod': '201712010000'},\n     {'count': 151212, 'timePeriod': '201711300000'},\n     {'count': 79311, 'timePeriod': '201711290000'},\n     {'count': 107175, 'timePeriod': '201711280000'},\n     {'count': 58192, 'timePeriod': '201711270000'},\n     {'count': 48327, 'timePeriod': '201711260000'},\n     {'count': 59639, 'timePeriod': '201711250000'},\n     {'count': 85201, 'timePeriod': '201711240000'},\n     {'count': 91544, 'timePeriod': '201711230000'},\n     {'count': 64129, 'timePeriod': '201711220000'},\n     {'count': 92065, 'timePeriod': '201711210000'},\n     {'count': 101617, 'timePeriod': '201711200000'},\n     {'count': 84733, 'timePeriod': '201711190000'},\n     {'count': 74887, 'timePeriod': '201711180000'},\n     {'count': 76091, 'timePeriod': '201711170000'},\n     {'count': 81849, 'timePeriod': '201711160000'},\n     {'count': 58423, 'timePeriod': '201711150000'},\n     {'count': 78004, 'timePeriod': '201711140000'},\n     {'count': 118077, 'timePeriod': '201711130000'}]\n\n\n\nDated searches / Full Archive Search\n------------------------------------\n\nLet's make a new rule and pass it dates this time.\n\n``gen_rule_payload`` takes dates of the forms ``YYYY-mm-DD`` and\n``YYYYmmDD``.\n\n**Note that this will only work with the full archive search option**,\nwhich is available to my account only via the enterprise options. Full\narchive search will likely require a different endpoint or access\nmethod; please see your developer console for details.\n\n.. code:: python\n\n    rule = gen_rule_payload(\"from:jack\", from_date=\"2017-09-01\", to_date=\"2017-10-30\", results_per_call=500)\n    print(rule)\n\n\n.. parsed-literal::\n\n    {\"query\":\"from:jack\",\"maxResults\":500,\"toDate\":\"201710300000\",\"fromDate\":\"201709010000\"}\n\n\n.. code:: python\n\n    tweets = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)\n\n.. code:: python\n\n    # usiing unidecode only to \n    [print(tweet.all_text) for tweet in tweets[0:10]];\n\n\n.. parsed-literal::\n\n    More clarity on our private information policy and enforcement. Working to build as much direct context into the product too https://t.co/IrwBexPrBA\n    To provide more clarity on our private information policy, we\u2019ve added specific examples of what is/is not a violation and insight into what we need to remove this type of content from the service. https://t.co/NGx5hh2tTQ\n    Launching violent groups and hateful images/symbols policy on November 22nd https://t.co/NaWuBPxyO5\n    We will now launch our policies on violent groups and hateful imagery and hate symbols on Nov 22. During the development process, we received valuable feedback that we\u2019re implementing before these are published and enforced. See more on our policy development process here \ud83d\udc47 https://t.co/wx3EeH39BI\n    @WillStick @lizkelley Happy birthday Liz!\n    Off-boarding advertising from all accounts owned by Russia Today (RT) and Sputnik.\n    \n    We\u2019re donating all projected earnings ($1.9mm) to support external research into the use of Twitter in elections, including use of malicious automation and misinformation. https://t.co/zIxfqqXCZr\n    @TMFJMo @anthonynoto Thank you\n    @gasca @stratechery @Lefsetz letter\n    @gasca @stratechery Bridgewater\u2019s Daily Observations\n    Yup!!!! \u2764\ufe0f\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f #davechappelle https://t.co/ybSGNrQpYF\n    @ndimichino Sometimes\n    Setting up at @CampFlogGnaw https://t.co/nVq8QjkKsf\n\n\n.. code:: python\n\n    rule = gen_rule_payload(\"from:jack\",\n                            from_date=\"2017-09-20\",\n                            to_date=\"2017-10-30\",\n                            count_bucket=\"day\",\n                            results_per_call=500)\n    print(rule)\n\n\n.. parsed-literal::\n\n    {\"query\":\"from:jack\",\"toDate\":\"201710300000\",\"fromDate\":\"201709200000\",\"bucket\":\"day\"}\n\n\n.. code:: python\n\n    counts = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)\n\n.. code:: python\n\n    [print(c) for c in counts];\n\n\n.. parsed-literal::\n\n    {'timePeriod': '201710290000', 'count': 0}\n    {'timePeriod': '201710280000', 'count': 0}\n    {'timePeriod': '201710270000', 'count': 3}\n    {'timePeriod': '201710260000', 'count': 6}\n    {'timePeriod': '201710250000', 'count': 4}\n    {'timePeriod': '201710240000', 'count': 4}\n    {'timePeriod': '201710230000', 'count': 0}\n    {'timePeriod': '201710220000', 'count': 0}\n    {'timePeriod': '201710210000', 'count': 3}\n    {'timePeriod': '201710200000', 'count': 2}\n    {'timePeriod': '201710190000', 'count': 1}\n    {'timePeriod': '201710180000', 'count': 6}\n    {'timePeriod': '201710170000', 'count': 2}\n    {'timePeriod': '201710160000', 'count': 2}\n    {'timePeriod': '201710150000', 'count': 1}\n    {'timePeriod': '201710140000', 'count': 64}\n    {'timePeriod': '201710130000', 'count': 3}\n    {'timePeriod': '201710120000', 'count': 4}\n    {'timePeriod': '201710110000', 'count': 8}\n    {'timePeriod': '201710100000', 'count': 4}\n    {'timePeriod': '201710090000', 'count': 1}\n    {'timePeriod': '201710080000', 'count': 0}\n    {'timePeriod': '201710070000', 'count': 0}\n    {'timePeriod': '201710060000', 'count': 1}\n    {'timePeriod': '201710050000', 'count': 3}\n    {'timePeriod': '201710040000', 'count': 5}\n    {'timePeriod': '201710030000', 'count': 8}\n    {'timePeriod': '201710020000', 'count': 5}\n    {'timePeriod': '201710010000', 'count': 0}\n    {'timePeriod': '201709300000', 'count': 0}\n    {'timePeriod': '201709290000', 'count': 0}\n    {'timePeriod': '201709280000', 'count': 9}\n    {'timePeriod': '201709270000', 'count': 41}\n    {'timePeriod': '201709260000', 'count': 13}\n    {'timePeriod': '201709250000', 'count': 6}\n    {'timePeriod': '201709240000', 'count': 7}\n    {'timePeriod': '201709230000', 'count': 3}\n    {'timePeriod': '201709220000', 'count': 0}\n    {'timePeriod': '201709210000', 'count': 1}\n    {'timePeriod': '201709200000', 'count': 7}",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/twitterdev/twitter_search_api",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "searchtweets",
    "platform": "",
    "project_url": "https://pypi.org/project/searchtweets/",
    "release_url": "https://pypi.org/project/searchtweets/1.0/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "Wrapper for Twitter's Premium and Enterprise search APIs",
    "version": "1.0"
  },
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "606e8552934393a3d7e990b3f78371e5",
          "sha256": "9fe9c5c44846d216002db5912038d37ed314c17817c6be3d05f6e459afe9bdfe"
        },
        "downloads": -1,
        "filename": "searchtweets-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "606e8552934393a3d7e990b3f78371e5",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 25464,
        "upload_time": "2018-01-08T21:36:36",
        "url": "https://files.pythonhosted.org/packages/1a/54/06c559fbcbb35d01da2e877a0a61174f7003c18e8b138a88cf1e97cc026b/searchtweets-1.0.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "606e8552934393a3d7e990b3f78371e5",
        "sha256": "9fe9c5c44846d216002db5912038d37ed314c17817c6be3d05f6e459afe9bdfe"
      },
      "downloads": -1,
      "filename": "searchtweets-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "606e8552934393a3d7e990b3f78371e5",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 25464,
      "upload_time": "2018-01-08T21:36:36",
      "url": "https://files.pythonhosted.org/packages/1a/54/06c559fbcbb35d01da2e877a0a61174f7003c18e8b138a88cf1e97cc026b/searchtweets-1.0.tar.gz"
    }
  ]
}