{
  "info": {
    "author": "Mingyu Gao",
    "author_email": "mgao12@stanford.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 2.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: System :: Hardware"
    ],
    "description": ".. image:: https://travis-ci.org/stanford-mast/nn_dataflow.svg?branch=master\n    :target: https://travis-ci.org/stanford-mast/nn_dataflow\n.. image:: https://coveralls.io/repos/github/stanford-mast/nn_dataflow/badge.svg?branch=master\n    :target: https://coveralls.io/github/stanford-mast/nn_dataflow?branch=master\n\n\nNeural Network Dataflow Scheduling\n==================================\n\nThis Python tool allows you to explore the energy-efficient dataflow scheduling\nfor neural networks (NNs), including array mapping, loop blocking and\nreordering, and parallel partitioning.\n\nFor hardware, we assume an Eyeriss-style NN accelerator [Chen16]_, i.e., a 2D\narray of processing elements (PEs) with a local register file in each PE, and a\nglobal SRAM buffer shared by all PEs. We further support a tiled architecture\nwith multiple nodes that can partition and process the NN computations in\nparallel. Each node is an Eyeriss-style engine as above.\n\nIn software, we decouple the dataflow scheduling into three subproblems:\n\n- Array mapping, which deals with mapping one 2D convolution computation (one\n  2D ifmap convolves with one 2D filter to get one 2D ofmap) onto the hardware\n  PE array. We support row stationary mapping [Chen16]_.\n- Loop blocking and reordering, which decides the order between all 2D\n  convolutions by blocking and reordering the nested loops. We support\n  exhaustive search over all blocking and reordering schemes [Yang16]_, and\n  analytical bypass solvers [Gao17]_.\n- Partitioning, which partitions the NN computations for parallel processing.\n  We support batch partitioning, fmap partitioning, output partitioning, input\n  partitioning, and the combination between them (hybrid) [Gao17]_.\n\nSee the details in our ASPLOS'17 paper [Gao17]_.\n\nIf you use this tool in your work, we kindly request that you reference our\npaper(s) below, and send us a citation of your work.\n\n- Gao et al., \"TETRIS: Scalable and Efficient Neural Network Acceleration with\n  3D Memory\", in ASPLOS, April 2017 [Gao17]_.\n\n\nUsage\n-----\n\nFirst, define the NN structure in ``nn_dataflow/nns``. We already defined\nseveral popular NNs for you, including AlexNet, VGG-16, GoogLeNet, ResNet-152,\netc.\n\nThen, use ``nn_dataflow/tools/nn_dataflow_search.py`` to search for the optimal\ndataflow for the NN. For detailed options, type::\n\n    > python ./nn_dataflow/tools/nn_dataflow_search.py -h\n\nYou can specify NN batch size and word size, PE array dimensions, number of\ntile nodes, register file and global buffer capacity, and the energy cost of\nall components. Note that, the energy cost of array bus should be the average\nenergy of transferring the data from the buffer to one PE, *not* local neighbor\ntransfer; the unit static energy cost should be the static energy of *one* node\nin one clock cycle.\n\nOther options include:\n\n- ``--mem-type``: ``2D`` or ``3D``. With 2D memory, memory channels are only on\n  the left and right sides of the chip; with 3D memory, memory channels are on\n  the top of all tile nodes (one per each).\n- ``--disable-bypass``: a combination of ``i``, ``o``, ``f``, whether to\n  disallow global buffer bypass for ifmaps, ofmaps, and weights.\n- ``--solve-loopblocking``: whether to use analytical bypass solvers for loop\n  blocking and reordering. See [Gao17]_.\n- ``--hybrid-partitioning``: whether to use hybrid partitioning in [Gao17]_.\n  If not enabled, use naive partitioning, i.e., fmap partitioning for CONV\n  layers, and output partitioning for FC layers.\n- ``--batch-partitioning`` and ``--ifmap-partitioning``: whether the hybrid\n  partitioning also explores batch and input partitioning.\n\n\nCode Structure\n--------------\n\n- ``nn_dataflow``\n    - ``core``\n        - Top-level dataflow exploration: ``nn_dataflow``,\n          ``nn_dataflow_scheme``.\n        - Layer scheduling: ``scheduling``.\n        - Array mapping: ``map_strategy``.\n        - Loop blocking and reordering: ``loop_blocking``,\n          ``loop_blocking_scheme``, ``loop_blocking_solver``.\n        - Partitioning: ``partition``, ``partition_scheme``.\n        - Network and layer: ``network``, ``layer``.\n    - ``nns``: example NN definitions.\n    - ``tests``: unit tests.\n    - ``tools``: executables.\n\n\nVerification and Testing\n------------------------\n\nTo verify the tool against the Eyeriss result [Chen16]_, see\n``nn_dataflow/tests/dataflow_test/test_nn_dataflow.py``.\n\nTo run (unit) tests, do one of the following::\n\n    > python -m unittest discover\n\n    > python -m pytest\n\n    > pytest\n\nTo check code coverage with ``pytest-cov`` plug-in::\n\n    > pytest --cov=nn_dataflow\n\n\nCopyright & License\n-------------------\n\n``nn_dataflow`` is free software; you can redistribute it and/or modify it\nunder the terms of the `BSD License <LICENSE>`__ as published by the Open\nSource Initiative, revised version.\n\n``nn_dataflow`` was originally written by Mingyu Gao at Stanford University,\nand per Stanford University policy, the copyright of this original code remains\nwith the Board of Trustees of Leland Stanford Junior University.\n\n\nReferences\n----------\n\n.. [Gao17] Gao, Pu, Yang, Horowitz, and Kozyrakis, `TETRIS: Scalable and\n  Efficient Neural Network Acceleration with 3D Memory\n  <//dl.acm.org/citation.cfm?id=3037697.3037702>`__, in ASPLOS. April, 2017.\n\n.. [Chen16] Chen, Emer, and Sze, `Eyeriss: A Spatial Architecture for\n  Energy-Efficient Dataflow for Convolutional Neural Networks\n  <//dl.acm.org/citation.cfm?id=3001177>`__, in ISCA. June, 2016.\n\n.. [Yang16] Yang, Pu, Rister, Bhagdikar, Richardson, Kvatinsky,\n  Ragan-Kelley, Pedram, and Horowitz, `A Systematic Approach to Blocking\n  Convolutional Neural Networks <//arxiv.org/abs/1606.04209>`__, arXiv\n  preprint, 2016.\n\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/stanford-mast/nn_dataflow",
    "keywords": "neural-network scheduling dataflow optimizer",
    "license": "BSD 3-clause",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nn-dataflow",
    "platform": "",
    "project_url": "https://pypi.org/project/nn-dataflow/",
    "release_url": "https://pypi.org/project/nn-dataflow/1.5/",
    "requires_dist": [
      "pytest-xdist (>=1)",
      "pytest-cov (>=2)",
      "pytest (>=3)",
      "coverage (>=4)",
      "argparse"
    ],
    "requires_python": "",
    "summary": "Explore the energy-efficient dataflow scheduling for neural networks.",
    "version": "1.5"
  },
  "releases": {
    "1.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1597459119f829240819fb70e0a2cbe8",
          "sha256": "c2fb4eae8ba3bb4600c2ad3420555f7199dc50de5211517d23705ed67e94699b"
        },
        "downloads": 0,
        "filename": "nn_dataflow-1.5-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "1597459119f829240819fb70e0a2cbe8",
        "packagetype": "bdist_wheel",
        "python_version": "py2",
        "size": 153486,
        "upload_time": "2017-08-23T20:45:00",
        "url": "https://files.pythonhosted.org/packages/b3/50/1842942006585ee27570429d36b20f211dd27b64a7be5de427f17b4cb86d/nn_dataflow-1.5-py2-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "ab3eb315e5aabdb90b408c7645267bd8",
          "sha256": "f02d237cc68eb34494414ac5b929614dcce54b524a40b9c1121a1ae39126b989"
        },
        "downloads": 0,
        "filename": "nn_dataflow-1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "ab3eb315e5aabdb90b408c7645267bd8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 86202,
        "upload_time": "2017-08-23T20:45:02",
        "url": "https://files.pythonhosted.org/packages/1a/ee/01f35471bb20005d7d15e28140702f7fd9d785bd74ad367344f848306f2e/nn_dataflow-1.5.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "1597459119f829240819fb70e0a2cbe8",
        "sha256": "c2fb4eae8ba3bb4600c2ad3420555f7199dc50de5211517d23705ed67e94699b"
      },
      "downloads": 0,
      "filename": "nn_dataflow-1.5-py2-none-any.whl",
      "has_sig": false,
      "md5_digest": "1597459119f829240819fb70e0a2cbe8",
      "packagetype": "bdist_wheel",
      "python_version": "py2",
      "size": 153486,
      "upload_time": "2017-08-23T20:45:00",
      "url": "https://files.pythonhosted.org/packages/b3/50/1842942006585ee27570429d36b20f211dd27b64a7be5de427f17b4cb86d/nn_dataflow-1.5-py2-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "ab3eb315e5aabdb90b408c7645267bd8",
        "sha256": "f02d237cc68eb34494414ac5b929614dcce54b524a40b9c1121a1ae39126b989"
      },
      "downloads": 0,
      "filename": "nn_dataflow-1.5.tar.gz",
      "has_sig": false,
      "md5_digest": "ab3eb315e5aabdb90b408c7645267bd8",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 86202,
      "upload_time": "2017-08-23T20:45:02",
      "url": "https://files.pythonhosted.org/packages/1a/ee/01f35471bb20005d7d15e28140702f7fd9d785bd74ad367344f848306f2e/nn_dataflow-1.5.tar.gz"
    }
  ]
}