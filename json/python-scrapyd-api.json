{
  "info": {
    "author": "Darian Moody",
    "author_email": "mail@djm.org.uk",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.6",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Topic :: Internet :: WWW/HTTP"
    ],
    "description": "==================\npython-scrapyd-api\n==================\n\n.. image:: https://badge.fury.io/py/python-scrapyd-api.png\n        :target: http://badge.fury.io/py/python-scrapyd-api\n        :alt: The PyPI version\n\n.. image:: https://travis-ci.org/djm/python-scrapyd-api.png?branch=master\n        :target: https://travis-ci.org/djm/python-scrapyd-api\n        :alt: Built Status on Travis-CI\n\n.. image:: https://coveralls.io/repos/djm/python-scrapyd-api/badge.png\n        :target: https://coveralls.io/r/djm/python-scrapyd-api\n        :alt: Coverage Status on Coveralls\n\n.. image:: https://readthedocs.org/projects/python-scrapyd-api/badge/?version=latest\n        :target: http://python-scrapyd-api.readthedocs.org/en/latest/\n        :alt: Documentation Status on ReadTheDocs\n\n\nA Python wrapper for working with Scrapyd_'s API_.\n\nCurrent released version: 2.0.1 (see history_).\n\nAllows a Python application to talk to, and therefore control, the Scrapy_\ndaemon: Scrapyd_.\n\n* Supports Python 2.6, 2.7, 3.3 & 3.4\n* Free software: BSD license\n* `Full documentation`_\n* On the `Python Package Index (PyPI)`_\n* Scrapyd's `API Documentation`_\n\n.. _Scrapy: http://scrapy.org/\n.. _Scrapyd: https://github.com/scrapy/scrapyd\n.. _API: http://scrapyd.readthedocs.org/en/latest/api.html\n.. _history: https://github.com/djm/python-scrapyd-api/blob/master/HISTORY.rst\n.. _Python Package Index (PyPI): https://pypi.python.org/pypi/python-scrapyd-api/\n.. _Full documentation: http://python-scrapyd-api.rtfd.org\n.. _API Documentation: http://scrapyd.readthedocs.org/en/latest/api.html\n\nInstall\n-------\n\nEasiest installation is via `pip`::\n\n    pip install python-scrapyd-api\n\nQuick Usage\n-----------\n\nPlease refer to the full documentation_ for more detailed usage but to get you started:\n\n.. code:: python\n\n    >>> from scrapyd_api import ScrapydAPI\n    >>> scrapyd = ScrapydAPI('http://localhost:6800')\n\n.. _documentation: http://python-scrapyd-api.rtfd.org\n\n**Add a project** egg as a new version:\n\n.. code:: python\n\n    >>> egg = open('some_egg.egg')\n    >>> scrapyd.add_version('project_name', 'version_name', egg)\n    # Returns the number of spiders in the project.\n    3\n    >>> egg.close()\n\n**Cancel a scheduled job**:\n\n.. code:: python\n\n    >>> scrapyd.cancel('project_name', '14a6599ef67111e38a0e080027880ca6')\n    # Returns the \"previous state\" of the job before it was cancelled: 'running' or 'pending'.\n    'running'\n\n**Delete a project** and all sibling versions:\n\n.. code:: python\n\n    >>> scrapyd.delete_project('project_name')\n    # Returns True if the request was met with an OK response.\n    True\n\n**Delete a version** of a project:\n\n.. code:: python\n\n    >>> scrapyd.delete_version('project_name', 'version_name')\n    # Returns True if the request was met with an OK response.\n    True\n\n**Request status** of a job:\n\n.. code:: python\n\n    >>> scrapyd.job_status('project_name', '14a6599ef67111e38a0e080027880ca6')\n    # Returns 'running', 'pending', 'finished' or '' for unknown state.\n    'running'\n\n**List all jobs** registered:\n\n.. code:: python\n\n    >>> scrapyd.list_jobs('project_name')\n    # Returns a dict of running, finished and pending job lists.\n    {\n        'pending': [\n            {\n                u'id': u'24c35...f12ae',\n                u'spider': u'spider_name'\n            },\n        ],\n        'running': [\n            {\n                u'id': u'14a65...b27ce',\n                u'spider': u'spider_name',\n                u'start_time': u'2014-06-17 22:45:31.975358'\n            },\n        ],\n        'finished': [\n            {\n                u'id': u'34c23...b21ba',\n                u'spider': u'spider_name',\n                u'start_time': u'2014-06-17 22:45:31.975358',\n                u'end_time': u'2014-06-23 14:01:18.209680'\n            }\n        ]\n    }\n\n**List all projects** registered:\n\n.. code:: python\n\n    >>> scrapyd.list_projects()\n    [u'ecom_project', u'estate_agent_project', u'car_project']\n\n**List all spiders** available to a given project:\n\n.. code:: python\n\n    >>> scrapyd.list_spiders('project_name')\n    [u'raw_spider', u'js_enhanced_spider', u'selenium_spider']\n\n**List all versions** registered to a given project:\n\n.. code:: python\n\n    >>> scrapyd.list_versions('project_name'):\n    [u'345', u'346', u'347', u'348']\n\n**Schedule a job** to run with a specific spider:\n\n.. code:: python\n\n    # Schedule a job to run with a specific spider.\n    >>> scrapyd.schedule('project_name', 'spider_name')\n    # Returns the Scrapyd job id.\n    u'14a6599ef67111e38a0e080027880ca6'\n\n**Schedule a job** to run while passing override settings:\n\n.. code:: python\n\n    >>> settings = {'DOWNLOAD_DELAY': 2}\n    >>> scrapyd.schedule('project_name', 'spider_name', settings=settings)\n    u'25b6588ef67333e38a0e080027880de7'\n\n**Schedule a job** to run while passing extra attributes to spider initialisation:\n\n.. code:: python\n\n    >>> scrapyd.schedule('project_name', 'spider_name', extra_attribute='value')\n    # NB: 'project', 'spider' and 'settings' are reserved kwargs for this\n    # method and therefore these names should be avoided when trying to pass\n    # extra attributes to the spider init.\n    u'25b6588ef67333e38a0e080027880de7'\n\n\nSetting up the project to contribute code\n-----------------------------------------\n\nPlease see CONTRIBUTING.rst_ which is also mirrored in the `full documentation`_.\nThis will guide you through our pull request guidelines, project setup and\ntesting requirements.\n\n.. _CONTRIBUTING.rst: https://github.com/djm/python-scrapyd-api/blob/master/CONTRIBUTING.rst\n.. _full documentation: http://python-scrapyd-api.rtfd.org\n\n\nLicense\n-------\n\n2-clause BSD. See the full LICENSE_.\n\n.. _LICENSE: https://github.com/djm/python-scrapyd-api/blob/master/LICENSE\n\n\n\n\nHistory\n-------\n\n2.0.0 (2016-02-27)\n++++++++++++++++++\n\nWhy Version 2? This package has been production ready and stable in use\nfor over a year now, so it's ready  to commit to a stable API /w semver.\nVersion 1 has deliberately been skipped to make it absolutely clear that\nthis release contains a breaking change:\n\nBreaking changes:\n\n* The cancel job endpoint now returns `True` on hearing a successful reply\n  from the Scrapyd API; before it would have returned `True` only if the\n  cancelled job was previously running, but this resulted in us incorrectly\n  reporting `False` when a *pending* job was actually cancelled.\n\nOther changes:\n\n* The cancel job endpoint now accepts a `signal` keyword argument which is\n  the termination signal Scrapyd uses to cancel the spider job. If not\n  specified, the value is not sent to the scrapyd endpoint at all, therefore\n  allows scrapyd control over which default signal gets used (currently `TERM`).\n\n\n0.2.0 (2015-01-14)\n++++++++++++++++++\n\n* Added the new ``job_status`` method which can retrieve the job status of a\n  specific job from a project. See docs for usage.\n* Increased and improved test coverage.\n\n0.1.0 (2014-09-16)\n++++++++++++++++++\n\n* First release on PyPI.",
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/djm/python-scrapyd-api",
    "keywords": "python-scrapyd-api scrapyd scrapy api wrapper",
    "license": "BSD",
    "maintainer": null,
    "maintainer_email": null,
    "name": "python-scrapyd-api",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/python-scrapyd-api/",
    "release_url": "https://pypi.org/project/python-scrapyd-api/2.0.1/",
    "requires_python": null,
    "summary": "A Python wrapper for working with the Scrapyd API",
    "version": "2.0.1"
  },
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e3a810aefe1c62c76ac34ba5b216e02e",
          "sha256": "bb5f11d461c930c5541c74e8c1fb5a0e7c86971773ed72727b2710e1d36b28f1"
        },
        "downloads": 1512,
        "filename": "python_scrapyd_api-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e3a810aefe1c62c76ac34ba5b216e02e",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 9417,
        "upload_time": "2014-09-16T12:45:18",
        "url": "https://files.pythonhosted.org/packages/cc/3d/872f6e2cba4354b599eb628f7fd9f2d53f10364bd4a4de499bc2b99d16b1/python_scrapyd_api-0.1.0-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "cb45354a1545eddd4124c32ea798a52a",
          "sha256": "9998ccec6791648e5fc382478f48ba7b4a05725dc57aa1ca9136211c4583e3c6"
        },
        "downloads": 1788,
        "filename": "python-scrapyd-api-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "cb45354a1545eddd4124c32ea798a52a",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 19804,
        "upload_time": "2014-09-16T12:45:15",
        "url": "https://files.pythonhosted.org/packages/db/30/a243f832bfd95db556ea494bb5372f5dbbfc9c26a4c38f7ab01dff57c4ba/python-scrapyd-api-0.1.0.tar.gz"
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "75c4bf08f729d2afb5ca9f56cd1e79bc",
          "sha256": "65d689b28878682bfceb93cc4dcda26261fc6da91d8bb7683f71a0263b370adc"
        },
        "downloads": 3550,
        "filename": "python_scrapyd_api-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "75c4bf08f729d2afb5ca9f56cd1e79bc",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 10324,
        "upload_time": "2015-01-15T00:40:43",
        "url": "https://files.pythonhosted.org/packages/7f/f8/6d7450dc0e6c5446776024096d2ba541ab9e1de2a1282b6345cfd1e5203d/python_scrapyd_api-0.2.0-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "1d4dd817b9cb6408780285d84213713e",
          "sha256": "674093e2cd0e3d0802bf91d24379713ae4427d01642f25aee1b1c4f2710d187c"
        },
        "downloads": 1408,
        "filename": "python-scrapyd-api-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1d4dd817b9cb6408780285d84213713e",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23304,
        "upload_time": "2015-01-15T00:40:39",
        "url": "https://files.pythonhosted.org/packages/8e/07/75da8d9b274592b05771be6753e647ba31cfd85f29896874865b0da66bb6/python-scrapyd-api-0.2.0.tar.gz"
      }
    ],
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1a2262efd3e9b469eec434926f68e44b",
          "sha256": "ddd9098bdf1f5c69c1be186d68a7b3aed2c79f47fa51b7a22697e5da89f6b37f"
        },
        "downloads": 109,
        "filename": "python_scrapyd_api-2.0.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1a2262efd3e9b469eec434926f68e44b",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 11217,
        "upload_time": "2016-03-27T15:54:26",
        "url": "https://files.pythonhosted.org/packages/d8/9b/d24fb7cf3db9bb0a10c64d6f56e3d634ea1e833ab5d3b34be62ddc63625d/python_scrapyd_api-2.0.0-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "f2e19deab07fd35730cc1f1b4073bd16",
          "sha256": "c8cf0feed8d2aa16354ae2d452b09b4dc3d72366fa8923157cfc0f60f0ff9bad"
        },
        "downloads": 113,
        "filename": "python-scrapyd-api-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f2e19deab07fd35730cc1f1b4073bd16",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21863,
        "upload_time": "2016-03-27T15:54:16",
        "url": "https://files.pythonhosted.org/packages/87/44/8da74fe399f146144f9d0b9a4415db40f97f7f49fac53e7d43fd0ea316fd/python-scrapyd-api-2.0.0.tar.gz"
      }
    ],
    "2.0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0407528e913e6910d34d5db5941bb4ab",
          "sha256": "536c05a2532a18f1511e0cf0eaa0e8539dda74dd88a7e5a1bdf8ba83265d4251"
        },
        "downloads": 1318,
        "filename": "python_scrapyd_api-2.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0407528e913e6910d34d5db5941bb4ab",
        "packagetype": "bdist_wheel",
        "python_version": "2.7",
        "size": 11327,
        "upload_time": "2016-03-27T16:15:25",
        "url": "https://files.pythonhosted.org/packages/41/b6/9431b7c30f86a37348ef17669bf19b41290ca43223ed1d7dab29bf51d3d7/python_scrapyd_api-2.0.1-py2.py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "76641febbe1dd20f08373404922ed21a",
          "sha256": "5d09e4bb8665fec54dc240bebebfb80b5c99c0488df45fd4b1e6ad50e52599f9"
        },
        "downloads": 138,
        "filename": "python-scrapyd-api-2.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "76641febbe1dd20f08373404922ed21a",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 21364,
        "upload_time": "2016-03-27T16:15:19",
        "url": "https://files.pythonhosted.org/packages/5f/98/59bf3c3779aadc2ecfd228d6334d20c87f1dc969218941be281fa5072240/python-scrapyd-api-2.0.1.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "0407528e913e6910d34d5db5941bb4ab",
        "sha256": "536c05a2532a18f1511e0cf0eaa0e8539dda74dd88a7e5a1bdf8ba83265d4251"
      },
      "downloads": 1318,
      "filename": "python_scrapyd_api-2.0.1-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0407528e913e6910d34d5db5941bb4ab",
      "packagetype": "bdist_wheel",
      "python_version": "2.7",
      "size": 11327,
      "upload_time": "2016-03-27T16:15:25",
      "url": "https://files.pythonhosted.org/packages/41/b6/9431b7c30f86a37348ef17669bf19b41290ca43223ed1d7dab29bf51d3d7/python_scrapyd_api-2.0.1-py2.py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "76641febbe1dd20f08373404922ed21a",
        "sha256": "5d09e4bb8665fec54dc240bebebfb80b5c99c0488df45fd4b1e6ad50e52599f9"
      },
      "downloads": 138,
      "filename": "python-scrapyd-api-2.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "76641febbe1dd20f08373404922ed21a",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 21364,
      "upload_time": "2016-03-27T16:15:19",
      "url": "https://files.pythonhosted.org/packages/5f/98/59bf3c3779aadc2ecfd228d6334d20c87f1dc969218941be281fa5072240/python-scrapyd-api-2.0.1.tar.gz"
    }
  ]
}