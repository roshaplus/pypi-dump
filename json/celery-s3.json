{
  "info": {
    "author": "Rob Golding",
    "author_email": "rob@robgolding.com",
    "bugtrack_url": "",
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: System :: Distributed Computing"
    ],
    "description": "=========\r\nCelery S3\r\n=========\r\n\r\nCelery-S3 is a simple S3 result backend for Celery.\r\n\r\nIf used in conjunction with the SQS broker, it allows for Celery deployments\r\nthat use only distributed AWS services -- with no dependency on individual\r\nmachines within your infrastructure.\r\n\r\nThis backend probably isn't suitable for particularly high-traffic Celery\r\ndeployments, but it works just fine in general -- and imposes no limits on the\r\nnumber of workers in the pool.\r\n\r\nInstallation\r\n============\r\n\r\nInstall via pip::\r\n\r\n    pip install celery-s3\r\n\r\nThen configure Celery to use the `S3Backend`::\r\n\r\n    CELERY_RESULT_BACKEND = 'celery_s3.backends.S3Backend'\r\n\r\n    CELERY_S3_BACKEND_SETTINGS = {\r\n        'aws_access_key_id': '<your_aws_access_key_id>',\r\n        'aws_secret_access_key': '<your_aws_secret_access_key>',\r\n        'bucket': '<your_bucket_name>',\r\n    }\r\n\r\nConfiguration\r\n=============\r\n\r\nTo use a folder within the specified bucket, set the `base_path` in your\r\n`CELERY_S3_BACKEND_SETTINGS`::\r\n\r\n\r\n    CELERY_S3_BACKEND_SETTINGS = {\r\n        ...\r\n        'base_path': '/celery/',\r\n        ...\r\n    }\r\n\r\nNotes\r\n=====\r\n\r\nStoring Celery results with this backend will obviously result in API calls\r\nbeing made to Amazon S3.  For each result, at least one `PUT` request will be\r\nmade (priced at $0.01 per 1,000 requests at the time of writing).  Also, the\r\ndata contained within the result object will be stored indefinitely, unless\r\notherwise specified.\r\n\r\nTo fetch a result for a task that has already finished, at least two requests\r\nwill be made (one `HEAD` and one `GET`).  If you use Celery's `result.get()` to\r\nwait for a task to finish, S3 will be polled continuously until the task has\r\nfinished.\r\n\r\nBy default, the poll interval is set to 0.5 seconds, which could result in\r\na lot of requests (two `HEAD` requests per second until the task has finished,\r\nthen one `GET` request to fetch the result).  If you need to use\r\n`result.get()`, consider increasing the interval and using a timeout to prevent\r\npolling forever: `result.get(interval=5, timeout=600)`.\r\n\r\nAlso, for tasks whose result you don't need, be sure to use `ignore_result`::\r\n\r\n    @celery.task(ignore_result=True)\r\n    def process_data(obj):\r\n        obj.do_processing()\r\n\r\nOnce task results have been used and are no longer needed, be sure to call\r\n`result.forget()` to delete the corresponding S3 key.  Otherwise, old results\r\nwill remain forever and contribute to storage costs (storage is priced at\r\n$0.095 per GB per month at the time of writing).\r\n\r\nAlso, the S3 lifecycle can be used to archive or delete old keys after\r\na certain period of time.",
    "docs_url": null,
    "download_url": "https://github.com/robgolding63/celery-s3/downloads",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/robgolding63/celery-s3",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "celery-s3",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/celery-s3/",
    "release_url": "https://pypi.org/project/celery-s3/0.1.1/",
    "requires_python": null,
    "summary": "An S3 result store backend for Celery",
    "version": "0.1.1"
  },
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "c65e5680cc3208cb1d6f3ca8e90b4d8d",
          "sha256": "9baf6981585868a6382f98884311a2142761eea4bd205de7f86becaec0eceaaf"
        },
        "downloads": 2898,
        "filename": "celery-s3-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c65e5680cc3208cb1d6f3ca8e90b4d8d",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 3234,
        "upload_time": "2013-02-20T10:11:59",
        "url": "https://files.pythonhosted.org/packages/4a/f8/ec9d6add9aaae066c317c4f8ae3fe14eadc7c561c661a417b1adaeea17a7/celery-s3-0.1.tar.gz"
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "95b81e2e82215f4f775bb7539c84a7d0",
          "sha256": "37d6843f58c61c18c8e1c511c4028a8182b0e492912b1ecd7c32433dd51a093c"
        },
        "downloads": 6267,
        "filename": "celery-s3-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "95b81e2e82215f4f775bb7539c84a7d0",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 4133,
        "upload_time": "2013-03-03T19:22:02",
        "url": "https://files.pythonhosted.org/packages/f3/64/eb7cfb9880a74b2db110b9ea903f562da41944c81549b3b15def9cd2f864/celery-s3-0.1.1.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "95b81e2e82215f4f775bb7539c84a7d0",
        "sha256": "37d6843f58c61c18c8e1c511c4028a8182b0e492912b1ecd7c32433dd51a093c"
      },
      "downloads": 6267,
      "filename": "celery-s3-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "95b81e2e82215f4f775bb7539c84a7d0",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 4133,
      "upload_time": "2013-03-03T19:22:02",
      "url": "https://files.pythonhosted.org/packages/f3/64/eb7cfb9880a74b2db110b9ea903f562da41944c81549b3b15def9cd2f864/celery-s3-0.1.1.tar.gz"
    }
  ]
}