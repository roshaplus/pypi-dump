{
  "info": {
    "author": "Stefan Codrescu",
    "author_email": "stefan.codrescu@noaa.gov",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "NetCDF Aggregation (ncagg)\n==========================\n\nSo... you want to aggregate time series NetCDF files?\n\nTL;DR\n-----\n\nInstall the utility with with pip:\n\n::\n\n    pip install ncagg\n\nOn the command line, use ``ncagg``:\n\n::\n\n    Usage: ncagg [OPTIONS] DST [SRC]...\n\n    Options:\n      -v, --version                   Show the version and exit.\n      --generate_template PATH        Print the default template generated for\n                                      PATH and exit.\n      -u TEXT                         Give an Unlimited Dimension Configuration as\n                                      udim:ivar[:hz[:hz]]\n      -b TEXT                         If -u given, specify bounds for ivar as\n                                      min:max or Tstart[:[T]stop]. min and max are\n                                      numerical, otherwise T indicates start and\n                                      stop are times.start and stop are of the\n                                      form YYYY[MM[DD[HH[MM]]]] and of stop is\n                                      omitted,it will be inferred to be the least\n                                      significantly specified date + 1.\n      -l [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n                                      log level\n      -t FILENAME                     Specify a configuration template\n      --help                          Show this message and exit.\n\nNotes:\n\n-  DST is the filename for the NetCDF output and should not already\n   exist, or will be overwritten.\n-  SRC is a list of input NetCDF files to aggregate.\n-  -u should specify an Unlimited Dimension Configuration. See below for\n   details.\n-  Taking tens of minutes for a day is normal, a progress bar will\n   indicate time remaining.\n-  For fine grained control over the output, specify a configuration\n   template (-t). See below for details.\n\nExamples:\n\n::\n\n    #     explicitly list files to aggregate\n    ncagg output_filename.nc file_0.nc file_02.nc #...\n\n    #     aggregate by globbing all files in some directory\n    ncagg output_filename.nc path_to_files/*.nc\n\n    #     sort the unlimited dimension record_number, according to the variable time\n    ncagg -u record_number:time output_filename.nc path_to_files/*.nc\n\n    #     sort the unlimited dimension record_number, according to the variable time, and insert\n    #     or remove fill values to ensure time occurrs at 10hz.\n    ncagg -u record_number:time:10 output_filename.nc path_to_files/*.nc\n\n    #     only include time values between 2017-06-01 to 2017-06-02 (bounds), including\n    #     sorting and filling, as above\n    ncagg -u record_number:time:10 -b T20170601:T20170602 output_filename.nc path_to_files/*.nc\n    #     or equivalently, if only one bound is specified, the end is inferred to be most significant + 1\n    ncagg -u record_number:time:10 -b T20170601 output_filename.nc path_to_files/*.nc\n\nFor more information, see the Unlimited Dimension Configuration below.\nThe ``ncagg`` Command Line Interface (CLI) builds a Config based on the\narguments specified.\n\nHigh level overview\n-------------------\n\nAggregation works in two stages:\n\n1. Create an AggreList object.\n2. Evaluate the AggreList.\n\nThe AggreList object is a data structure that describes how to combine a\nset of files. The structure of the AggreList specifies the order of the\nfiles, what pieces of those files to include, fill values between files,\nas well as fill values and sorting inside files.\n\nDuring stage 1, the AggreList is generated. The level of configuration\ngiven determines how much is done here. At most, each file is inspected\naccording to it's unlimited dimension and the variable that indexes it\nto determine sorting and filling. No data except for index\\_by variables\nare read and none written to disk during this stage.\n\nDuring stage 2, the AggreList is evaluated. Evaluating the AggreList\nmeans simply iterating over it and copying data from the nodes into the\noutput aggregation file, while keeping track of global attributes.\n\nReasons for using this approach:\n\n-  Possible to aggregate more data than fits in memory.\n-  Sort once per unlimited dimension.\n-  Modular code, easier to maintain, extend, and debug.\n\nConfiguration\n-------------\n\nThe sophistication of the aggregation is determine by how much\nconfiguration information is given on generation of the AggreList.\n\n-  No Config -> agg files along unlimited dims, sorted by filename.\n-  Config with index\\_by -> agg such that index\\_by is in ascending\n   order.\n-  Config with index\\_by and expected\\_cadences -> agg and regularize,\n   removing duplicates/inserting fills if needed.\n\nThe Config contains information that a NetCDF CDL specification would,\nbut in json format, extended with aggregation configuration information.\nIf not provided, a default version will be created using the first file\nin the list to aggregate.\n\nThe Config contains three properties (keys):\n\n-  dimensions\n-  variables\n-  attributes\n\nEach property is associated with a list of objects so to preserve\nordering. The order in the objects corresponds to the order of\nappearence in the output. Objects of all sections have a \"name\"\nproperty.\n\nDimensions specify the dimensions of the file and has at minimum a\n\"name\", and a \"size\" which can be null for an unlimited dimension.\nUnlimited dimensions may also have an Unlimited Dimension Configuration\nwhich will be described in a dedicated section below.\n\nVariable objects contain a \"name\", \"dimensions\", \"datatype\",\n\"attributes\", and \"chunksizes\". The dimensions property is a list of\ndimension names on which the variable depends, each must be configured\nin the dimensions section. datatype is something like int8, float32,\nstring, etc. Finally, attributes is another property containing key and\nvalues corresponding to variable attributes commonly including \"units\",\n\"valid\\_min\", \"\\_FillValue\", etc.\n\nAttributes objects contain \"name\", \"strategy\", and optionally \"value\"\nfor NetCDF Global Attributes. The strategies are described below.\n\nUnlimited Dimension Configuration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe Unlimited Dimension Configuration associates a particular unlimited\ndimension with a variable by which it can be indexed. Commonly, a\ndimension named time is associated with a variable also named time which\nindicates some epoch value for all data associated with that index of\nthe dimension.\n\nFor example, a file may have a dimension \"record\\_number\" which is\nindexed by a variable \"time\". Using the Unlimited Dimension\nConfiguration, we can specify to aggregate record\\_number such that the\nvariable \"time\" forms a monotonic sequence increasing at some expected\nfrequency.\n\nHere is what a typical GOES-R L1b product aggregation output looks like:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"time\",\n        \"expected_cadence\": {\"report_number\": 1},\n    }\n\nIn English, the configuration above says \"Order the dimension\nreport\\_number by the values in the variable time, where time values are\nexpected to increase along the dimension report\\_number incrementing at\n1hz.\" This would be specified to the ncagg CLI using\n``ncagg -u report_number:time:1 output.nc in1.nc in2.nc``.\n\nThe configuration allows to even index by multidimensional time (ehem,\nmag with 10 samples per report). On the command line specified as\n``-u report_number:OB_time:1:10``, or as json:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"OB_time\",\n        \"other_dim_indicies\": {\"samples_per_record\": 0},\n        \"expected_cadence\": {\"report_number\": 1, \"number_samples_per_report\": 10},\n    }\n\nOne design constraint was to not reshape the data, so above, we order\nthe data by looking at index 0 of samples\\_per\\_record for every value\nalong the report\\_number dimension. We assume that the other timestamps\nalong samples\\_per\\_record are correct. Also, given the configuration\nabove, we only insert fill records of OB\\_time if a full report\\_number\nrecord is missing (all 10 values along the number\\_samples\\_per\\_report\ndimension missing).\n\n--------------\n\nIndexing an unlimited dimension was described above. In addition to\nsimply indexing by a variable, in the case that the variable represents\ntime, a common operation would be to restrict value to some range, to,\nfor example, create a day file. The Unlimited Dimension Configuration\nwould look like:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"time\",\n        \"min\": 14000000,  # in units of the variable \"time\", expected\n        \"max\": 14000060,  # something like \"seconds since 2000-01-01 12:00:00\"\n        \"expected_cadence\": {\"report_number\": 1}\n    }\n\nWhich would be specified on the command line as\n``... -u report_number:time:1 -b1400000:14000060 ...`` where the ``-b``\noption stands for \"bounds\".\n\nAs min and max almost exclusively indicate datetime values, for\nconvenience, they are accepted as types: numerical, string, or python\ndatetime. In string representation, they must start with \"T\" and can be\nof the form \"TYYYY[MM[DD[HH[MM]]]]\" where brackets indicate optional and\nif omitted, will be inferred to be minimum valid value, ie: 01 for MM\n(month). A units attribute must available for the index\\_by variable in\nthe form of \" since \". On the command line, string time can be given as\n``... -u report_number:time:1 -bT20170101:T20170102 ...`` or\nequivalently the end bound can be omitted and will be inferred to be the\nrightmost specified of the beginning YYYY[MM[DD[HH[MM]]]] incremented by\none: ie: ``... -u report_number:time:1 -bT20170101 ...``.\n\n--------------\n\nConsider the suvi-l2-flloc (flare location) product which has two\nunlimited dimensions, time and feature\\_number. At any time record,\nthere can exist an arbitrary number of features. Consider a variable\nreporting the flux from each feature at each time:\n``flux(time, feature_number)``. Although feature\\_number is unlimited,\nit is unique to each time and thus needs to be \"flattened\":\n\n::\n\n    flux([0], [0]) -> [[3.2e-6]]\n    flux([0], [0, 1]) -> [[3.3e-6, 5.4e-7]]\n\n    undesired_aggregated_flux(time, feature_number):\n    [[3.2e-6,      _,      _],\n     [     _, 3.3e-6, 5.4e-7]]\n\n    desired_aggregated_flux(time, feature_number):\n    [[3.2e-6,      _],\n     [3.3e-6, 5.4e-7]]\n\nThe ``desired_aggregated_flux`` is achieved by setting {\"flatten\": true}\nwithin an the unlimited dimension configuration for feature\\_number.\n\n.. code:: json\n\n    [{\n        \"name\": \"time\",\n        \"size\": null,\n        \"index_by\": \"time\",\n    }, {\n        \"name\": \"feature_number\",\n        \"size\": null,\n        \"flatten\": true,\n    }]\n\nSpecify Global Attribute Aggregation Strategies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe aggregated netcdf file contains global attributes formed from the\nconstituent granules. A number of strategies exist to aggregate Global\nAttributes across the granules. Most are quite self explanatory:\n\n-  \"static\": use the configured \"value\" in the template, ignoring any\n   values that may be in the file.\n-  \"first\": first value seen will be taken as the output value for this\n   global attribute\n-  \"last\": the last value seen will be taken as global attribute\n-  \"unique\\_list\": compile values into a unique list \"first, second,\n   etc\"\n-  \"int\\_sum\": resulting in integer sum of the inputs\n-  \"float\\_sum\": StratFloatSum\n-  \"constant\": StratAssertConst, similar to first, but raises an error\n   if value changes among input files.\n-  \"date\\_created\": simply yeilds the current date when finalized,\n   standard dt fmt\n-  \"time\\_coverage\\_start\": start bound, if specified, standard dt fmt\n-  \"time\\_coverage\\_end\": end bound, if specified, standard dt fmt\n-  \"filename\": StratOutputFilename, set attribute to name of output file\n-  \"remove\": remove/do not include this global attribute\n-  \"first\\_input\": Filename of first file included in aggregate\n-  \"last\\_input\": Filename of last file included in aggregate\n-  \"input\\_count\": Number of files included in aggregate\n\nThe configuration format expects a key \"global attributes\" associated\nwith a list of objects each containing a global attribute name,\nstrategy, and possible value (for static). A list is used to preserve\norder, as the order in the configuration will be the resulting order in\nthe output NetCDF.\n\n.. code:: json\n\n    {\n        \"global attributes\": [\n            {\n                \"name\": \"production_site\", \n                \"strategy\": \"unique_list\"\n            }, {\n                \"name\": \"creator\",\n                \"strategy\": \"static\",\n                \"value\": \"Stefan Codrescu\"\n            }, {\n\n            ...\n            }\n         ]\n    }\n\nSpecify Dimension Indecies to Extract and Flatten\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNOT IMPLEMENTED. IN PROGRESS. SUBJECT TO CHANGE.\n\nConsider SEIS SGPS files which contain the data from two sensor units,\n+X and -X. Most variables are of the form var[record\\_number,\nsensor\\_unit, channel, ...]. It is possible to create an aggregate file\nfor the +X and -X sensor units individually using the\ntake\\_dim\\_indicies configuration key.\n\n.. code:: json\n\n    {\n        \"take_dim_indicies\": {\n            \"sensor_unit\": 0\n        }\n    }\n\nWith the above configuration, sensor\\_unit must be removed from the\ndimensions configuration. Please also ensure that variables do not list\nsensor\\_unit as a dimension, and also update chunk sizes accordingly.\nChunk sizes must be a list of values of the same length as dimensions.\n\nConfiguration Template\n~~~~~~~~~~~~~~~~~~~~~~\n\n``ncagg`` can be configured to output files into a format specified by a\nconfiguration template file. It is expected that this is a json format\nfile. A generic template can be created using the\n``ncagg --generate_template [SAMPLE_NC]`` command. The output of the\ntemplate command is the default template that is used internally if no\ntemplate is specified.\n\nExample usage\n^^^^^^^^^^^^^\n\nUse ``ncagg --generate_template example_netcdf.nc > my_template.json``\nto save the default template for an example\\_netcdf.nc file into\nmy\\_template.nc. Edit my\\_template.json to your liking, then run\naggregation using ``ncagg -t my_template.json [...]``.\n\nTemplate syntax\n^^^^^^^^^^^^^^^\n\nThe template syntax is verbose, but hopefully straightforward and clear.\nThe incoming template will be validated upon initiating an aggregation,\nbut some issues may only be found at runtime.\n\nAttributes\n''''''''''\n\nThe attributes section is a list of objects contianing global\nattributes:\n\n-  name: name of global attribute\n-  strategy: `aggregation\n   strategy <#Specify-Global-Attribute-Aggregation-Strategies>`__ to use\n   for attribute.\n-  value: value used by strategy, if required. Eg. constant, where the\n   value is \"test\".\n\nDimensions\n''''''''''\n\nThe dimensions section is a list of objects containing the dimensions of\nthe file. Most configuration options are covered in `Unlimited Dimension\nConfiguration <#Unlimited-Dimension-Configuration>`__ section, but to\nclarify:\n\n-  size: integer if dimension has a fixed size. null if it's unlimited.\n\nVariables\n'''''''''\n\nSimilarly, variables section is a list of objects configuring output\nvariables. Remove the object corresponding to some variable to remove it\nfrom the output.\n\nImportant notes:\n\n-  The dimensions referenced must exist.\n-  chunksizes must be the same number of elements as dimensions.\n\nTake care that everything is consistent when doing heavy modifications.\n\nTechnical and Implementation details\n------------------------------------\n\nAn AggreList is composed of two types of objects, InputFileNode and\nFillNode objects. These inherit in common from an AbstractNode and must\nimplement the ``get_size_along(unlimited_dim)`` and\n``data_for(var, dim)`` methods. Evaluating an aggregation list is simply\ngoing though the AggreList and calling something like:\n\n::\n\n    nc_out.variable[var][write_slice] = node.data_for(var)\n\nThe ``data_for`` must return data consistent with the shape promised\nfrom ``node.get_size_along(dim)``.\n\nThe complixity of aggregation comes in handling the dimensions and\nbuilding the aggregation list. In addition to the interface exposed by\nan AbstractNode, each InputFileNode and FillNode implement their own\nspecific functionality.\n\nA FillNode is simpler, and needs to be told how many fills to insert\nalong a certain unlimited dimension and optionally, can be configured to\nreturn values from ``data_for`` that are increasing along multiple\ndimensions according to configured ``expected_cadence`` values from a\ncertain start value.\n\nAn InputFileNode is more complicated and exposes methods to find the\ntime bounds of the file, and additionally, is internally capable of\nsorting itself and inserting fill values into itself. Of course, it\ndoesn't modify the actual input file, this is all done on the fly as\ndata is being read out through ``data_for``. Implementation wise, an\nInputFileNode may contain within itself a mini aggregation list\ncontaining two types of objects: slice and FillNode objects. Similarly\nto the large scale process of aggregating, an InputFileNode returns data\nthat has been assembled according to it's internal aggregation list and\ninternal sorting.\n\nTesting\n-------\n\nThis software is written for aggregation of GOES-R series Space Weather\ndata products (L1b and L2+). As such, it contains extensive tests\nagainst real GOES-16 satellite data. Many \"features\" in this code are\nintended to address \"quirks\" in the ground processing (implemented by a\ncertain contractor...).\n\nTests are in the ``test`` subdirectory. Run all tests with\n\n.. code:: bash\n\n    python -m unittest discover \n\nThe code is compatible with Python2 (2.7) and Python3, so unittests\nshould be run with both.\n\nDevelopment\n-----------\n\nSetting up a virtualenv is recommended for development.\n\n::\n\n    virtualenv venv\n    . venv/bin/activate\n    pip install --editable .\n\n--------------\n\nDeploy to pip, after testing with python2 and python3:\n\n.. code:: bash\n\n    rm -r dist/\n    python setup.py bdist_wheel --universal\n    twine upload dist/*\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://ctor.space/gitlab/work/ncagg",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ncagg",
    "platform": "",
    "project_url": "https://pypi.org/project/ncagg/",
    "release_url": "https://pypi.org/project/ncagg/0.4.0/",
    "requires_dist": [
      "numpy",
      "netCDF4",
      "cerberus",
      "Click"
    ],
    "requires_python": "",
    "summary": "Utility for aggregation of NetCDF data.",
    "version": "0.4.0"
  },
  "releases": {
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "76a3b8678a40b11fa96ed2b758b61490",
          "sha256": "af2c247eda44e1ff9824e503e0047953d0b2d12b2a2e3af476faa67fb6af2e79"
        },
        "downloads": -1,
        "filename": "ncagg-0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "76a3b8678a40b11fa96ed2b758b61490",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 33424,
        "upload_time": "2017-08-23T17:38:09",
        "url": "https://files.pythonhosted.org/packages/d5/f6/249cae10548074ee977a0e161261357509a7a4e61ef82ffe3ed7ece31ee8/ncagg-0.2-py2.py3-none-any.whl"
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "ae1af422d772861d0fb9553e0474df71",
          "sha256": "852c2bcaa5016d15650213b1bdcaba46fa1d441e563a331ffdf5dd752e9dd043"
        },
        "downloads": -1,
        "filename": "ncagg-0.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ae1af422d772861d0fb9553e0474df71",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 33447,
        "upload_time": "2017-08-31T19:16:14",
        "url": "https://files.pythonhosted.org/packages/82/cf/6939f8a06d073d2ea3538e15241379ce73a1c42a8927a025b6e42949570b/ncagg-0.3-py2.py3-none-any.whl"
      }
    ],
    "0.3.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "eb221ebefa454df8a33f719068b51c30",
          "sha256": "63fa1c3bbfbd968e235e5ed768ae8208fee7e0ca16cb4433e9440cc0c69fa3a7"
        },
        "downloads": -1,
        "filename": "ncagg-0.3.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eb221ebefa454df8a33f719068b51c30",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 37005,
        "upload_time": "2017-11-09T00:23:35",
        "url": "https://files.pythonhosted.org/packages/80/c0/f4177d9201cd29454a284bc457a48c0fae4524e32eafbadee39969fd62b0/ncagg-0.3.3-py2.py3-none-any.whl"
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "041224e56c64fab9ac4096561735e212",
          "sha256": "dc39f439d1765b7ec93652540d9eb10d6fa1fd1f5ce02e9aae3ec80514165e51"
        },
        "downloads": -1,
        "filename": "ncagg-0.4.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "041224e56c64fab9ac4096561735e212",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 37627,
        "upload_time": "2017-11-13T17:39:37",
        "url": "https://files.pythonhosted.org/packages/d0/e4/3f12f8393dd9c4fccc31bf5e63bd93cbe2c1421fc733a12a26779b41af1d/ncagg-0.4.0-py2.py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "041224e56c64fab9ac4096561735e212",
        "sha256": "dc39f439d1765b7ec93652540d9eb10d6fa1fd1f5ce02e9aae3ec80514165e51"
      },
      "downloads": -1,
      "filename": "ncagg-0.4.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "041224e56c64fab9ac4096561735e212",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 37627,
      "upload_time": "2017-11-13T17:39:37",
      "url": "https://files.pythonhosted.org/packages/d0/e4/3f12f8393dd9c4fccc31bf5e63bd93cbe2c1421fc733a12a26779b41af1d/ncagg-0.4.0-py2.py3-none-any.whl"
    }
  ]
}