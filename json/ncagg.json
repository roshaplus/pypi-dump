{
  "info": {
    "author": "Stefan Codrescu",
    "author_email": "stefan.codrescu@noaa.gov",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "NetCDF Aggregation (ncagg)\n==========================\n\nSo... you want to aggregate time series NetCDF files?\n\nTL;DR\n-----\n\nInstall the utility with with pip:\n\n::\n\n    pip install git+https://ctor.space/gitlab/work/ncagg.git\n\nOn the command line, use ``ncagg``:\n\n::\n\n    Usage: ncagg [OPTIONS] DST [SRC]...\n\n    Options:\n      -u TEXT  Give an Unlimited Dimension Configuration as udim:ivar[:hz[:hz]]\n      -b TEXT  If -u given, specify bounds for ivar as min:max. min and max should\n               be numbers, or start with T to indicate a time and then should be\n               TYYYY[MM[DD[HH[MM]]]] format.\n      --help   Show this message and exit.\n\nNotes:\n\n-  DST is the filename for the NetCDF output and should not already\n   exist, or will be overwritten.\n-  SRC is a list of input NetCDF files to aggregate.\n-  -u should specify an Unlimited Dimension Configuration. See below for\n   details.\n-  Taking tens of minutes for a day is normal, a progress bar will\n   indicate time remaining.\n\nExamples:\n\n::\n\n    #     explicitly list files to aggregate\n    ncagg output_filename.nc file_0.nc file_02.nc #...\n\n    #     aggregate by globbing all files in some directory\n    ncagg output_filename.nc path_to_files/*.nc\n\n    #     sort the unlimited dimension record_number, according to the variable time\n    ncagg -u record_number:time output_filename.nc path_to_files/*.nc\n\n    #     sort the unlimited dimension record_number, according to the variable time, and insert\n    #     or remove fill values to ensure time occurrs at 10hz.\n    ncagg -u record_number:time:10 output_filename.nc path_to_files/*.nc\n\n    #     only include time values between 2017-06-01 to 2017-06-02 (bounds), including\n    #     sorting and filling, as above\n    ncagg -u record_number:time:10 -b T20170601:T20170602 output_filename.nc path_to_files/*.nc\n    #     or equivalently, if only one bound is specified, the end is inferred to be most significant + 1\n    ncagg -u record_number:time:10 -b T20170601 output_filename.nc path_to_files/*.nc\n\nFor more information, see the Unlimited Dimension Configuration below.\nThe ``ncagg`` Command Line Interface (CLI) builds a Config based on the\narguments specified.\n\nHigh level overview\n-------------------\n\nAggregation works in two stages:\n\n1. Create an AggreList object.\n2. Evaluate the AggreList.\n\nThe AggreList object is a data structure that describes how to combine a\nset of files. The structure of the AggreList specifies the order of the\nfiles, what pieces of those files to include, fill values between files,\nas well as fill values and sorting inside files.\n\nDuring stage 1, the AggreList is generated. The level of configuration\ngiven determines how much is done here. At most, each file is inspected\naccording to it's unlimited dimension and the variable that indexes it\nto determine sorting and filling. No data except for index\\_by variables\nare read and none written to disk during this stage.\n\nDuring stage 2, the AggreList is evaluated. Evaluating the AggreList\nmeans simply iterating over it and copying data from the nodes into the\noutput aggregation file, while keeping track of global attributes.\n\nReasons for using this approach:\n\n-  Possible to aggregate more data than fits in memory.\n-  Sort once per unlimited dimension.\n-  Modular code, easier to maintain, extend, and debug.\n\nConfiguration\n-------------\n\nThe sophistication of the aggregation is determine by how much\nconfiguration information is given on generation of the AggreList.\n\n-  No Config -> agg files along unlimited dims, sorted by filename.\n-  Config with index\\_by -> agg such that index\\_by is in ascending\n   order.\n-  Config with index\\_by and expected\\_cadences -> agg and regularize,\n   removing duplicates/inserting fills if needed.\n\nThe Config contains information that a NetCDF CDL specification would,\nbut in json format, extended with aggregation configuration information.\nIf not provided, a default version will be created using the first file\nin the list to aggregate.\n\nThe Config contains three properties (keys):\n\n-  dimensions\n-  variables\n-  attributes\n\nEach property is associated with a list of objects so to preserve\nordering. The order in the objects corresponds to the order of\nappearence in the output. Objects of all sections have a \"name\"\nproperty.\n\nDimensions specify the dimensions of the file and has at minimum a\n\"name\", and a \"size\" which can be null for an unlimited dimension.\nUnlimited dimensions may also have an Unlimited Dimension Configuration\nwhich will be described in a dedicated section below.\n\nVariable objects contain a \"name\", \"dimensions\", \"datatype\",\n\"attributes\", and \"chunksizes\". The dimensions property is a list of\ndimension names on which the variable depends, each must be configured\nin the dimensions section. datatype is something like int8, float32,\nstring, etc. Finally, attributes is another property containing key and\nvalues corresponding to variable attributes commonly including \"units\",\n\"valid\\_min\", \"\\_FillValue\", etc.\n\nAttributes objects contain \"name\", \"strategy\", and optionally \"value\"\nfor NetCDF Global Attributes. The strategies are described below.\n\nUnlimited Dimension Configuration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe Unlimited Dimension Configuration associates a particular unlimited\ndimension with a variable by which it can be indexed. Commonly, a\ndimension named time is associated with a variable also named time which\nindicates some epoch value for all data associated with that index of\nthe dimension.\n\nFor example, a file may have a dimension \"record\\_number\" which is\nindexed by a variable \"time\". Using the Unlimited Dimension\nConfiguration, we can specify to aggregate record\\_number such that the\nvariable \"time\" forms a monotonic sequence increasing at some expected\nfrequency.\n\nHere is what a typical GOES-R L1b product aggregation output looks like:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"time\",\n        \"expected_cadence\": {\"report_number\": 1},\n    }\n\nIn English, the configuration above says \"Order the dimension\nreport\\_number by the values in the variable time, where time values are\nexpected to increase along the dimension report\\_number incrementing at\n1hz.\" This would be specified to the ncagg CLI using\n``ncagg -u report_number:time:1 output.nc in1.nc in2.nc``.\n\nThe configuration allows to even index by multidimensional time (ehem,\nmag with 10 samples per report). On the command line specified as\n``-u report_number:OB_time:1:10``, or as json:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"OB_time\",\n        \"other_dim_indicies\": {\"samples_per_record\": 0},\n        \"expected_cadence\": {\"report_number\": 1, \"number_samples_per_report\": 10},\n    }\n\nOne design constraint was to not reshape the data, so above, we order\nthe data by looking at index 0 of samples\\_per\\_record for every value\nalong the report\\_number dimension. We assume that the other timestamps\nalong samples\\_per\\_record are correct. Also, given the configuration\nabove, we only insert fill records of OB\\_time if a full report\\_number\nrecord is missing (all 10 values along the number\\_samples\\_per\\_report\ndimension missing).\n\n--------------\n\nIndexing an unlimited dimension was described above. In addition to\nsimply indexing by a variable, in the case that the variable represents\ntime, a common operation would be to restrict value to some range, to,\nfor example, create a day file. The Unlimited Dimension Configuration\nwould look like:\n\n.. code:: json\n\n    {\n        \"name\": \"report_number\",\n        \"size\": null,\n        \"index_by\": \"time\",\n        \"min\": 14000000,  # in units of the variable \"time\", expected\n        \"max\": 14000060,  # something like \"seconds since 2000-01-01 12:00:00\"\n        \"expected_cadence\": {\"report_number\": 1}\n    }\n\nWhich would be specified on the command line as\n``... -u report_number:time:1 -b1400000:14000060 ...`` where the ``-b``\noption stands for \"bounds\".\n\nAs min and max almost exclusively indicate datetime values, for\nconvenience, they are accepted as types: numerical, string, or python\ndatetime. In string representation, they must start with \"T\" and can be\nof the form \"TYYYY[MM[DD[HH[MM]]]]\" where brackets indicate optional and\nif omitted, will be inferred to be minimum valid value, ie: 01 for MM\n(month). A units attribute must available for the index\\_by variable in\nthe form of \" since \". On the command line, string time can be given as\n``... -u report_number:time:1 -bT20170101:T20170102 ...`` or\nequivalently the end bound can be omitted and will be inferred to be the\nrightmost specified of the beginning YYYY[MM[DD[HH[MM]]]] incremented by\none: ie: ``... -u report_number:time:1 -bT20170101 ...``.\n\n--------------\n\nConsider the suvi-l2-flloc (flare location) product which has two\nunlimited dimensions, time and feature\\_number. At any time record,\nthere can exist an arbitrary number of features. Consider a variable\nreporting the flux from each feature at each time:\n``flux(time, feature_number)``. Although feature\\_number is unlimited,\nit is unique to each time and thus needs to be \"flattened\":\n\n::\n\n    flux([0], [0]) -> [[3.2e-6]]\n    flux([0], [0, 1]) -> [[3.3e-6, 5.4e-7]]\n\n    undesired_aggregated_flux(time, feature_number):\n    [[3.2e-6,      _,      _],\n     [     _, 3.3e-6, 5.4e-7]]\n\n    desired_aggregated_flux(time, feature_number):\n    [[3.2e-6,      _],\n     [3.3e-6, 5.4e-7]]\n\nThe ``desired_aggregated_flux`` is achieved by setting {\"flatten\": true}\nwithin an the unlimited dimension configuration for feature\\_number.\n\n.. code:: json\n\n    [{\n        \"name\": \"time\",\n        \"size\": null,\n        \"index_by\": \"time\",\n    }, {\n        \"name\": \"feature_number\",\n        \"size\": null,\n        \"flatten\": true,\n    }]\n\nSpecify Global Attribute Aggregation Strategies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe aggregated result file contains global attributes formed from the\nconstituent granules. A number of strategies exist to aggregate Global\nAttributes across the granules. Most are quite self explanatory:\n\n-  \"first\": first value seen will be taken as the output value for this\n   global attribute\n-  \"last\": the last value seen will be taken as global attribute\n-  \"unique\\_list\": compile values into a unique list \"first, second,\n   etc\"\n-  \"int\\_sum\": resulting in integer sum of the inputs\n-  \"float\\_sum\": StratFloatSum\n-  \"constant\": StratAssertConst\n-  \"date\\_created\": simply yeilds the current date when finalized,\n   standard dt fmt\n-  \"time\\_coverage\\_start\": start bound, if specified, standard dt fmt\n-  \"time\\_coverage\\_end\": end bound, if specified, standard dt fmt\n-  \"filename\": StratOutputFilename\n-  \"remove\": remove/do not include this global attribute\n\nThe configuration format expects a key \"global attributes\" associated\nwith a list of objects each containing a global attribute name and\nstrategy. A list is used to preserve order, as the order in the\nconfiguration will be the resulting order in the output NetCDF.\n\n.. code:: json\n\n    {\n        \"global attributes\": [\n            {\n                \"name\": \"production_site\", \n                \"strategy\": \"unique_list\"\n            }, {\n            ...\n            }\n         ]\n    }\n\nSpecify Dimension Indecies to Extract and Flatten\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNOT IMPLEMENTED. IN PROGRESS. SUBJECT TO CHANGE.\n\nConsider SEIS SGPS files which contain the data from two sensor units,\n+X and -X. Most variables are of the form var[record\\_number,\nsensor\\_unit, channel, ...]. It is possible to create an aggregate file\nfor the +X and -X sensor units individually using the\ntake\\_dim\\_indicies configuration key.\n\n.. code:: json\n\n    {\n        \"take_dim_indicies\": {\n            \"sensor_unit\": 0\n        }\n    }\n\nWith the above configuration, sensor\\_unit must be removed from the\ndimensions configuration. Please also ensure that variables do not list\nsensor\\_unit as a dimension, and also update chunk sizes accordingly.\nChunk sizes must be a list of values of the same length as dimensions.\n\nTechnical and Implementation details\n------------------------------------\n\nAn AggreList is composed of two types of objects, InputFileNode and\nFillNode objects. These inherit in common from an AbstractNode and must\nimplement the ``get_size_along(unlimited_dim)`` and\n``data_for(var, dim)`` methods. Evaluating an aggregation list is simply\ngoing though the AggreList and calling something like:\n\n::\n\n    nc_out.variable[var][write_slice] = node.data_for(var)\n\nThe ``data_for`` must return data consistent with the shape promised\nfrom ``node.get_size_along(dim)``.\n\nThe complixity of aggregation comes in handling the dimensions and\nbuilding the aggregation list. In addition to the interface exposed by\nan AbstractNode, each InputFileNode and FillNode implement their own\nspecific functionality.\n\nA FillNode is simpler, and needs to be told how many fills to insert\nalong a certain unlimited dimension and optionally, can be configured to\nreturn values from ``data_for`` that are increasing along multiple\ndimensions according to configured ``expected_cadence`` values from a\ncertain start value.\n\nAn InputFileNode is more complicated and exposes methods to find the\ntime bounds of the file, and additionally, is internally capable of\nsorting itself and inserting fill values into itself. Of course, it\ndoesn't modify the actual input file, this is all done on the fly as\ndata is being read out through ``data_for``. Implementation wise, an\nInputFileNode may contain within itself a mini aggregation list\ncontaining two types of objects: slice and FillNode objects. Similarly\nto the large scale process of aggregating, an InputFileNode returns data\nthat has been assembled according to it's internal aggregation list and\ninternal sorting.\n\nTesting\n-------\n\nPortions of this software are unit tested, and more-so end to end tested\nin a number of scenarios using real GOES-16 satellite data. Tests are in\nthe ``test`` subdirectory. Run all tests with\n\n.. code:: bash\n\n    python -m unittest discover\n\nDevelopment\n-----------\n\nSetting up a virtualenv is recommended for development.\n\n::\n\n    virtualenv venv\n    . venv/bin/activate\n    pip install --editable .\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://ctor.space/gitlab/work/ncagg",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ncagg",
    "platform": "",
    "project_url": "https://pypi.org/project/ncagg/",
    "release_url": "https://pypi.org/project/ncagg/0.3/",
    "requires_dist": [
      "numpy",
      "netCDF4",
      "cerberus",
      "Click"
    ],
    "requires_python": "",
    "summary": "Utility for aggregation of NetCDF data.",
    "version": "0.3"
  },
  "releases": {
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "76a3b8678a40b11fa96ed2b758b61490",
          "sha256": "af2c247eda44e1ff9824e503e0047953d0b2d12b2a2e3af476faa67fb6af2e79"
        },
        "downloads": 0,
        "filename": "ncagg-0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "76a3b8678a40b11fa96ed2b758b61490",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 33424,
        "upload_time": "2017-08-23T17:38:09",
        "url": "https://files.pythonhosted.org/packages/d5/f6/249cae10548074ee977a0e161261357509a7a4e61ef82ffe3ed7ece31ee8/ncagg-0.2-py2.py3-none-any.whl"
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "ae1af422d772861d0fb9553e0474df71",
          "sha256": "852c2bcaa5016d15650213b1bdcaba46fa1d441e563a331ffdf5dd752e9dd043"
        },
        "downloads": 0,
        "filename": "ncagg-0.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ae1af422d772861d0fb9553e0474df71",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 33447,
        "upload_time": "2017-08-31T19:16:14",
        "url": "https://files.pythonhosted.org/packages/82/cf/6939f8a06d073d2ea3538e15241379ce73a1c42a8927a025b6e42949570b/ncagg-0.3-py2.py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "ae1af422d772861d0fb9553e0474df71",
        "sha256": "852c2bcaa5016d15650213b1bdcaba46fa1d441e563a331ffdf5dd752e9dd043"
      },
      "downloads": 0,
      "filename": "ncagg-0.3-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ae1af422d772861d0fb9553e0474df71",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 33447,
      "upload_time": "2017-08-31T19:16:14",
      "url": "https://files.pythonhosted.org/packages/82/cf/6939f8a06d073d2ea3538e15241379ce73a1c42a8927a025b6e42949570b/ncagg-0.3-py2.py3-none-any.whl"
    }
  ]
}