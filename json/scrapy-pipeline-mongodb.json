{
  "info": {
    "author": "Grammy Jiang",
    "author_email": "grammy.jiang@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Environment :: Plugins",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Topic :: Internet :: WWW/HTTP",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "=======================\nScrapy-Pipeline-MongoDB\n=======================\n\n.. image:: https://img.shields.io/pypi/v/scrapy-pipeline-mongodb.svg\n   :target: https://pypi.python.org/pypi/scrapy-pipeline-mongodb\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/travis/grammy-jiang/scrapy-pipeline-mongodb/master.svg\n   :target: http://travis-ci.org/grammy-jiang/scrapy-pipeline-mongodb\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/scrapy-pipeline-mongodb\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/grammy-jiang/scrapy-pipeline-mongodb/master.svg\n   :target: http://codecov.io/github/grammy-jiang/scrapy-pipeline-mongodb?branch=master\n   :alt: Coverage report\n\nOverview\n========\n\nScrapy is a great framework for web crawling. This package provides two\npipelines to save items into MongoDB in a async or sync way. And also provide a\na highly customized way to interact with MongoDB in a async or sync way.\n\n* Save an item and get Object ID from this pipeline\n\n* Update an item and get Object ID from this pipeline\n\nRequirements\n============\n\n* Txmongo, a async MongoDB driver with Twisted\n\n* Not support Python 2.7\n\n* Tests on Python 3.5, but it should work on other version higher then Python\n  3.3\n\n* Tests on Linux, but it's a pure python module, it should work on other\n  platforms with official python and Twisted supported, e.g. Windows, Mac OSX,\n  BSD\n\nInstallation\n============\n\nThe quick way::\n\n    pip install scrapy-pipeline-mongodb\n\nOr put this middleware just beside the scrapy project.\n\nDocumentation\n=============\n\nBlock Inspector in spider middleware, in ``settings.py``, for example::\n\n    from txmongo.filter import ASCENDING\n    from txmongo.filter import DESCENDING\n\n    # -----------------------------------------------------------------------------\n    # PIPELINE MONGODB ASYNC\n    # -----------------------------------------------------------------------------\n\n    ITEM_PIPELINES.update({\n        'scrapy_pipeline_mongodb.pipelines.mongodb_async.PipelineMongoDBAsync': 500,\n    })\n\n    MONGODB_USERNAME = 'user'\n    MONGODB_PASSWORD = 'password'\n    MONGODB_HOST = 'localhost'\n    MONGODB_PORT = 27017\n    MONGODB_DATABASE = 'test_mongodb_async_db'\n    MONGODB_COLLECTION = 'test_mongodb_async_coll'\n\n    # MONGODB_OPTIONS_ = 'MONGODB_OPTIONS_'\n\n    MONGODB_INDEXES = [('field_0', ASCENDING, {'unique': True}),\n                       (('field_0', 'field_1'), ASCENDING, {}),\n                       (('field_0', ASCENDING, {}), ('field_0', DESCENDING, {}))]\n\n    MONGODB_PROCESS_ITEM = 'scrapy_pipeline_mongodb.utils.process_item.process_item'\n\n\nSettings Reference\n==================\n\nMONGODB_USERNAME\n----------------\n\nA string of the username of the database.\n\nMONGODB_PASSWORD\n----------------\n\nA string of the password of the database.\n\nMONGODB_HOST\n------------\n\nA string of the ip address or the domain of the database.\n\nMONGODB_PORT\n------------\n\nA int of the port of the database.\n\nMONGODB_DATABASE\n----------------\n\nA string of the name of the database.\n\nMONGODB_COLLECTION\n------------------\n\nA list of the indexes to create on the collection.\n\nMONGODB_OPTIONS_*\n-----------------\n\nOptions can be attached when the pipeline start to connect to MongoBD.\n\nIf any options are needed, the name of the option can be with the prefix\n`MONGODB_OPTIONS_`, the pipeline will parse it.\n\nFor example:\n\n+---------------+-------------------------------+\n| option name   | in `settings.py`              |\n+---------------+-------------------------------+\n| authMechanism | MONGODB_OPTIONS_authMechanism |\n+---------------+-------------------------------+\n\n\nFor more options, please refer to the page:\n\n`Connection String URI Format \u2014 MongoDB Manual 3.4`_\n\n.. _`Connection String URI Format \u2014 MongoDB Manual 3.4`: https://docs.mongodb.com/manual/reference/connection-string/#connections-standard-connection-string-format\n\nMONGODB_INDEXES\n---------------\n\nA list of the indexes defined in this setting will be created when the spider is\nopen.\n\nIf the index has already existed, there will be no warning or error raised.\n\nMONGODB_PROCESS_ITEM\n--------------------\n\nTo highly customize to interact with MongoDB, this pipeline provide a setting to\ndefine the function `process_item`. And with this package, there is one default\nfunction: just call the method `insert_one` of the collection to save the item\ninto MongoDB, then return the item.\n\nIf a customize is provided to replace the default one, please note the behavior\nshould follow the requirement which is clearly written in the scrapy documents:\n\n`Item Pipeline \u2014 Scrapy 1.4.0 documentation`_\n\n.. _`Item Pipeline \u2014 Scrapy 1.4.0 documentation`: https://doc.scrapy.org/en/latest/topics/item-pipeline.html#writing-your-own-item-pipelin\n\nBuild-in Functions For Processing Item\n======================================\n\nscrapy_pipeline_mongodb.utils.process_item.process_item\n-------------------------------------------------------\n\nThis is a build-in function to call the method `insert_one` of the collection,\nand return the item.\n\nTo use this function, in `settings.py`::\n\n    MONGODB_PROCESS_ITEM = 'scrapy_pipeline_mongodb.utils.process_item.process_item'\n\nNOTE\n====\n\nThe drivers may have different api for the same operation, this pipeline adopts\ntxmongo as the async driver for MongoDB, please read the relative documents to\nmake sure the customized functions can run fluently in this pipeline.\n\nTODO\n====\n* Add a unit test for the index created function\n\n* Add a sync pipeline\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/grammy-jiang/scrapy-pipeline-mongodb",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-pipeline-mongodb",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-pipeline-mongodb/",
    "release_url": "https://pypi.org/project/scrapy-pipeline-mongodb/0.0.3/",
    "requires_dist": [
      "scrapy (>=1.4.0)",
      "txmongo"
    ],
    "requires_python": "",
    "summary": "",
    "version": "0.0.3"
  },
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "813752bb8c67cf0bd5ad8a29de027b5c",
          "sha256": "ebf5618507fb3f317280f1cd8432b49e4da1b5bc29138ddd4619ff09f4170f90"
        },
        "downloads": -1,
        "filename": "scrapy_pipeline_mongodb-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "813752bb8c67cf0bd5ad8a29de027b5c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 11137,
        "upload_time": "2017-10-27T03:33:33",
        "url": "https://files.pythonhosted.org/packages/08/5a/dc90c44c87202b7ff1132da2e5a2f1b76ba3708b3f1b808f43560d71db49/scrapy_pipeline_mongodb-0.0.1-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "b57e267b30a484ca3622c77e25750d41",
          "sha256": "0e97fe3a23628eb914581df17e778580d50b4c5e9de0ab123deae6e868bb25ba"
        },
        "downloads": -1,
        "filename": "scrapy-pipeline-mongodb-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b57e267b30a484ca3622c77e25750d41",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23899,
        "upload_time": "2017-10-27T03:33:49",
        "url": "https://files.pythonhosted.org/packages/07/07/c5a44c7b4f54ad014abb7a3025d0271fd257c6883b90063f9a03532d63c5/scrapy-pipeline-mongodb-0.0.1.tar.gz"
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "5407251cf5e355224bb067adc2c0faf8",
          "sha256": "27b5eb49c63e92c86a2bda92206f04c9a4bfa2d48419dd2a997549457dd68c8f"
        },
        "downloads": -1,
        "filename": "scrapy_pipeline_mongodb-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5407251cf5e355224bb067adc2c0faf8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 11187,
        "upload_time": "2017-10-27T09:15:52",
        "url": "https://files.pythonhosted.org/packages/e6/3a/95a64731325497c4c8bc4209a3092de62335c018941ab3b0c965026c99d5/scrapy_pipeline_mongodb-0.0.3-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "0c2d2bacfdc494ced8b0d36dba985a63",
          "sha256": "73856e2c245133a7e2640842b2edc3cdc55a4e1465cd40bc8f06170befec46a4"
        },
        "downloads": -1,
        "filename": "scrapy-pipeline-mongodb-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "0c2d2bacfdc494ced8b0d36dba985a63",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23947,
        "upload_time": "2017-10-27T09:15:55",
        "url": "https://files.pythonhosted.org/packages/91/e2/3951970b7c5cf7ed7d36f88e623d0c0982f6327e78e4c699a1fe60730cb4/scrapy-pipeline-mongodb-0.0.3.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "5407251cf5e355224bb067adc2c0faf8",
        "sha256": "27b5eb49c63e92c86a2bda92206f04c9a4bfa2d48419dd2a997549457dd68c8f"
      },
      "downloads": -1,
      "filename": "scrapy_pipeline_mongodb-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5407251cf5e355224bb067adc2c0faf8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 11187,
      "upload_time": "2017-10-27T09:15:52",
      "url": "https://files.pythonhosted.org/packages/e6/3a/95a64731325497c4c8bc4209a3092de62335c018941ab3b0c965026c99d5/scrapy_pipeline_mongodb-0.0.3-py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "0c2d2bacfdc494ced8b0d36dba985a63",
        "sha256": "73856e2c245133a7e2640842b2edc3cdc55a4e1465cd40bc8f06170befec46a4"
      },
      "downloads": -1,
      "filename": "scrapy-pipeline-mongodb-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "0c2d2bacfdc494ced8b0d36dba985a63",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 23947,
      "upload_time": "2017-10-27T09:15:55",
      "url": "https://files.pythonhosted.org/packages/91/e2/3951970b7c5cf7ed7d36f88e623d0c0982f6327e78e4c699a1fe60730cb4/scrapy-pipeline-mongodb-0.0.3.tar.gz"
    }
  ]
}