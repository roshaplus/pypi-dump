{
  "info": {
    "author": "Kensuke Mitsuzawa",
    "author_email": "kensuke.mit@gmail.com",
    "bugtrack_url": "",
    "classifiers": [],
    "description": "What's this?\n============\n\nThis is simple wrapper for Japanese Tokenizers(A.K.A Morphology\nSplitter)\n\nThis project aims to call Tokenizer and split into tokens as easy as\npossible.\n\nAnd this project supports various Tokenization tools. You can compare\nresults among them.\n\nThis project is available also in\n`Github <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers>`__.\n\nIf you find any bugs, please report them to github issues. Or any pull\nrequests are welcomed!\n\nRequirements\n============\n\n-  Python 2.7\n-  Python 3.5\n\nFeatures\n========\n\n-  You can get set of tokens from input sentence\n-  You can filter some tokens with your Part-of-Speech condition or\n   stopwords\n-  You can add extension dictionary like mecab-neologd dictionary\n-  You can define your original dictionary. And this dictionary forces\n   mecab to make it one token\n\nSupported Tokenization tool\n---------------------------\n\nMecab\n~~~~~\n\n`Mecab <http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html?sess=3f6a4f9896295ef2480fa2482de521f6>`__\nis open source tokenizer system for various language(if you have\ndictionary for it)\n\nSee `english\ndocumentation <https://github.com/jordwest/mecab-docs-en>`__ for detail\n\nJuman\n~~~~~\n\n`Juman <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN>`__ is\ntokenizer tool developped by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nJuman++\n~~~~~~~\n\n`Juman++ <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++>`__ is\ntokenizer developped by Kurohashi laboratory, Kyoto University, Japan.\n\nJuman++ is succeeding system of Juman. It adopts RNN model for\ntokenization.\n\nJuman++ is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nKytea\n~~~~~\n\n`Kytea <http://www.phontron.com/kytea/>`__ is tokenizer tool developped\nby Graham Neubig.\n\nKytea has a different algorithm from one of Mecab or Juman.\n\nSetting up\n==========\n\nTokenizers auto-install\n-----------------------\n\n::\n\n    make install\n\nmecab-neologd dictionary auto-install\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n    make install_neologd\n\nTokenizers manual-install\n-------------------------\n\nMeCab\n~~~~~\n\nSee `here <https://github.com/jordwest/mecab-docs-en>`__ to install\nMeCab system.\n\nMecab Neologd dictionary\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nMecab-neologd dictionary is a dictionary-extension based on\nipadic-dictionary, which is basic dictionary of Mecab.\n\nWith, Mecab-neologd dictionary, you're able to parse new-coming words\nmake one token.\n\nHere, new-coming words is such like, movie actor name or company\nname.....\n\nSee `here <https://github.com/neologd/mecab-ipadic-neologd>`__ and\ninstall mecab-neologd dictionary.\n\nJuman\n~~~~~\n\n::\n\n    wget -O juman7.0.1.tar.bz2 \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/juman/juman-7.01.tar.bz2&name=juman-7.01.tar.bz2\"\n    bzip2 -dc juman7.0.1.tar.bz2  | tar xvf -\n    cd juman-7.01\n    ./configure\n    make   \n    [sudo] make install\n\nJuman++\n-------\n\n-  GCC version must be >= 5\n\n::\n\n    wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.01.tar.xz\n    tar xJvf jumanpp-1.01.tar.xz\n    cd jumanpp-1.01/\n    ./configure\n    make\n    [sudo] make install\n\nKytea\n-----\n\nInstall Kytea system\n\n::\n\n    wget http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz\n    tar -xvf kytea-0.4.7.tar\n    cd kytea-0.4.7\n    ./configure\n    make\n    make install\n\nKytea has `python wrapper <https://github.com/chezou/Mykytea-python>`__\nthanks to michiaki ariga. Install Kytea-python wrapper\n\n::\n\n    pip install kytea\n\ninstall\n-------\n\n::\n\n    [sudo] python setup.py install\n\nNote\n~~~~\n\nDuring install, you see warning message when it fails to install\n``pyknp`` or ``kytea``.\n\nif you see these messages, try to re-install these packages manually.\n\nUsage\n=====\n\nTokenization Example(For python2x. To see exmaple code for Python3.x,\nplaese see\n`here <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers/blob/master/examples/examples.py>`__)\n\n::\n\n    # input is `unicode` type(in python2x)\n    sentence = u'\u30c6\u30d8\u30e9\u30f3\uff08\u30da\u30eb\u30b7\u30a2\u8a9e: \u062a\u0647\u0631\u0627\u0646  ; Tehr\u0101n Tehran.ogg \u767a\u97f3[\u30d8\u30eb\u30d7/\u30d5\u30a1\u30a4\u30eb]/te\u0266\u02c8r\u0254\u02d0n/\u3001\u82f1\u8a9e:Tehran\uff09\u306f\u3001\u897f\u30a2\u30b8\u30a2\u3001\u30a4\u30e9\u30f3\u306e\u9996\u90fd\u3067\u3042\u308a\u304b\u3064\u30c6\u30d8\u30e9\u30f3\u5dde\u306e\u5dde\u90fd\u3002\u4eba\u53e312,223,598\u4eba\u3002\u90fd\u5e02\u570f\u4eba\u53e3\u306f13,413,348\u4eba\u306b\u9054\u3059\u308b\u3002'\n\n    # make MecabWrapper object\n    # path where `mecab-config` command exists. You can check it with `which mecab-config`\n    # default value is '/usr/local/bin'\n    path_mecab_config='/usr/local/bin'\n\n    # you can choose from \"neologd\", \"all\", \"ipaddic\", \"user\", \"\"\n    # \"ipadic\" and \"\" is equivalent\n    dictType = \"\"\n\n    mecab_wrapper = MecabWrapper(dictType=dictType, path_mecab_config=path_mecab_config)\n\n    # tokenize sentence. Returned object is list of tuples\n    tokenized_obj = mecab_wrapper.tokenize(sentence=sentence)\n    assert isinstance(tokenized_obj, list)\n\n    # Returned object is \"TokenizedSenetence\" class if you put return_list=False\n    tokenized_obj = mecab_wrapper.tokenize(sentence=sentence, return_list=False)\n\nFiltering example\n\n::\n\n    stopwords = [u'\u30c6\u30d8\u30e9\u30f3']\n    assert isinstance(tokenized_obj, TokenizedSenetence)\n    # returned object is \"FilteredObject\" class\n    filtered_obj = mecab_wrapper.filter(\n        parsed_sentence=tokenized_obj,\n        stopwords=stopwords\n    )\n    assert isinstance(filtered_obj, FilteredObject)\n\n    # pos condition is list of tuples\n    # You can set POS condition \"ChaSen \u54c1\u8a5e\u4f53\u7cfb (IPA\u54c1\u8a5e\u4f53\u7cfb)\" of this page http://www.unixuser.org/~euske/doc/postag/#chasen\n    pos_condition = [(u'\u540d\u8a5e', u'\u56fa\u6709\u540d\u8a5e'), (u'\u52d5\u8a5e', u'\u81ea\u7acb')]\n    filtered_obj = mecab_wrapper.filter(\n        parsed_sentence=tokenized_obj,\n        pos_condition=pos_condition\n    )\n\nPart-of-speech structure\n------------------------\n\nMecab, Juman uses different system of Part-of-Speech(POS).\n\nKeep in your mind when you use it.\n\nYou can check tables of Part-of-Speech(POS)\n`here <http://www.unixuser.org/~euske/doc/postag/>`__\n\nSimilar Package\n===============\n\nnatto-py\n--------\n\nnatto-py is sophisticated package for tokenization. It supports\nfollowing features\n\n-  easy interface for tokenization\n-  importing additional dictionary\n-  partial parsing mode\n\nCHANGES\n=======\n\n0.6(2016-03-05)\n---------------\n\n-  first release to Pypi\n\n0.7(2016-03-06)\n---------------\n\n-  Juman supports(only for python2.x)\n-  Kytea supports(only for python2.x)\n\n0.8(2016-04-03)\n---------------\n\n-  removed a bug when interface calls JUMAN\n-  fixed the version number of jctconv\n\n0.9 (2016-04-05)\n----------------\n\n-  Kytea supports also for Python3.x(Thanks to @chezou)\n\n1.0 (2016-06-19)\n----------------\n\n-  Juman supports also for Python3.x\n\n1.2.5 (2016-12-28)\n------------------\n\n-  It fixed bugs in Juman server mode in python3.x\n-  It supports Juman++\n-  It supports ``filter`` method with chain expression\n\n1.2.6 (2017-01-12)\n------------------\n\n-  It introduced a paramter on text normalization function\n\n   -  All ``\\n`` strings are converted into ``\u3002``. This is because\n      ``\\n`` string in input-text causes tokenization error especially\n      with server-mode.\n\n1.2.8 (2017-02-22)\n------------------\n\n-  It has make file for installing tokenizers.\n-  It is tested with travis.\n\n1.3.0 (2017-02-23)\n------------------\n\n-  It introduced de-normalization function after tokenization process.\n   (\u5168\u89d2\u82f1\u6570 -> \u534a\u89d2\u82f1\u6570)\n-  For mecab-config, it detects path to mecab-config automatically\n-  It fixed a bug of initializing juman-object in python2\n\nLICENSE\n=======\n\nMIT license\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers",
    "keywords": "MeCab",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "JapaneseTokenizer",
    "platform": "",
    "project_url": "https://pypi.org/project/JapaneseTokenizer/",
    "release_url": "https://pypi.org/project/JapaneseTokenizer/1.3.1/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "",
    "version": "1.3.1"
  },
  "releases": {
    "0.6a1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e55f95650df77ef126ee23b2cc88ad20",
          "sha256": "c15a23d01f1ad997049e1f89333bb421b98fd5f0a2fbe7ae005662a9cd5a383a"
        },
        "downloads": 219,
        "filename": "JapaneseTokenizer-0.6a1.tar.gz",
        "has_sig": false,
        "md5_digest": "e55f95650df77ef126ee23b2cc88ad20",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 10587,
        "upload_time": "2016-03-05T15:16:52",
        "url": "https://files.pythonhosted.org/packages/5b/96/cc92357c7e7261c791db6f769bda1cd2c1f4547ea64bd1b75227c2818643/JapaneseTokenizer-0.6a1.tar.gz"
      }
    ],
    "0.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "bdba410c246604b53599e6431ee8ba97",
          "sha256": "82a76d0309a31c09f653b3ba0d1ac99e42a0c9a80e55b5f48b52072a30c84214"
        },
        "downloads": 221,
        "filename": "JapaneseTokenizer-0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "bdba410c246604b53599e6431ee8ba97",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15964,
        "upload_time": "2016-03-06T05:06:38",
        "url": "https://files.pythonhosted.org/packages/c6/7a/e5b1c02a0e1c055d17f171fe04925556f419b59f871902a77f6e83a048af/JapaneseTokenizer-0.7.tar.gz"
      }
    ],
    "0.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "aecb2a18cabf7ab5fcae6dd5147390e8",
          "sha256": "35297b98855676f04f910e4025e7888be9ef69bc3bfb2fad5dc9b34740053274"
        },
        "downloads": 207,
        "filename": "JapaneseTokenizer-0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "aecb2a18cabf7ab5fcae6dd5147390e8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15860,
        "upload_time": "2016-04-02T19:53:42",
        "url": "https://files.pythonhosted.org/packages/13/61/6586dc1b41cdcf05f1f49ea3d92cc4e14341c7879dcce33fab693383da25/JapaneseTokenizer-0.8.tar.gz"
      }
    ],
    "0.9": [
      {
        "comment_text": "",
        "digests": {
          "md5": "965d1893a35b65ed9c81d09492834f3b",
          "sha256": "f7b4ea9753a8e8a5894ba0ccc1f0b9713aa78f62cc7dc49dbb4fa86ef06c14de"
        },
        "downloads": 501,
        "filename": "JapaneseTokenizer-0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "965d1893a35b65ed9c81d09492834f3b",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16284,
        "upload_time": "2016-04-04T17:23:05",
        "url": "https://files.pythonhosted.org/packages/d5/6a/e52ba521d54169b40acf7cb025b68e1e7119e28da6be9416c99054111bcd/JapaneseTokenizer-0.9.tar.gz"
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "2b10f181b00cc626eed106965163d8e3",
          "sha256": "71f5675bd8b6815fcc8242f48be9a4b6197294e2626683a7431380469cfc78f6"
        },
        "downloads": 281,
        "filename": "JapaneseTokenizer-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2b10f181b00cc626eed106965163d8e3",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17118,
        "upload_time": "2016-08-03T04:28:53",
        "url": "https://files.pythonhosted.org/packages/54/b5/79590c298941926efd0b4fbe4906979bfcb16199dd371e46d2a88144b8c4/JapaneseTokenizer-1.0.tar.gz"
      }
    ],
    "1.0a0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "87fa79c3609690d6b86295c494af7d6f",
          "sha256": "fa3ec1860b5df9688d0d2c0037af19b568be2663e8b06e9814effb258cf82a4b"
        },
        "downloads": 260,
        "filename": "JapaneseTokenizer-1.0a0.tar.gz",
        "has_sig": false,
        "md5_digest": "87fa79c3609690d6b86295c494af7d6f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17105,
        "upload_time": "2016-06-19T08:18:31",
        "url": "https://files.pythonhosted.org/packages/82/47/8727b3a859e6c3cff4c3fdb2a5fc068d355bd346a22db38730e3ae9bd1ce/JapaneseTokenizer-1.0a0.tar.gz"
      }
    ],
    "1.0b0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
          "sha256": "fdb6b46290ae0907e62d16d32c762b6c7f4f433619c22c34519e32d6be253d21"
        },
        "downloads": 177,
        "filename": "JapaneseTokenizer-1.0b0.tar.gz",
        "has_sig": false,
        "md5_digest": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17113,
        "upload_time": "2016-06-22T02:34:50",
        "url": "https://files.pythonhosted.org/packages/b6/b2/089232b3c76d3e802e0fd1c17eb7d96f9bbdc4034d8a55b3bfa18d241d64/JapaneseTokenizer-1.0b0.tar.gz"
      }
    ],
    "1.2.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "da97ce49089ebc76f08aa476a9c1cdce",
          "sha256": "7d9540ee440c9393bdf4232cc694d5bc8b6ba60b5bbde87edaf06381d046263d"
        },
        "downloads": 70,
        "filename": "JapaneseTokenizer-1.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "da97ce49089ebc76f08aa476a9c1cdce",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16609,
        "upload_time": "2016-12-08T18:25:46",
        "url": "https://files.pythonhosted.org/packages/87/2b/6ad05fb2afe606550828fba2c1da98629bf2f34a5523535d7214ea9c15c0/JapaneseTokenizer-1.2.3.tar.gz"
      }
    ],
    "1.2.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "30d57d0c20f73a976f3be343738f5f55",
          "sha256": "2ea333d144fd9f0ba44e6001d74d63855beb3a30a4c87924938130664231518f"
        },
        "downloads": 284,
        "filename": "JapaneseTokenizer-1.2.5.tar.gz",
        "has_sig": false,
        "md5_digest": "30d57d0c20f73a976f3be343738f5f55",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 19943,
        "upload_time": "2016-12-28T07:03:26",
        "url": "https://files.pythonhosted.org/packages/2a/26/0c91f48b42d8059f969c2ab9a56b29b351eac15a139744f17fb98fa0accb/JapaneseTokenizer-1.2.5.tar.gz"
      }
    ],
    "1.2.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e6389d1aa28127631fb9595e19a6fd23",
          "sha256": "f2010906f997bc36db4fddb438abaf6d6380bca065beefd2859428fda0a5c0f4"
        },
        "downloads": 11,
        "filename": "JapaneseTokenizer-1.2.6.tar.gz",
        "has_sig": false,
        "md5_digest": "e6389d1aa28127631fb9595e19a6fd23",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 20216,
        "upload_time": "2017-01-11T21:26:02",
        "url": "https://files.pythonhosted.org/packages/eb/9c/9197907e3cd0c13f03cd887efef234bc0fe359a05a844e7b82405ddab282/JapaneseTokenizer-1.2.6.tar.gz"
      }
    ],
    "1.2.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4caa2f2fd7e0a4e71d0d95b319e2f960",
          "sha256": "bddfc293a9c221f37b631c226568361c2599e74d49c4e0cb57ff859df725d4b8"
        },
        "downloads": 13,
        "filename": "JapaneseTokenizer-1.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "4caa2f2fd7e0a4e71d0d95b319e2f960",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 20224,
        "upload_time": "2017-01-13T00:37:10",
        "url": "https://files.pythonhosted.org/packages/08/ed/e970d4d49554e7ddf5ed3edec1e5f1d8265c47e44c6de0b4e71191a15c7d/JapaneseTokenizer-1.2.7.tar.gz"
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "13b4072229b14619ae7b8cd672c47fbe",
          "sha256": "b51dd61c3a4192bb70d47310e67d42b29b8e6ec7702b556f4b718836fc772ccf"
        },
        "downloads": 22,
        "filename": "JapaneseTokenizer-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "13b4072229b14619ae7b8cd672c47fbe",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23199,
        "upload_time": "2017-02-23T03:41:02",
        "url": "https://files.pythonhosted.org/packages/79/19/c950a7b3ba817a9fc56e9d44d6698a401e876f54de404a760f8a1100a34a/JapaneseTokenizer-1.3.0.tar.gz"
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f70e1e0d82111714c1958084eef1c813",
          "sha256": "3608d6135eb00d6b59fccf675c894ff978f22359ed37a99fde8bb006b6d948f0"
        },
        "downloads": 0,
        "filename": "JapaneseTokenizer-1.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f70e1e0d82111714c1958084eef1c813",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 28932,
        "upload_time": "2017-06-29T11:05:09",
        "url": "https://files.pythonhosted.org/packages/c0/06/e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5/JapaneseTokenizer-1.3.1.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "f70e1e0d82111714c1958084eef1c813",
        "sha256": "3608d6135eb00d6b59fccf675c894ff978f22359ed37a99fde8bb006b6d948f0"
      },
      "downloads": 0,
      "filename": "JapaneseTokenizer-1.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "f70e1e0d82111714c1958084eef1c813",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 28932,
      "upload_time": "2017-06-29T11:05:09",
      "url": "https://files.pythonhosted.org/packages/c0/06/e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5/JapaneseTokenizer-1.3.1.tar.gz"
    }
  ]
}