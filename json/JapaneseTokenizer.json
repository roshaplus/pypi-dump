{
  "info": {
    "author": "Kensuke Mitsuzawa",
    "author_email": "kensuke.mit@gmail.com",
    "bugtrack_url": "",
    "classifiers": [],
    "description": "|MIT License|\\ |Build Status|\n\nWhat's this?\n============\n\nThis is simple python-wrapper for Japanese Tokenizers(A.K.A Tokenizer)\n\nThis project aims to call tokenizers and split a sentence into tokens as\neasy as possible.\n\nAnd, this project supports various Tokenization tools common interface.\nThus, it's easy to compare output from various tokenizers.\n\nThis project is available also in\n`Github <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers>`__.\n\nIf you find any bugs, please report them to github issues. Or any pull\nrequests are welcomed!\n\nRequirements\n============\n\n-  Python 2.7\n-  Python 3.5\n\nFeatures\n========\n\n-  simple/common interface among various tokenizers\n-  simple/common interface for filtering with stopwords or\n   Part-of-Speech condition\n-  simple interface to add user-dictionary(mecab only)\n\nSupported Tokenizers\n--------------------\n\nMecab\n~~~~~\n\n`Mecab <http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html?sess=3f6a4f9896295ef2480fa2482de521f6>`__\nis open source tokenizer system for various language(if you have\ndictionary for it)\n\nSee `english\ndocumentation <https://github.com/jordwest/mecab-docs-en>`__ for detail\n\nJuman\n~~~~~\n\n`Juman <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman is strong for ambiguous writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nJuman++\n~~~~~~~\n\n`Juman++ <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman++ is succeeding system of Juman. It adopts RNN model for\ntokenization.\n\nJuman++ is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nKytea\n~~~~~\n\n`Kytea <http://www.phontron.com/kytea/>`__ is tokenizer tool developped\nby Graham Neubig.\n\nKytea has a different algorithm from one of Mecab or Juman.\n\nSetting up\n==========\n\nTokenizers auto-install\n-----------------------\n\n::\n\n    make install\n\nmecab-neologd dictionary auto-install\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n    make install_neologd\n\nTokenizers manual-install\n-------------------------\n\nMeCab\n~~~~~\n\nSee `here <https://github.com/jordwest/mecab-docs-en>`__ to install\nMeCab system.\n\nMecab Neologd dictionary\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nMecab-neologd dictionary is a dictionary-extension based on\nipadic-dictionary, which is basic dictionary of Mecab.\n\nWith, Mecab-neologd dictionary, you're able to parse new-coming words\nmake one token.\n\nHere, new-coming words is such like, movie actor name or company\nname.....\n\nSee `here <https://github.com/neologd/mecab-ipadic-neologd>`__ and\ninstall mecab-neologd dictionary.\n\nJuman\n~~~~~\n\n::\n\n    wget -O juman7.0.1.tar.bz2 \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/juman/juman-7.01.tar.bz2&name=juman-7.01.tar.bz2\"\n    bzip2 -dc juman7.0.1.tar.bz2  | tar xvf -\n    cd juman-7.01\n    ./configure\n    make   \n    [sudo] make install\n\nJuman++\n-------\n\n-  GCC version must be >= 5\n\n::\n\n    wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.02.tar.xz\n    tar xJvf jumanpp-1.02.tar.xz\n    cd jumanpp-1.02/\n    ./configure\n    make\n    [sudo] make install\n\nKytea\n-----\n\nInstall Kytea system\n\n::\n\n    wget http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz\n    tar -xvf kytea-0.4.7.tar\n    cd kytea-0.4.7\n    ./configure\n    make\n    make install\n\nKytea has `python wrapper <https://github.com/chezou/Mykytea-python>`__\nthanks to michiaki ariga. Install Kytea-python wrapper\n\n::\n\n    pip install kytea\n\ninstall\n-------\n\n::\n\n    [sudo] python setup.py install\n\nNote\n~~~~\n\nDuring install, you see warning message when it fails to install\n``pyknp`` or ``kytea``.\n\nif you see these messages, try to re-install these packages manually.\n\nUsage\n=====\n\nTokenization Example(For python3.x. To see exmaple code for Python2.x,\nplaese see\n`here <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers/blob/master/examples/examples.py>`__)\n\n::\n\n    import JapaneseTokenizer\n    input_sentence = '10\u65e5\u653e\u9001\u306e\u300c\u4e2d\u5c45\u6b63\u5e83\u306e\u30df\u306b\u306a\u308b\u56f3\u66f8\u9928\u300d\uff08\u30c6\u30ec\u30d3\u671d\u65e5\u7cfb\uff09\u3067\u3001SMAP\u306e\u4e2d\u5c45\u6b63\u5e83\u304c\u3001\u7be0\u539f\u4fe1\u4e00\u306e\u904e\u53bb\u306e\u52d8\u9055\u3044\u3092\u660e\u304b\u3059\u4e00\u5e55\u304c\u3042\u3063\u305f\u3002'\n    # ipadic is well-maintained dictionary #\n    mecab_wrapper = JapaneseTokenizer.MecabWrapper(dictType='ipadic')\n    print(mecab_wrapper.tokenize(input_sentence).convert_list_object())\n\n    # neologd is automatically-generated dictionary from huge web-corpus #\n    mecab_neologd_wrapper = JapaneseTokenizer.MecabWrapper(dictType='neologd')\n    print(mecab_neologd_wrapper.tokenize(input_sentence).convert_list_object())\n\nFiltering example\n-----------------\n\n::\n\n    import JapaneseTokenizer\n    # with word filtering by stopword & part-of-speech condition #\n    print(mecab_wrapper.tokenize(input_sentence).filter(stopwords=['\u30c6\u30ec\u30d3\u671d\u65e5'], pos_condition=[('\u540d\u8a5e', '\u56fa\u6709\u540d\u8a5e')]).convert_list_object())\n\nPart-of-speech structure\n------------------------\n\nMecab, Juman, Kytea have different system of Part-of-Speech(POS).\n\nYou can check tables of Part-of-Speech(POS)\n`here <http://www.unixuser.org/~euske/doc/postag/>`__\n\nSimilar Package\n===============\n\nnatto-py\n--------\n\nnatto-py is sophisticated package for tokenization. It supports\nfollowing features\n\n-  easy interface for tokenization\n-  importing additional dictionary\n-  partial parsing mode\n\nLICENSE\n=======\n\nMIT license\n\nCHANGES\n=======\n\n0.6(2016-03-05)\n---------------\n\n-  first release to Pypi\n\n0.7(2016-03-06)\n---------------\n\n-  Juman supports(only for python2.x)\n-  Kytea supports(only for python2.x)\n\n0.8(2016-04-03)\n---------------\n\n-  removed a bug when interface calls JUMAN\n-  fixed the version number of jctconv\n\n0.9 (2016-04-05)\n----------------\n\n-  Kytea supports also for Python3.x(Thanks to @chezou)\n\n1.0 (2016-06-19)\n----------------\n\n-  Juman supports also for Python3.x\n\n1.2.5 (2016-12-28)\n------------------\n\n-  It fixed bugs in Juman server mode in python3.x\n-  It supports Juman++\n-  It supports ``filter`` method with chain expression\n\n1.2.6 (2017-01-12)\n------------------\n\n-  It introduced a paramter on text normalization function\n\n   -  All ``\\n`` strings are converted into ``\u3002``. This is because\n      ``\\n`` string in input-text causes tokenization error especially\n      with server-mode.\n\n1.2.8 (2017-02-22)\n------------------\n\n-  It has make file for installing tokenizers.\n-  It is tested with travis.\n\n1.3.0 (2017-02-23)\n------------------\n\n-  It introduced de-normalization function after tokenization process.\n   (\u5168\u89d2\u82f1\u6570 -> \u534a\u89d2\u82f1\u6570)\n-  For mecab-config, it detects path to mecab-config automatically\n-  It fixed a bug of initializing juman-object in python2\n\nafter 1.3.0\n-----------\n\nchange logs are in github release.\n\n.. |MIT License| image:: http://img.shields.io/badge/license-MIT-blue.svg?style=flat\n   :target: LICENSE\n.. |Build Status| image:: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers.svg?branch=master\n   :target: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers",
    "keywords": "MeCab",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "JapaneseTokenizer",
    "platform": "",
    "project_url": "https://pypi.org/project/JapaneseTokenizer/",
    "release_url": "https://pypi.org/project/JapaneseTokenizer/1.3.6/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "",
    "version": "1.3.6"
  },
  "releases": {
    "0.6a1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e55f95650df77ef126ee23b2cc88ad20",
          "sha256": "c15a23d01f1ad997049e1f89333bb421b98fd5f0a2fbe7ae005662a9cd5a383a"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.6a1.tar.gz",
        "has_sig": false,
        "md5_digest": "e55f95650df77ef126ee23b2cc88ad20",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 10587,
        "upload_time": "2016-03-05T15:16:52",
        "url": "https://files.pythonhosted.org/packages/5b/96/cc92357c7e7261c791db6f769bda1cd2c1f4547ea64bd1b75227c2818643/JapaneseTokenizer-0.6a1.tar.gz"
      }
    ],
    "0.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "bdba410c246604b53599e6431ee8ba97",
          "sha256": "82a76d0309a31c09f653b3ba0d1ac99e42a0c9a80e55b5f48b52072a30c84214"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "bdba410c246604b53599e6431ee8ba97",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15964,
        "upload_time": "2016-03-06T05:06:38",
        "url": "https://files.pythonhosted.org/packages/c6/7a/e5b1c02a0e1c055d17f171fe04925556f419b59f871902a77f6e83a048af/JapaneseTokenizer-0.7.tar.gz"
      }
    ],
    "0.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "aecb2a18cabf7ab5fcae6dd5147390e8",
          "sha256": "35297b98855676f04f910e4025e7888be9ef69bc3bfb2fad5dc9b34740053274"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "aecb2a18cabf7ab5fcae6dd5147390e8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 15860,
        "upload_time": "2016-04-02T19:53:42",
        "url": "https://files.pythonhosted.org/packages/13/61/6586dc1b41cdcf05f1f49ea3d92cc4e14341c7879dcce33fab693383da25/JapaneseTokenizer-0.8.tar.gz"
      }
    ],
    "0.9": [
      {
        "comment_text": "",
        "digests": {
          "md5": "965d1893a35b65ed9c81d09492834f3b",
          "sha256": "f7b4ea9753a8e8a5894ba0ccc1f0b9713aa78f62cc7dc49dbb4fa86ef06c14de"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "965d1893a35b65ed9c81d09492834f3b",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16284,
        "upload_time": "2016-04-04T17:23:05",
        "url": "https://files.pythonhosted.org/packages/d5/6a/e52ba521d54169b40acf7cb025b68e1e7119e28da6be9416c99054111bcd/JapaneseTokenizer-0.9.tar.gz"
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "2b10f181b00cc626eed106965163d8e3",
          "sha256": "71f5675bd8b6815fcc8242f48be9a4b6197294e2626683a7431380469cfc78f6"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2b10f181b00cc626eed106965163d8e3",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17118,
        "upload_time": "2016-08-03T04:28:53",
        "url": "https://files.pythonhosted.org/packages/54/b5/79590c298941926efd0b4fbe4906979bfcb16199dd371e46d2a88144b8c4/JapaneseTokenizer-1.0.tar.gz"
      }
    ],
    "1.0a0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "87fa79c3609690d6b86295c494af7d6f",
          "sha256": "fa3ec1860b5df9688d0d2c0037af19b568be2663e8b06e9814effb258cf82a4b"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0a0.tar.gz",
        "has_sig": false,
        "md5_digest": "87fa79c3609690d6b86295c494af7d6f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17105,
        "upload_time": "2016-06-19T08:18:31",
        "url": "https://files.pythonhosted.org/packages/82/47/8727b3a859e6c3cff4c3fdb2a5fc068d355bd346a22db38730e3ae9bd1ce/JapaneseTokenizer-1.0a0.tar.gz"
      }
    ],
    "1.0b0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
          "sha256": "fdb6b46290ae0907e62d16d32c762b6c7f4f433619c22c34519e32d6be253d21"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0b0.tar.gz",
        "has_sig": false,
        "md5_digest": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 17113,
        "upload_time": "2016-06-22T02:34:50",
        "url": "https://files.pythonhosted.org/packages/b6/b2/089232b3c76d3e802e0fd1c17eb7d96f9bbdc4034d8a55b3bfa18d241d64/JapaneseTokenizer-1.0b0.tar.gz"
      }
    ],
    "1.2.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "da97ce49089ebc76f08aa476a9c1cdce",
          "sha256": "7d9540ee440c9393bdf4232cc694d5bc8b6ba60b5bbde87edaf06381d046263d"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "da97ce49089ebc76f08aa476a9c1cdce",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 16609,
        "upload_time": "2016-12-08T18:25:46",
        "url": "https://files.pythonhosted.org/packages/87/2b/6ad05fb2afe606550828fba2c1da98629bf2f34a5523535d7214ea9c15c0/JapaneseTokenizer-1.2.3.tar.gz"
      }
    ],
    "1.2.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "30d57d0c20f73a976f3be343738f5f55",
          "sha256": "2ea333d144fd9f0ba44e6001d74d63855beb3a30a4c87924938130664231518f"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.5.tar.gz",
        "has_sig": false,
        "md5_digest": "30d57d0c20f73a976f3be343738f5f55",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 19943,
        "upload_time": "2016-12-28T07:03:26",
        "url": "https://files.pythonhosted.org/packages/2a/26/0c91f48b42d8059f969c2ab9a56b29b351eac15a139744f17fb98fa0accb/JapaneseTokenizer-1.2.5.tar.gz"
      }
    ],
    "1.2.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e6389d1aa28127631fb9595e19a6fd23",
          "sha256": "f2010906f997bc36db4fddb438abaf6d6380bca065beefd2859428fda0a5c0f4"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.6.tar.gz",
        "has_sig": false,
        "md5_digest": "e6389d1aa28127631fb9595e19a6fd23",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 20216,
        "upload_time": "2017-01-11T21:26:02",
        "url": "https://files.pythonhosted.org/packages/eb/9c/9197907e3cd0c13f03cd887efef234bc0fe359a05a844e7b82405ddab282/JapaneseTokenizer-1.2.6.tar.gz"
      }
    ],
    "1.2.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4caa2f2fd7e0a4e71d0d95b319e2f960",
          "sha256": "bddfc293a9c221f37b631c226568361c2599e74d49c4e0cb57ff859df725d4b8"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "4caa2f2fd7e0a4e71d0d95b319e2f960",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 20224,
        "upload_time": "2017-01-13T00:37:10",
        "url": "https://files.pythonhosted.org/packages/08/ed/e970d4d49554e7ddf5ed3edec1e5f1d8265c47e44c6de0b4e71191a15c7d/JapaneseTokenizer-1.2.7.tar.gz"
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "13b4072229b14619ae7b8cd672c47fbe",
          "sha256": "b51dd61c3a4192bb70d47310e67d42b29b8e6ec7702b556f4b718836fc772ccf"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "13b4072229b14619ae7b8cd672c47fbe",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 23199,
        "upload_time": "2017-02-23T03:41:02",
        "url": "https://files.pythonhosted.org/packages/79/19/c950a7b3ba817a9fc56e9d44d6698a401e876f54de404a760f8a1100a34a/JapaneseTokenizer-1.3.0.tar.gz"
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f70e1e0d82111714c1958084eef1c813",
          "sha256": "3608d6135eb00d6b59fccf675c894ff978f22359ed37a99fde8bb006b6d948f0"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f70e1e0d82111714c1958084eef1c813",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 28932,
        "upload_time": "2017-06-29T11:05:09",
        "url": "https://files.pythonhosted.org/packages/c0/06/e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5/JapaneseTokenizer-1.3.1.tar.gz"
      }
    ],
    "1.3.3": [
      {
        "comment_text": "",
        "digests": {
          "md5": "3c19ae9d41dc190c59be3664e3c9b659",
          "sha256": "f18bdbb1883a02d2cacfa42cf41b3d5198a804b986c06ec5f61b37c4d0ca0a82"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.3.tar.gz",
        "has_sig": false,
        "md5_digest": "3c19ae9d41dc190c59be3664e3c9b659",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 29010,
        "upload_time": "2017-09-11T09:30:36",
        "url": "https://files.pythonhosted.org/packages/b4/1c/9939737367a9fbd76d83fb3785676f1c25f441e73612e2b8ef7cd0f96ca4/JapaneseTokenizer-1.3.3.tar.gz"
      }
    ],
    "1.3.4": [
      {
        "comment_text": "",
        "digests": {
          "md5": "29140724b608a9a2b8232c7c6bfb35d8",
          "sha256": "d0f89a39f575af46b1ada42b0cd6980e56ecad7aa74397a13fa7c04df7ffa896"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.4.tar.gz",
        "has_sig": false,
        "md5_digest": "29140724b608a9a2b8232c7c6bfb35d8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 25226,
        "upload_time": "2017-09-21T08:10:52",
        "url": "https://files.pythonhosted.org/packages/ab/73/cfc8a35e964e78dbc8a72cd631b4f84d837279ae34501381a115d83f6c58/JapaneseTokenizer-1.3.4.tar.gz"
      }
    ],
    "1.3.5": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e24bafd9da63971195ce7f1590e7487c",
          "sha256": "5276308eae6a34221446069a254ef98e20f3a5b90f2e5ab8c682d45a269295b8"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.5.tar.gz",
        "has_sig": false,
        "md5_digest": "e24bafd9da63971195ce7f1590e7487c",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 25785,
        "upload_time": "2017-09-27T03:30:33",
        "url": "https://files.pythonhosted.org/packages/0c/b7/c54719202bbd67b7a44ba1d53f58d5557261d4a523fe1bfe1aeceab9ca65/JapaneseTokenizer-1.3.5.tar.gz"
      }
    ],
    "1.3.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "4528fc7cca5ee812f63f6183435eb2c5",
          "sha256": "55cd3f4b0979609bddadd88cedd31afdc6cec39f5b2c4225b814b58944bf9a92"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.6.tar.gz",
        "has_sig": false,
        "md5_digest": "4528fc7cca5ee812f63f6183435eb2c5",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 26011,
        "upload_time": "2017-11-01T11:05:19",
        "url": "https://files.pythonhosted.org/packages/1f/08/0b839490cd42fce5b727bbf43500d2bd268584c0519720cd1b03738899d0/JapaneseTokenizer-1.3.6.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "4528fc7cca5ee812f63f6183435eb2c5",
        "sha256": "55cd3f4b0979609bddadd88cedd31afdc6cec39f5b2c4225b814b58944bf9a92"
      },
      "downloads": -1,
      "filename": "JapaneseTokenizer-1.3.6.tar.gz",
      "has_sig": false,
      "md5_digest": "4528fc7cca5ee812f63f6183435eb2c5",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 26011,
      "upload_time": "2017-11-01T11:05:19",
      "url": "https://files.pythonhosted.org/packages/1f/08/0b839490cd42fce5b727bbf43500d2bd268584c0519720cd1b03738899d0/JapaneseTokenizer-1.3.6.tar.gz"
    }
  ]
}