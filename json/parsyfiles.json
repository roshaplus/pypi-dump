{
  "info": {
    "author": "Sylvain Mari\u00e9",
    "author_email": "sylvain.marie@schneider-electric.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 3.5",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "python simple file collection parsing framework (parsyfiles)\n============================================================\n\nA declarative framework that combines many popular python parsers (json,\njprops, yaml, pickle, pandas...) with your user-defined parsers and type\nconverters, in order to easily read objects from files. It is able to\nread an object even if the object's content comes from several files\nrequiring several parsers. This is typically useful to read test data\nwhere you want to combine datasets, parameters, expected results and\nwhat not.\n\nThis library provides a *framework*, not a specific parser for a\nspecific file format. By default several classic parsers from the python\nworld are already registered, but it is also extremely easy to add more\nif your favourite parser is missing. Even better: if you just need to\nparse a derived type of a type that can already be parsed, you may\nsimply need to register a *type converter*, the framework will link it\nto any compliant parser for you.\n\nContents\n--------\n\n-  `Overview <#overview>`__\n\n   -  `1- Intended audience <#1--intended-audience>`__\n   -  `2- Typical use cases <#2--typical-use-cases>`__\n   -  `3- Main features <#3--main-features>`__\n\n-  `Installation <#installation>`__\n\n   -  `1- Recommended : create a clean virtual\n      environment <#1--recommended--create-a-clean-virtual-environment>`__\n   -  `2- Installation steps <#2--installation-steps>`__\n   -  `3- Uninstalling <#3--uninstalling>`__\n\n-  `Usage <#usage>`__\n\n   -  `1- Collections of known types <#1--collections-of-known-types>`__\n   -  `(a) Example: parsing a list of\n      DataFrame <#a-example-parsing-a-list-of-dataframe>`__\n   -  `(b) Understanding the log\n      output <#b-understanding-the-log-output>`__\n   -  `(c) Parsing a single file only <#c-parsing-a-single-file-only>`__\n   -  `(d) Default collection type and other supported\n      types <#d-default-collection-type-and-other-supported-types>`__\n   -  `2- Simple user-defined types <#2--simple-user-defined-types>`__\n   -  `(a) Example: parsing a collection of test\n      cases <#a-example-parsing-a-collection-of-test-cases>`__\n   -  `(b) Under the hood : why does it work, even on ambiguous\n      files? <#b-under-the-hood--why-does-it-work-even-on-ambiguous-files>`__\n\n      -  `Solved Difficulty 1 - Several formats/parsers for the same\n         file\n         extension <#solved-difficulty-1---several-formatsparsers-for-the-same-file-extension>`__\n      -  `Solved Difficulty 2 - Generic\n         parsers <#solved-difficulty-2---generic-parsers>`__\n      -  `Understanding the inference logic - in which order are the\n         parsers tried\n         ? <#understanding-the-inference-logic---in-which-order-are-the-parsers-tried->`__\n\n   -  `3- Multifile objects: combining several\n      parsers <#3--multifile-objects-combining-several-parsers>`__\n   -  `4- Advanced topics <#4--advanced-topics>`__\n   -  `(a) Lazy parsing <#a-lazy-parsing>`__\n   -  `(b) Passing options to existing\n      parsers <#b-passing-options-to-existing-parsers>`__\n   -  `(c) Parsing subclasses of existing types - registering\n      converters <#c-parsing-subclasses-of-existing-types---registering-converters>`__\n   -  `(d) Registering a new parser <#d-registering-a-new-parser>`__\n   -  `(e) Contract validation for parsed objects : combo with\n      classtools-autocode and\n      attrs <#e-contract-validation-for-parsed-objects--combo-with-classtools-autocode-and-attrs>`__\n\n      -  `classtools-autocode example <#classtools-autocode-example>`__\n      -  `attrs example <#attrs-example>`__\n\n   -  `(e) File mappings: Wrapped/Flat and\n      encoding <#e-file-mappings-wrappedflat-and-encoding>`__\n   -  `(f) Recursivity: Multifile children of Multifile\n      objects <#f-recursivity-multifile-children-of-multifile-objects>`__\n\n      -  `Example recursivity in flat\n         mode <#example-recursivity-in-flat-mode>`__\n      -  `Example recursivity in wrapped\n         mode <#example-recursivity-in-wrapped-mode>`__\n\n   -  `(g) Diversity of formats supported: DataFrames -\n      revisited <#g-diversity-of-formats-supported-dataframes---revisited>`__\n\n-  `See Also <#see-also>`__\n-  `Developers <#developers>`__\n\n   -  `Packaging <#packaging>`__\n   -  `Releasing memo <#releasing-memo>`__\n\n*ToC created by\n`gh-md-toc <https://github.com/ekalinin/github-markdown-toc>`__*\n\nOverview\n--------\n\n1- Intended audience\n~~~~~~~~~~~~~~~~~~~~\n\n-  developers looking for an easy way to parse dictionaries and simple\n   objects from various formats (json, properties, cfg, csv...) using\n   standard python libraries. They can combine this library with\n   `classtools\\_autocode <https://github.com/smarie/python-classtools-autocode>`__\n   or `attrs <https://attrs.readthedocs.io/en/stable/>`__ to preserve\n   compact, readable and content-validated classes.\n\n-  developers who already know how to parse their various files\n   independently, but looking for a higher-level tool to read complex\n   objects made of several files/folders and potentially requiring to\n   combine several parsers.\n\n2- Typical use cases\n~~~~~~~~~~~~~~~~~~~~\n\n-  **read collections of test cases** on the file system - each test\n   case being composed of several files (for example 2 'test inputs'\n   .csv files, 1 'test configuration' .cfg file, and one 'reference test\n   results' json or yaml file)\n-  more generally, **read complex objects that for some reason are not\n   represented by a single file representation**, for example objects\n   made of several csv files (timeseries + descriptive data),\n   combinations of csv and xml/json files, configuration files, pickle\n   files, etc.\n\n*Note on the Intelligence/Speed tradeoff:* This framework contains a bit\nof nontrivial logic in order to transparently infer which parser and\nconversion chain to use, and even in some cases to try several\nalternatives in order to guess what was your intent. This makes it quite\npowerful but will certainly be slower and more memory-consuming than\nwriting a dedicated parser tailored for your specific case. However if\nyou are looking for a tool to speedup your development so that you may\nfocus on what's important (your business logic, algorithm\nimplementation, test logic, etc) then have a try, it might do the job.\n\n3- Main features\n~~~~~~~~~~~~~~~~\n\n-  **Declarative (class-based)**: you *first* define the objects to\n   parse - by creating or importing their class -, *then* you use\n   ``parse_item`` or ``parse_collection`` on the appropriate folder or\n   file path.\n-  **Simple objects out-of-the-box**: if you're interested in parsing\n   singlefile objects only requiring simple types in their constructor,\n   then the framework is *already* able to parse them, for many\n   singlefile formats (json, properties, txt, csv, yaml and more.).\n-  **Multifile collections out-of-the-box**: the framework is able to\n   parse collections of objects, each represented by a file. Parsing may\n   optionally be done in a lazy fashion (each item is only read if\n   needed).\n-  **Serialization**: pickle files (.pyc) are supported too.\n   Base64-encoded pickle objects can also be included in any simple file\n   content.\n-  **Multiparser**: the library will use the best parser adapted to each\n   file format and desired type. At the time of writing the library:\n\n   -  knows **44** ways to parse a file\n   -  is able to parse **15** object types (including *'AnyObject'* for\n      generic parsers)\n   -  from **13** file extensions\n\n-  **Multifile+Multiparser objects**: You may therefore parse complex\n   objects requiring a combination of several parsers to be built from\n   several files. The framework will introspect the object constructor\n   in order to know the list of required attributes to parse as well as\n   their their type, and if they are mandatory or optional.\n-  **Recursive**: attributes may themselves be collections or complex\n   types.\n-  Supports **two main file mapping flavours for Multifile objects**:\n   the library comes with two ways to organize multifile objects such as\n   collections: *wrapped* (each multifile object is a folder), or *flat*\n   (all files are in the same folder, files belonging to the same\n   multifile object have the same prefix)\n\nIn addition the library is\n\n-  **Extensible**. You may register any number of additional file\n   parsers, or type converters, or both. When registering a parser you\n   just have to declare the object types that it can parse, *and* the\n   file extensions it can read. The same goes for converters: you\n   declare the object type it can read, and the object type it can\n   convert to.\n-  **Intelligent** Since several parsers may be registered for the same\n   file extension, and more generally several parsing chains (parser +\n   converters) may be eligible to a given task, the library has a\n   built-in set of rules to select the relevant parsing chains and test\n   them in most plausible order. This provides you with several ways to\n   parse the same object. This might be useful for example if some of\n   your data comes from nominal tests, some other from field tests, some\n   other from web service calls, etc. You don't need anymore to convert\n   all of these to the same format before using it.\n-  **No annotations required**: as opposed to some data binding\n   frameworks, this library is meant to parse object types that may\n   already exist, and potentially only for tests. Therefore the\n   framework does not require annotations on the type if there is there\n   is a registered way to parse it. However if you wish to build\n   higher-level objects encapsulating the result of several parsers,\n   then PEP484 type hints are required. But that's probably less a\n   problem since these objects are yours (they are part of your tests\n   for example)\n\nInstallation\n------------\n\n1- Recommended : create a clean virtual environment\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe strongly recommend that you use conda *environment* or pip\n*virtualenv*/*venv* in order to better manage packages. Once you are in\nyour virtual environment, open a terminal and check that the python\ninterpreter is correct:\n\n.. code:: bash\n\n    (Windows)>  where python\n    (Linux)  >  which python\n\nThe first executable that should show up should be the one from the\nvirtual environment.\n\n2- Installation steps\n~~~~~~~~~~~~~~~~~~~~~\n\nThis package is available on ``PyPI``. You may therefore use ``pip`` to\ninstall from a release\n\n.. code:: bash\n\n    > pip install parsyfiles\n\n3- Uninstalling\n~~~~~~~~~~~~~~~\n\nAs usual :\n\n.. code:: bash\n\n    > pip uninstall parsyfiles\n\nUsage\n-----\n\n1- Collections of known types\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n(a) Example: parsing a list of DataFrame\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe most simple case of all: you wish to parse a collection of files\nthat all have the same type, and for which a parser is already\nregistered. For example your wish to parse a list of ``DataFrame`` for a\ndata folder that looks like this:\n\n.. code:: bash\n\n    ./demo/simple_collection\n    \u251c\u2500\u2500 a.csv\n    \u251c\u2500\u2500 b.txt\n    \u251c\u2500\u2500 c.xls\n    \u251c\u2500\u2500 d.xlsx\n    \u2514\u2500\u2500 e.xlsm\n\n*Note: you may find this example data folder in the `project\nsources <https://github.com/smarie/python-simple-file-collection-parsing-framework/tree/master/parsyfiles/test_data>`__*\n\nParsing all of these dataframes is straightforward:\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_collection\n    from pandas import DataFrame\n\n    dfs = parse_collection('./demo/simple_collection', DataFrame)\n    pprint(dfs)\n\nHere is the result\n\n::\n\n    **** Starting to parse  collection of <DataFrame> at location ./demo/simple_collection ****\n    Checking all files under ./demo/simple_collection\n    ./demo/simple_collection (multifile)\n    ./demo/simple_collection\\a (singlefile, .csv)\n    (...)\n    ./demo/simple_collection\\e (singlefile, .xlsm)\n    File checks done\n\n    Building a parsing plan to parse ./demo/simple_collection (multifile) into a Dict[str, DataFrame]\n    ./demo/simple_collection (multifile) > Dict[str, DataFrame] ------- using Multifile Dict parser (based on 'parsyfiles defaults' to find the parser for each item)\n    ./demo/simple_collection\\a (singlefile, .csv) > DataFrame ------- using <read_df_or_series_from_csv(stream mode)>\n    (...)\n    ./demo/simple_collection\\e (singlefile, .xlsm) > DataFrame ------- using <read_dataframe_from_xls(file mode)>\n    Parsing Plan created successfully\n\n    Executing Parsing Plan for ./demo/simple_collection (multifile) > Dict[str, DataFrame] ------- using Multifile Dict parser (based on 'parsyfiles defaults' to find the parser for each item)\n    Parsing ./demo/simple_collection (multifile) > Dict[str, DataFrame] ------- using Multifile Dict parser (based on 'parsyfiles defaults' to find the parser for each item)\n    Parsing ./demo/simple_collection\\a (singlefile, .csv) > DataFrame ------- using <read_df_or_series_from_csv(stream mode)>\n    --> Successfully parsed a DataFrame from ./demo/simple_collection\\a\n    (...)\n    Parsing ./demo/simple_collection\\e (singlefile, .xlsm) > DataFrame ------- using <read_dataframe_from_xls(file mode)>\n    --> Successfully parsed a DataFrame from ./demo/simple_collection\\e\n    Assembling all parsed child items into a Dict[str, DataFrame] to build ./demo/simple_collection (multifile)\n    --> Successfully parsed a Dict[str, DataFrame] from ./demo/simple_collection\n    Completed parsing successfully\n\n    {'a':    a  b  c  d\n          0  1  2  3  4,\n     'b':    a  b  c  d\n          0  1  2  3  4,\n     'c':    c   5\n          0  d   8\n          1  e  12\n          2  f   3,\n     'd':    c   5\n          0  d   8\n          1  e  12\n          2  f   3,\n     'e':    c   5\n          0  d   8\n          1  e  12\n          2  f   3}\n\n*Note: the above capture was slightly 'improved' for readability,\nbecause unfortunately pprint does not display dictionaries of dataframes\nas nicely as this.*\n\n(b) Understanding the log output\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default the library uses a ``Logger`` that has an additional handler\nto print to ``stdout``. If you do not want to see all these messages\nprinted to the console, or if you want to use a different logging\nconfiguration, you may provide a custom logger to the function:\n\n.. code:: python\n\n    from logging import getLogger\n    dfs = parse_collection('./demo/simple_collection', DataFrame, logger=getLogger('my_logger'))\n\nIn the log output you see a couple hints on how the parsing framework\nworks:\n\n-  first it recursively **checks your folder** to check that it is\n   entirely compliant with the file mapping format. That is the log\n   section beginning with\n   \"``Checking all files under ./demo/simple_collection``\". If the same\n   item appears twice (e.g. ``a.csv`` and ``a.txt``) it will throw an\n   error at this stage (an\n   ``ObjectPresentMultipleTimesOnFileSystemError``).\n\n-  then it recursively **creates a parsing plan** that is able to\n   produce an object the required type. That's the section beginning\n   with\n   \"``Building a parsing plan to parse ./demo/simple_collection (multifile) into a Dict[str, DataFrame]``\".\n   Here you may note that by default, a collection of items is actually\n   parsed as an object of type dictionary, where the key is the name of\n   the file without extension, and the value is the object that is\n   parsed from the file. If at this stage it does not find a way to\n   parse a given file into the required object type, it will fail. For\n   example if you add a file in the folder, named\n   ``unknown_ext_for_dataframe.ukn``, you will get an error (a\n   ``NoParserFoundForObjectExt``).\n\n-  finally it **executes the parsing plan**. That's the section\n   beginning with\n   \"``Executing Parsing Plan for ./demo/simple_collection (multifile) > Dict[str, DataFrame] (...)``\".\n\nIt is important to understand these 3 log sections, since the main issue\nwith complex frameworks is debugging when something unexpected happens\n:-).\n\n(c) Parsing a single file only\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe following code may be used to parse a single file explicitly:\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_item\n    from pandas import DataFrame\n\n    df = parse_item('./demo/simple_collection/c', DataFrame)\n    pprint(df)\n\nImportant : note that the file extension does not appear in the argument\nof the ``parse_item`` function.\n\n(d) Default collection type and other supported types\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nYou might have noticed that the demonstrated collection example returned\na ``dict`` of dataframes, not a ``list``. This is the default behaviour\nof the ``parse_collection`` method - it has the advantage of not making\nany assumption on the sorting order.\n\nBehind the scenes, ``parse_collection`` redirects to the ``parse_item``\ncommand. So the following code leads to the exact same results:\n\n.. code:: python\n\n    from parsyfiles import parse_item\n    from pandas import DataFrame\n    from typing import Dict\n\n    df = parse_item('./demo/simple_collection/c', Dict[str, DataFrame])\n\nThe ``typing`` module is used here to entirely specify the type of item\nthat you want to parse (``Dict[str, DataFrame]``). The parsed item will\nbe a dictionary with string keys (the file names) and DataFrame values\n(the parsed file contents).\n\nYou may parse a ``list``, a ``set``, or a ``tuple`` exactly the same\nway, using the corresponding ``typing`` class:\n\n.. code:: python\n\n    from parsyfiles import parse_item\n    from pandas import DataFrame\n    from typing import List, Set, Tuple\n\n    dfl = parse_item('./demo/simple_collection', List[DataFrame])\n    # dfs = parse_item('./demo/simple_collection', Set[DataFrame])\n    dft = parse_item('./demo/simple_collection', Tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame])\n\nFor ``List`` and ``Tuple`` the implied order is alphabetical on the file\nnames (similar to using ``sorted()`` on the items of the dictionary).\nNote that ``DataFrame`` objects are not mutable, so in this particular\ncase the collection cannot be parsed as a ``Set``.\n\nFinally, note that it is not possible to mix collection and\nnon-collection items together (for example, ``Union[int, List[int]]`` is\nnot supported).\n\n2- Simple user-defined types\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n(a) Example: parsing a collection of test cases\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSuppose that you want to test the following ``exec_op`` function, and\nyou want to read your test datasets from a bunch of files.\n\n.. code:: python\n\n    def exec_op(x: float, y: float, op: str) -> float:\n        if op is '+':\n            return x+y\n        elif op is '-':\n            return x-y\n        else:\n            raise ValueError('Unsupported operation : \\'' + op + '\\'')\n\nEach test dataset could be represented as an object, containing the\ninputs and expected outputs for ``exec_op``. For example:\n\n.. code:: python\n\n    class ExecOpTest(object):\n\n        def __init__(self, x: float, y: float, op: str, expected_result: float):\n            self.x = x\n            self.y = y\n            self.op = op\n            self.expected_result = expected_result\n\n        def __str__(self):\n            return self.__repr__()\n\n        def __repr__(self):\n            return str(self.x) + ' ' + self.op + ' ' + str(self.y) + ' =? ' + str(self.expected_result)\n\nObviously this class is not known by the ``parsyfiles`` framework: there\nis no registered parser for the ``ExecOpTest`` type. However the type is\nfairly simple, so it can actually fit into a dictionary containing the\nvalues for ``x``, ``y``, ``op``, and ``expected_results``.\n``parsyfiles`` knows a couple ways to parse dictionaries, using python\nstandard libraries:\n\n-  From a ``.cfg`` or ``.ini`` file using the ``configparser`` module\n-  From a ``.json`` file using the ``json`` module\n-  From a ``.properties`` or ``.txt`` file using the ``jprops`` module\n-  From a ``.yaml`` or ``.yml`` file using the ``yaml`` module\n-  From a ``.csv``, ``.txt``, ``.xls``, ``.xlsx``, ``.xlsm`` file using\n   the ``pandas`` module\n-  etc.\n\nIt also knows how to convert a dictionary into an object, as long as the\nobject constructor contains the right information about expected types.\nFor example in the example above, the constructor has explicit PEP484\nannotations ``x: float, y: float, op: str, expected_result: float``.\n\nSo let's try to parse instances of ``ExecOpTest`` from various files.\nOur test data folder looks like this (available in the `project\nsources <https://github.com/smarie/python-simple-file-collection-parsing-framework/tree/master/parsyfiles/test_data>`__):\n\n.. code:: bash\n\n    ./demo/simple_objects\n    \u251c\u2500\u2500 test_diff_1.cfg\n    \u251c\u2500\u2500 test_diff_2.ini\n    \u251c\u2500\u2500 test_diff_3_csv_format.txt\n    \u251c\u2500\u2500 test_sum_1.json\n    \u251c\u2500\u2500 test_sum_2.properties\n    \u251c\u2500\u2500 test_sum_3_properties_format.txt\n    \u251c\u2500\u2500 test_sum_4.yaml\n    \u251c\u2500\u2500 test_sum_5.xls\n    \u251c\u2500\u2500 test_sum_6.xlsx\n    \u2514\u2500\u2500 test_sum_7.xlsm\n\nAs usual, we tell the framework that we want to parse a collection of\nobjects of type ``ExecOpTest``:\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_collection\n\n    sf_tests = parse_collection('./demo/simple_objects', ExecOpTest)\n    pprint(sf_tests)\n\nHere is the result:\n\n::\n\n    **** Starting to parse  collection of <ExecOpTest> at location ./demo/simple_objects ****\n    Checking all files under ./demo/simple_objects\n    (...)\n    File checks done\n\n    Building a parsing plan to parse ./demo/simple_objects (multifile) into a Dict[str, ExecOpTest]\n    (...)\n    Parsing Plan created successfully\n\n    Executing Parsing Plan for ./demo/simple_objects (multifile) > Dict[str, ExecOpTest] ------- using Multifile Dict parser (based on 'parsyfiles defaults' to find the parser for each item)\n    (...)\n    --> Successfully parsed a Dict[str, ExecOpTest] from ./demo/simple_objects\n    Completed parsing successfully\n\n    {'test_diff_1': 1.0 - 1.0 =? 0.0,\n     'test_diff_2': 0.0 - 1.0 =? -1.0,\n     'test_diff_3_csv_format': 5.0 - 4.0 =? 1.0,\n     'test_diff_4_csv_format2': 4.0 - 4.0 =? 0.0,\n     'test_sum_1': 1.0 + 2.0 =? 3.0,\n     'test_sum_2': 0.0 + 1.0 =? 1.0,\n     'test_sum_3_properties_format': 1.0 + 1.0 =? 2.0,\n     'test_sum_4': 2.0 + 5.0 =? 7.0,\n     'test_sum_5': 56.0 + 12.0 =? 68.0,\n     'test_sum_6': 56.0 + 13.0 =? 69.0,\n     'test_sum_7': 56.0 + 14.0 =? 70.0}\n\n(b) Under the hood : why does it work, even on ambiguous files?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn the example above, three files were actually quite difficult to parse\ninto a ``dict`` before being converted to an ``ExecOpTest``:\n``test_diff_3_csv_format.txt``, ``test_diff_4_csv_format2.txt`` and\n``test_sum_4.yaml``. Let's look at both cases in details.\n\nSolved Difficulty 1 - Several formats/parsers for the same file extension\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\n``test_diff_3_csv_format.txt`` and ``test_diff_4_csv_format2.txt`` are\nboth .txt file that contains csv-format data. But\n\n-  there are several way to write a dictionary in a csv format (one row\n   of header + one row of values, or one column of names + one column of\n   values).\n-  .txt files may also contain many other formats such as for example,\n   the 'properties' format.\n\nHow does the framework manage to parse these files ? Lets look at the\nlog output for ``test_diff_3_csv_format.txt``:\n\n::\n\n    Parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_dict_from_properties> => <dict_to_object>$\n      !! Caught error during execution !!\n      File \"C:\\W_dev\\_pycharm_workspace\\python-parsyfiles\\parsyfiles\\support_for_objects.py\", line 273, in dict_to_object\n        attr_name)\n      ParsingException : Error while parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) as a ExecOpTest with parser '$<read_dict_from_properties> => <dict_to_object>$' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : caught \n      InvalidAttributeNameForConstructorError : Cannot parse object of type <ExecOpTest> using the provided configuration file: configuration contains a property name ('5,4,-,1')that is not an attribute of the object constructor. <ExecOpTest> constructor attributes are : ['y', 'x', 'expected_result', 'op']\n\n    Rebuilding local parsing plan with next candidate parser: $<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$\n    ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$\n    Parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$\n      !! Caught error during execution !!\n      File \"C:\\Anaconda3\\envs\\azuremlbricks\\lib\\base64.py\", line 88, in b64decode\n        return binascii.a2b_base64(s)\n      ParsingException : Error while parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) as a ExecOpTest with parser '$<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : caught \n      Error : Incorrect padding\n\n    Rebuilding local parsing plan with next candidate parser: $<read_str_from_txt> => <constructor_with_str_arg>$\n    ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_str_from_txt> => <constructor_with_str_arg>$\n    Parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_str_from_txt> => <constructor_with_str_arg>$\n      !! Caught error during execution !!\n      File \"C:\\W_dev\\_pycharm_workspace\\python-parsyfiles\\parsyfiles\\support_for_primitive_types.py\", line 98, in constructor_with_str_arg\n        return desired_type(source)\n      ParsingException : Error while parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) as a ExecOpTest with parser '$<read_str_from_txt> => <constructor_with_str_arg>$' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : caught \n      CaughtTypeError : Caught TypeError while calling conversion function 'constructor_with_str_arg'. Note that the conversion function signature should be 'def my_convert_fun(desired_type: Type[T], source: S, logger: Logger, **kwargs) -> T' (unpacked options mode - default) or def my_convert_fun(desired_type: Type[T], source: S, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T (unpack_options = False).Caught error message is : TypeError : __init__() missing 3 required positional arguments: 'y', 'op', and 'expected_result'\n\n    Rebuilding local parsing plan with next candidate parser: $<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$\n    ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$\n    Parsing ./demo/simple_objects\\test_diff_3_csv_format (singlefile, .txt) > ExecOpTest ------- using $<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$\n    --> Successfully parsed a ExecOpTest from ./demo/simple_objects\\test_diff_3_csv_format\n\nYou can see from the logs that the framework successively tries several\nways to parse this file :\n\n-  ``$<read_dict_from_properties> => <dict_to_object>$``: the txt file\n   is read in the 'properties' format (using ``jprops``) into a\n   dictionary, and then the dictionary is converted to a ``ExecOpTest``\n   object. *This fails.*\n-  ``$<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$`` :\n   the txt file is read as a string, and then the string is interpreted\n   as a base64-encoded pickle ``ExecOpTest`` object (!). *This fails.*\n-  ``$<read_str_from_txt> => <constructor_with_str_arg>$``: the txt file\n   is read as a string, and then the constructor of ``ExecOpTest`` is\n   called with that string as unique argument. *This fails again.*\n-  ``$<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$``:\n   the txt file is read as a csv into a DataFrame, then the DataFrame is\n   converted to a dictionary, and finally the dictionary is converted\n   into a ``ExecOpTest`` object. *This finally succeeds*.\n\nThe same goes for the other file ``test_diff_4_csv_format2.txt``.\n\nSolved Difficulty 2 - Generic parsers\n'''''''''''''''''''''''''''''''''''''\n\nFor ``test_sum_4.yaml``, the difficulty is that yaml format may contain\na dictionary directly, but is also able to contain any typed object\nthanks to the YAML ``object`` directive. Therefore it could contain a\n``ExecOpTest``.\n\nThe parsing logs are the following:\n\n::\n\n    Parsing ./demo/simple_objects\\test_sum_4 (singlefile, .yaml) > ExecOpTest ------- using <read_object_from_yaml>\n      !! Caught error during execution !!\n      File \"C:\\W_dev\\_pycharm_workspace\\python-parsyfiles\\parsyfiles\\parsing_core_api.py\", line 403, in execute\n        res, options)\n      ParsingException : Error while parsing ./demo/simple_objects\\test_sum_4 (singlefile, .yaml) as a <class 'test_parsyfiles.DemoTests.test_simple_objects.<locals>.ExecOpTest'> with parser '<read_object_from_yaml>' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : \n          parser returned {'y': 5, 'x': 2, 'op': '+', 'expected_result': 7} of type <class 'dict'> which is not an instance of <class 'test_parsyfiles.DemoTests.test_simple_objects.<locals>.ExecOpTest'>\n\n    Rebuilding local parsing plan with next candidate parser: $<read_collection_from_yaml> => <dict_to_object>$\n    ./demo/simple_objects\\test_sum_4 (singlefile, .yaml) > ExecOpTest ------- using $<read_collection_from_yaml> => <dict_to_object>$\n    Parsing ./demo/simple_objects\\test_sum_4 (singlefile, .yaml) > ExecOpTest ------- using $<read_collection_from_yaml> => <dict_to_object>$\n    --> Successfully parsed a ExecOpTest from ./demo/simple_objects\\test_sum_4\n\nYou can see from the logs that the framework successively tries several\nways to parse this file :\n\n-  ``<read_object_from_yaml>``: the file is read according to the yaml\n   format, as an ``ExecOpTest`` object directly. This fails.\n\n-  ``$<read_collection_from_yaml> => <dict_to_object>$``: the file is\n   read according to the yaml format, as a dictionary. Then this\n   dictionary is converted into a ``ExecOpTest`` object. **This\n   succeeds**\n\nUnderstanding the inference logic - in which order are the parsers tried ?\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nThese example show how ``parsyfiles`` intelligently combines all\nregistered parsers and converters to create parsing chains that make\nsense. These parsing chains are tried **in order** until a solution is\nfound. Note that the order is deterministic:\n\n-  First all **exact match** parsers. This includes combinations of\n   {parser + converter chain} that lead to an exact match, sorted by\n   converter chain size: first the small conversion chains, last the\n   large conversion chains.\n\n-  Then all **approximative match** parsers. This is similar to the\n   \"exact match\" except that these are parsers able to parse a\n   **subclass** of what you're asking for.\n\n-  Finally all **generic** parsers. This includes combinations of\n   {parser + converter chain} that end with a generic converter (for\n   example the \"dict to object\" converter seen in the example above)\n\nIn order to know in advance which file extensions and formats the\nframework will be able to parse, you may wish to use the following\ncommand to ask the framework:\n\n.. code:: python\n\n    from parsyfiles import RootParser\n    RootParser().print_capabilities_for_type(typ=ExecOpTest)\n\nThe result is a dictionary where each entry is a file extension:\n\n::\n\n    {'.cfg': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [$<read_config> => <merge_all_config_sections_into_a_single_dict> -> <dict_to_object>$,\n                            $<read_config> => <config_to_dict_of_dict> -> <dict_of_dict_to_object>$,\n                            $<read_config> => <config_to_dict_of_dict> -> <dict_to_object>$]},\n     '.csv': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [$<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$]},\n     '.ini': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [$<read_config> => <merge_all_config_sections_into_a_single_dict> -> <dict_to_object>$,\n                            $<read_config> => <config_to_dict_of_dict> -> <dict_of_dict_to_object>$,\n                            $<read_config> => <config_to_dict_of_dict> -> <dict_to_object>$]},\n     '.json': {'1_exact_match': [],\n               '2_approx_match': [],\n               '3_generic': [$<read_dict_or_list_from_json> => <dict_to_object>$]},\n     '.properties': {'1_exact_match': [],\n                     '2_approx_match': [],\n                     '3_generic': [$<read_dict_from_properties> => <dict_to_object>$]},\n     '.pyc': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [<read_object_from_pickle>]},\n     '.txt': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [$<read_dict_from_properties> => <dict_to_object>$,\n                            $<read_str_from_txt> => <base64_ascii_str_pickle_to_object>$,\n                            $<read_str_from_txt> => <constructor_with_str_arg>$,\n                            $<read_df_or_series_from_csv> => <single_row_or_col_df_to_dict> -> <dict_to_object>$]},\n     '.xls': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [$<read_dataframe_from_xls> => <single_row_or_col_df_to_dict> -> <dict_to_object>$]},\n     '.xlsm': {'1_exact_match': [],\n               '2_approx_match': [],\n               '3_generic': [$<read_dataframe_from_xls> => <single_row_or_col_df_to_dict> -> <dict_to_object>$]},\n     '.xlsx': {'1_exact_match': [],\n               '2_approx_match': [],\n               '3_generic': [$<read_dataframe_from_xls> => <single_row_or_col_df_to_dict> -> <dict_to_object>$]},\n     '.yaml': {'1_exact_match': [],\n               '2_approx_match': [],\n               '3_generic': [<read_object_from_yaml>,\n                             $<read_collection_from_yaml> => <dict_to_object>$]},\n     '.yml': {'1_exact_match': [],\n              '2_approx_match': [],\n              '3_generic': [<read_object_from_yaml>,\n                            $<read_collection_from_yaml> => <dict_to_object>$]},\n     '<multifile>': {'1_exact_match': [],\n                     '2_approx_match': [],\n                     '3_generic': [Multifile Object parser (parsyfiles defaults)]}}\n\nLooking at the entries for ``.txt`` and ``.yaml``, we can find back the\nordered list of parsers that were automatically tried in the above\nexamples.\n\n3- Multifile objects: combining several parsers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis **'the'** typical use case for this library. Suppose that you want\nto test the following ``exec_op_series`` function, that uses complex\ntypes ``Series`` and ``AlgoConf`` as inputs and ``AlgoResults`` as\noutput:\n\n.. code:: python\n\n    class AlgoConf(object):\n        def __init__(self, foo_param: str, bar_param: int):\n            self.foo_param = foo_param\n            self.bar_param = bar_param\n\n    class AlgoResults(object):\n        def __init__(self, score: float, perf: float):\n            self.score = score\n            self.perf = perf\n\n    from pandas import Series\n    def exec_op_series(x: Series, y: AlgoConf) -> AlgoResults:\n        # ... intelligent stuff here...\n        pass\n\nSimilar to what we've done in previous chapter, each test dataset can be\nrepresented as an object, containing the inputs and expected outputs.\nFor example with this class:\n\n.. code:: python\n\n    class ExecOpSeriesTest(object):\n\n        def __init__(self, x: Series, y: AlgoConf, expected_results: AlgoResults):\n            self.x = x\n            self.y = y\n            self.expected_results = expected_results\n\nOur test data folder look like this :\n\n::\n\n    ./demo/complex_objects\n    \u251c\u2500\u2500 case1\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_results.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 x.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 y.txt\n    \u2514\u2500\u2500 case2\n        \u251c\u2500\u2500 expected_results.txt\n        \u251c\u2500\u2500 x.csv\n        \u2514\u2500\u2500 y.txt\n\nYou may notice that in this case, we chose to represent each instance of\n``ExecOpSeriesTest`` as a folder. This makes them 'multifile'. The\ndefault multifile object parser in the framework will try to parse each\nattribute of the constructor as an independent file in the folder.\n\nThe code for parsing remains the same - we tell the framework that we\nwant to parse a collection of objects of type ``ExecOpSeriesTest``. The\nrest is handled automatically by the framework:\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_collection\n\n    mf_tests = parse_collection('./demo/complex_objects', ExecOpSeriesTest)\n    pprint(mf_tests)\n\nHere are the results :\n\n::\n\n    **** Starting to parse  collection of <ExecOpSeriesTest> at location ./demo/complex_objects ****\n    Checking all files under ./demo/complex_objects\n    ./demo/complex_objects (multifile)\n    (...)\n    File checks done\n\n    Building a parsing plan to parse ./demo/complex_objects (multifile) into a Dict[str, ExecOpSeriesTest]\n    (...)\n    Parsing Plan created successfully\n\n    Executing Parsing Plan for ./demo/complex_objects (multifile) > Dict[str, ExecOpSeriesTest] ------- using Multifile Collection parser (parsyfiles defaults)\n    (...)\n    --> Successfully parsed a Dict[str, ExecOpSeriesTest] from ./demo/complex_objects\n    Completed parsing successfully\n\n    {'case1': <ExecOpSeriesTest object at 0x00000000087DDF98>,\n     'case2': <ExecOpSeriesTest object at 0x000000000737FBE0>}\n\nNote that multifile objects and singlefile objects may coexist in the\nsame folder, and that parsing is recursive - meaning that multifile\nobjects or collections may contain multifile children as well.\n\n4- Advanced topics\n~~~~~~~~~~~~~~~~~~\n\nThe ``parse_collection`` and ``parse_item`` that we have used in most\nexamples are actually just helper methods to build a parser registry\n(``RootParser()``) and use it. Most of the advanced topics below use\nthis object directly.\n\n(a) Lazy parsing\n^^^^^^^^^^^^^^^^\n\nThe multifile collection parser included in the library provides an\noption to return a lazy collection instead of a standard ``set``,\n``list``, ``dict`` or ``tuple``. This collection will trigger parsing of\neach element only when that element is required. In addition to better\ncontrolling the parsing time, this feature is especially useful if you\nwant to parse the most items possible, even if one item in the list\nfails parsing.\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_collection\n    from pandas import DataFrame\n\n    dfs = parse_collection('./demo/simple_collection', DataFrame, lazy_mfcollection_parsing=True)\n    print('dfs length : ' + str(len(dfs)))\n    print('dfs keys : ' + str(dfs.keys()))\n    print('Is b in dfs : ' + str('b' in dfs))\n    pprint(dfs.get('b'))\n\nThe result log shows that ``parse_collection`` returned without parsing,\nand that parsing is executed when item ``'b'`` is read from the\ndictionary:\n\n::\n\n    Executing Parsing Plan for ./demo/simple_collection (multifile) > Dict[str, DataFrame] ------- using Multifile Collection parser (parsyfiles defaults)\n    Parsing ./demo/simple_collection (multifile) > Dict[str, DataFrame] ------- using Multifile Collection parser (parsyfiles defaults)\n    Assembling a Dict[str, DataFrame] from all children of ./demo/simple_collection (multifile) (lazy parsing: children will be parsed when used) \n    --> Successfully parsed a Dict[str, DataFrame] from ./demo/simple_collection\n    Completed parsing successfully\n\n    dfs length : 5\n    dfs keys : {'a', 'e', 'd', 'c', 'b'}\n    Is b in dfs : True\n    Executing Parsing Plan for ./demo/simple_collection\\b (singlefile, .txt) > DataFrame ------- using <read_df_or_series_from_csv>\n    Parsing ./demo/simple_collection\\b (singlefile, .txt) > DataFrame ------- using <read_df_or_series_from_csv>\n    --> Successfully parsed a DataFrame from ./demo/simple_collection\\b\n    Completed parsing successfully\n       a  b  c  d\n    0  1  2  3  4\n\n(b) Passing options to existing parsers\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nParsers and converters support options. In order to know which options\nare available for a specific parser, the best is to identify it and ask\nit. For example if you want to know what are the options available for\nthe parsers reading ``DataFrame`` objects :\n\n.. code:: python\n\n    from pandas import DataFrame\n    from parsyfiles import RootParser\n\n    # create a root parser\n    parser = RootParser()\n\n    # retrieve the parsers of interest\n    parsers = parser.get_capabilities_for_type(DataFrame, strict_type_matching=False)\n    df_csv_parser = parsers['.csv']['1_exact_match'][0]\n    p_id_csv = df_csv_parser.get_id_for_options()\n    print('Parser id for csv is : ' + p_id_csv + ', implementing function is ' + repr(df_csv_parser._parser_func))\n    print(' * ' + df_csv_parser.options_hints())\n    df_xls_parser = parsers['.xls']['1_exact_match'][0]\n    p_id_xls = df_xls_parser.get_id_for_options()\n    print('Parser id for csv is : ' + p_id_xls + ', implementing function is ' + repr(df_xls_parser._parser_func))\n    print(' * ' + df_xls_parser.options_hints())\n\nThe result is:\n\n::\n\n    Parser id for csv is : read_df_or_series_from_csv, implementing function is <function read_df_or_series_from_csv at 0x0000000007391378>\n     * read_df_or_series_from_csv: all options from read_csv are supported, see http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n    Parser id for csv is : read_dataframe_from_xls, implementing function is <function read_dataframe_from_xls at 0x0000000007391158>\n     * read_dataframe_from_xls: all options from read_excel are supported, see http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\n\nThen you may set the options accordingly on the root parser before\ncalling it\n\n.. code:: python\n\n    from parsyfiles import create_parser_options, add_parser_options\n\n    # configure the DataFrame parsers to automatically parse dates and use the first column as index\n    opts = create_parser_options()\n    opts = add_parser_options(opts, 'read_df_or_series_from_csv', {'parse_dates': True, 'index_col': 0})\n    opts = add_parser_options(opts, 'read_dataframe_from_xls', {'index_col': 0})\n\n    dfs = parser.parse_collection('./test_data/demo/ts_collection', DataFrame, options=opts)\n    print(dfs)\n\nResults:\n\n::\n\n    {'a':                    a  b  c  d\n        time                           \n        2015-08-28 23:30:00  1  2  3  4\n        2015-08-29 00:00:00  1  2  3  5, \n     'c':           a  b\n        date            \n        2015-01-01  1  2\n        2015-01-02  4  3, \n     'b':                    a  b  c  d\n        time                           \n        2015-08-28 23:30:00  1  2  3  4\n        2015-08-29 00:00:00  1  2  3  5}\n\n(c) Parsing subclasses of existing types - registering converters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nImagine that you want to parse a subtype of something the framework\nalready knows to parse. For example a ``TimeSeries`` class of your own,\nthat extends ``DataFrame``:\n\n.. code:: python\n\n    from pandas import DataFrame, DatetimeIndex\n\n    class TimeSeries(DataFrame):\n        \"\"\"\n        A dummy timeseries class that extends DataFrame\n        \"\"\"\n\n        def __init__(self, df: DataFrame):\n            \"\"\"\n            Constructor from a DataFrame. The DataFrame index should be an instance of DatetimeIndex\n            :param df:\n            \"\"\"\n            if isinstance(df, DataFrame) and isinstance(df.index, DatetimeIndex):\n                if df.index.tz is None:\n                    df.index = df.index.tz_localize(tz='UTC')# use the UTC hypothesis in absence of other hints\n                self._df = df\n            else:\n                raise ValueError('Error creating TimeSeries from DataFrame: provided DataFrame does not have a '\n                                 'valid DatetimeIndex')\n\n        def __getattr__(self, item):\n            # Redirects anything that is not implemented here to the base dataframe.\n            # this is called only if the attribute was not found the usual way\n\n            # easy version of the dynamic proxy just to save time :)\n            # see http://code.activestate.com/recipes/496741-object-proxying/ for \"the answer\"\n            df = object.__getattribute__(self, '_df')\n            if hasattr(df, item):\n                return getattr(df, item)\n            else:\n                raise AttributeError('\\'' + self.__class__.__name__ + '\\' object has no attribute \\'' + item + '\\'')\n\n        def update(self, other, join='left', overwrite=True, filter_func=None, raise_conflict=False):\n            \"\"\" For some reason this method was abstract in DataFrame so we have to implement it \"\"\"\n            return self._df.update(other, join=join, overwrite=overwrite, filter_func=filter_func,\n                                   raise_conflict=raise_conflict)\n\nIt is relatively easy to write a converter between a ``DataFrame`` and a\n``TimeSeries``. ``parsyfiles`` provides classes that you should use to\ndefine your converters, for example here ``ConverterFunction``, that\ntakes as argument a conversion method with a specific signature - hence\nthe extra unused arguments in ``df_to_ts``:\n\n.. code:: python\n\n    from typing import Type\n    from logging import Logger\n    from parsyfiles.converting_core import ConverterFunction\n\n    def df_to_ts(desired_type: Type[TimeSeries], df: DataFrame, logger: Logger) -> TimeSeries:\n        \"\"\" Converter from DataFrame to TimeSeries \"\"\"\n        return TimeSeries(df)\n\n    my_converter = ConverterFunction(from_type=DataFrame, to_type=TimeSeries, conversion_method=df_to_ts)\n\nYou have to create the parser manually in order to register your\nconverter:\n\n.. code:: python\n\n    from parsyfiles import RootParser, create_parser_options, add_parser_options\n\n    # create a parser\n    parser = RootParser('parsyfiles with timeseries')\n    parser.register_converter(my_converter)\n\nIn some cases you may wish to change the underlying parsers options.\nThis is possible provided that you know the identifier of the parser you\nwish to configure (typically it is the one appearing in the logs):\n\n.. code:: python\n\n    # configure the DataFrame parsers to read the first column as an datetime index\n    opts = create_parser_options()\n    opts = add_parser_options(opts, 'read_df_or_series_from_csv', {'parse_dates': True, 'index_col': 0})\n    opts = add_parser_options(opts, 'read_dataframe_from_xls', {'index_col': 0})\n\nFinally, parsing is done the same way than before:\n\n.. code:: python\n\n    dfs = parser.parse_collection('./test_data/demo/ts_collection', TimeSeries, options=opts)\n\n*Note: you might have noticed that ``TimeSeries`` is a dynamic proxy.\nThe ``TimeSeries`` class extends the ``DataFrame`` class, but delegates\neverything to the underlying ``DataFrame`` implementation provided in\nthe constructor. This pattern is a good way to create specialized\nversions of generic objects created by your favourite parsers. For\nexample two ``DataFrame`` might represent a training set, and a\nprediction table. Both objects, although similar (both are tables with\nrows and columns), might have very different contents (column names,\ncolumn types, number of rows, etc.). We can make this fundamental\ndifference appear at parsing level, by creating two classes.*\n\n(d) Registering a new parser\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nParsyfiles offers several ways to register a parser. Here is a simple\nexample, where we register a basic 'singlefile' xml parser:\n\n.. code:: python\n\n    from typing import Type\n    from parsyfiles import RootParser\n    from parsyfiles.parsing_core import SingleFileParserFunction, T\n    from logging import Logger\n    from xml.etree.ElementTree import ElementTree, parse, tostring\n\n    def read_xml(desired_type: Type[T], file_path: str, encoding: str,\n                 logger: Logger, **kwargs):\n        \"\"\"\n        Opens an XML file and returns the tree parsed from it as an ElementTree.\n\n        :param desired_type:\n        :param file_path:\n        :param encoding:\n        :param logger:\n        :param kwargs:\n        :return:\n        \"\"\"\n        return parse(file_path)\n\n    my_parser = SingleFileParserFunction(parser_function=read_xml,\n                                         streaming_mode=False,\n                                         supported_exts={'.xml'},\n                                         supported_types={ElementTree})\n\n    parser = RootParser('parsyfiles with timeseries')\n    parser.register_parser(my_parser)\n    xmls = parser.parse_collection('./test_data/demo/xml_collection', ElementTree)\n    print({name: tostring(x.getroot()) for name, x in xmls.items()})\n\nFor more examples on how the parser API can be used, please have a look\nat the\n`core <https://github.com/smarie/python-simple-file-collection-parsing-framework/tree/master/parsyfiles/plugins_base>`__\nand\n`optional <https://github.com/smarie/python-simple-file-collection-parsing-framework/tree/master/parsyfiles/plugins_optional>`__\nplugins.\n\n(e) Contract validation for parsed objects : combo with classtools-autocode and attrs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nUsers may wish to use\n`classtools\\_autocode <https://github.com/smarie/python-classtools-autocode>`__\nor `attrs <https://attrs.readthedocs.io/en/stable/>`__ in order to\ncreate very compact classes representing their objects while at the same\ntime ensuring that parsed data is valid according to some contract.\nParsyfiles is totally compliant with such classes, as shown in the\nexamples below\n\nclasstools-autocode example\n'''''''''''''''''''''''''''\n\n.. code:: python\n\n    from classtools_autocode import autoprops, autoargs\n    from contracts import contract, new_contract\n\n    # custom PyContract used in the class\n    new_contract('allowed_op', lambda x: x in {'+','*'})\n\n    @autoprops\n    class ExecOpTest(object):\n        @autoargs\n        @contract(x='int|float', y='int|float', op='str,allowed_op', expected_result='int|float')\n        def __init__(self, x: float, y: float, op: str, expected_result: float):\n            pass\n\n        def __str__(self):\n            return self.__repr__()\n\n        def __repr__(self):\n            return str(self.x) + ' ' + self.op + ' ' + str(self.y) + ' =? ' + str(self.expected_result)\n\n    sf_tests = parse_collection('./demo/simple_objects', ExecOpTest)\n\nThe above code has a contract associated to ``allowed_op`` that checks\nthat it must be in ``{'+','*'}``. When ``'-'`` is found in a test file,\nit fails:\n\n.. code:: bash\n\n    ParsingException : Error while parsing ./demo/simple_objects\\test_diff_1 (singlefile, .cfg) as a ExecOpTest with parser '$<read_config> => <merge_all_config_sections_into_a_single_dict> -> <dict_to_object>$' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : caught \n    ObjectInstantiationException : Error while building object of type <ExecOpTest> using its constructor and parsed contents : {'y': 1.0, 'x': 1.0, 'expected_result': 0.0, 'op': '-'} : \n    <class 'contracts.interface.ContractNotRespected'> Breach for argument 'op' to ExecOpTest:generated_setter_fun().\n    Value does not pass criteria of <lambda>()() (module: test_parsyfiles).\n    checking: callable()       for value: Instance of <class 'str'>: '-'   \n    checking: allowed_op       for value: Instance of <class 'str'>: '-'   \n    checking: str,allowed_op   for value: Instance of <class 'str'>: '-'   \n\nattrs example\n'''''''''''''\n\nIn order for parsyfiles to find the required type for each attribute\ndeclared using ``attrs``, you will have to use\n``attr.validators.instance_of``. However, since you may wish to also\nimplement some custom validation logic, we provide (until it is\noffically added in ``attrs``) a chaining operator. The code below shows\nhow to create a similar example than the previous one:\n\n.. code:: python\n\n    import attr\n    from attr.validators import instance_of\n    from parsyfiles.plugins_optional.support_for_attrs import chain\n\n    # custom contract used in the class\n    def validate_op(instance, attribute, value):\n        allowed = {'+','*'}\n        if value not in allowed:\n            raise ValueError('\\'op\\' has to be a string, in ' + str(allowed) + '!')\n\n    @attr.s\n    class ExecOpTest(object):\n        x = attr.ib(convert=float, validator=instance_of(float))\n        y = attr.ib(convert=float, validator=instance_of(float))\n        # we use the 'chain' validator here to keep using instance_of\n        op = attr.ib(convert=str, validator=chain(instance_of(str), validate_op))\n        expected_result = attr.ib(convert=float, validator=instance_of(float))\n\n    # with self.assertRaises(ParsingException):\n    sf_tests = parse_collection('./test_data/demo/simple_objects', ExecOpTest)\n\nWhen ``'-'`` is found in a test file, it also fails with a nice error\nmessage:\n\n.. code:: bash\n\n    ParsingException : Error while parsing ./test_data/demo/simple_objects\\test_diff_1 (singlefile, .cfg) as a ExecOpTest with parser '$<read_config> => <merge_all_config_sections_into_a_single_dict> -> <dict_to_object>$' using options=({'MultifileCollectionParser': {'lazy_parsing': False}}) : caught \n    ObjectInstantiationException : Error while building object of type <ExecOpTest> using its constructor and parsed contents : {'y': 1.0, 'x': 1.0, 'expected_result': 0.0, 'op': '-'} : \n    <class 'ValueError'> 'op' has to be a string, in {'*', '+'}!\n\nNote: unfortunately, as of today (version 16.3), ``attrs`` does not\nvalidate attribute contents when fields are later modified on the object\ndirectly. A pull request is ongoing.\n\n(e) File mappings: Wrapped/Flat and encoding\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn `3- Multifile objects: combining several\nparsers <#3--multifile-objects-combining-several-parsers>`__ we used\nfolders to encapsulate objects. In previous examples we also used the\nroot folder to encapsulate the main item collection. This default\nsetting is known as 'Wrapped' mode and correspond behind the scenes to a\n``WrappedFileMappingConfiguration`` being used, with default python\nencoding.\n\nAlternatively you may wish to use flat mode. In this case the folder\nstructure should be flat, as shown below. Item names and field names are\nseparated by a configurable character string. For example to parse the\nsame example as in `3- Multifile objects: combining several\nparsers <#3--multifile-objects-combining-several-parsers>`__ but with\nthe following flat tree structure:\n\n.. code:: bash\n\n    .\n    \u251c\u2500\u2500 case1--expected_results.txt\n    \u251c\u2500\u2500 case1--x.csv\n    \u251c\u2500\u2500 case1--y.txt\n    \u251c\u2500\u2500 case2--expected_results.txt\n    \u251c\u2500\u2500 case2--x.csv\n    \u2514\u2500\u2500 case2--y.txt\n\nyou'll need to call\n\n.. code:: python\n\n    from parsyfiles import FlatFileMappingConfiguration\n    dfs = parse_collection('./demo/complex_objects_flat', DataFrame, file_mapping_conf=FlatFileMappingConfiguration())\n\nNote that ``FlatFileMappingConfiguration`` may be configured to use\nanother separator sequence than ``'--'`` by passing it to the\nconstructor: e.g. ``FlatFileMappingConfiguration(separator='_')``. A dot\n``'.'`` may be safely used as a separator too.\n\nFinally you may change the file encoding used by both file mapping\nconfigurations : ``WrappedFileMappingConfiguration(encoding='utf-16')``\n``FlatFileMappingConfiguration(encoding='utf-16')``.\n\n(f) Recursivity: Multifile children of Multifile objects\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAs said earlier in this tutorial, parsyfiles is able to parse multifile\nrecursively, for example multifile collections of multifile objects,\nmultifile objects containing attributes, etc.\n\nExample recursivity in flat mode\n''''''''''''''''''''''''''''''''\n\n::\n\n    ./custom_old_demo_flat_coll\n    \u251c\u2500\u2500 case1--input_a.txt\n    \u251c\u2500\u2500 case1--input_b.txt\n    \u251c\u2500\u2500 case1--output.txt\n    \u251c\u2500\u2500 case2--input_a.txt\n    \u251c\u2500\u2500 case2--input_b.txt\n    \u251c\u2500\u2500 case2--options.txt\n    \u251c\u2500\u2500 case2--output.txt\n    \u251c\u2500\u2500 case3--input_a.txt\n    \u251c\u2500\u2500 case3--input_b.txt\n    \u251c\u2500\u2500 case3--input_c--keyA--item1.txt\n    \u251c\u2500\u2500 case3--input_c--keyA--item2.txt\n    \u251c\u2500\u2500 case3--input_c--keyB--item1.txt\n    \u251c\u2500\u2500 case3--options.cfg\n    \u2514\u2500\u2500 case3--output.txt\n\nExample recursivity in wrapped mode\n'''''''''''''''''''''''''''''''''''\n\n::\n\n    ./custom_old_demo_coll\n    \u251c\u2500\u2500 case1\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input_a.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input_b.txt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 output.txt\n    \u251c\u2500\u2500 case2\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input_a.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input_b.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 options.txt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 output.txt\n    \u2514\u2500\u2500 case3\n        \u251c\u2500\u2500 input_a.txt\n        \u251c\u2500\u2500 input_b.txt\n        \u251c\u2500\u2500 input_c\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 keyA\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 item1.txt\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 item2.txt\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 keyB\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 item1.txt\n        \u251c\u2500\u2500 options.cfg\n        \u2514\u2500\u2500 output.txt\n\n(g) Diversity of formats supported: DataFrames - revisited\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNow that we've seen that parsyfiles is able to combine parsers and\nconverters, we can try to parse ``DataFrame`` objects from many more\nsources:\n\n::\n\n    ./demo/simple_collection_dataframe_inference\n    \u251c\u2500\u2500 a.csv\n    \u251c\u2500\u2500 b.txt\n    \u251c\u2500\u2500 c.xls\n    \u251c\u2500\u2500 d.xlsx\n    \u251c\u2500\u2500 s_b64_pickle.txt\n    \u251c\u2500\u2500 t_pickle.pyc\n    \u251c\u2500\u2500 u.json\n    \u251c\u2500\u2500 v_properties.txt\n    \u251c\u2500\u2500 w.properties\n    \u251c\u2500\u2500 x.yaml\n    \u251c\u2500\u2500 y.cfg\n    \u2514\u2500\u2500 z.ini\n\n*Note: once again you may find this example data folder in the `project\nsources <https://github.com/smarie/python-simple-file-collection-parsing-framework/tree/master/parsyfiles/test_data>`__*\n\nThe code is the same:\n\n.. code:: python\n\n    from pprint import pprint\n    from parsyfiles import parse_collection\n    from pandas import DataFrame\n\n    dfs = parse_collection('./demo/simple_collection_dataframe_inference', DataFrame)\n    pprint(dfs)\n\nAnd here is the result\n\n::\n\n    TODO\n\nSee Also\n--------\n\n-  Check `here <https://github.com/webmaven/python-parsing-tools>`__ for\n   other parsers in Python, that you might wish to register as unitary\n   parsers to perform specific file format parsing (binary, json,\n   custom...) for some of your objects.\n\n-  Do you like this library ? You might also like\n   `these <https://github.com/smarie?utf8=%E2%9C%93&tab=repositories&q=&type=&language=python>`__\n\n-  This `cattrs <https://cattrs.readthedocs.io/en/latest/readme.html>`__\n   project seems to have related interests, to check.\n\nDevelopers\n----------\n\nPackaging\n~~~~~~~~~\n\nThis project uses ``setuptools_scm`` to synchronise the version number.\nTherefore the following command should be used for development snapshots\nas well as official releases:\n\n.. code:: bash\n\n    python setup.py egg_info bdist_wheel rotate -m.whl -k3\n\nReleasing memo\n~~~~~~~~~~~~~~\n\n.. code:: bash\n\n    twine register dist/* -r pypitest\n    twine upload dist/* -r pypitest\n    twine register dist/*\n    twine upload dist/*\n\n\n",
    "docs_url": null,
    "download_url": "https://github.com/smarie/python-simple-file-collection-parsing-framework/tarball/2.1.0",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/smarie/python-simple-file-collection-parsing-framework",
    "keywords": "file collection complex simple test object parser converter parsing framework PEP484 typing data binding SFiCoPaF",
    "license": "BSD 3-Clause",
    "maintainer": "",
    "maintainer_email": "",
    "name": "parsyfiles",
    "platform": "",
    "project_url": "https://pypi.org/project/parsyfiles/",
    "release_url": "https://pypi.org/project/parsyfiles/2.1.0/",
    "requires_dist": [
      "pyyaml; extra == 'yaml_parser'",
      "pandas; extra == 'pandas_parser'",
      "numpy; extra == 'pandas_parser'",
      "numpy; extra == 'numpy_parser'",
      "jprops; extra == 'jprops_parser'",
      "classtools-autocode; extra == 'classtools_autocode'"
    ],
    "requires_python": "",
    "summary": "Combines most popular python parsers (json, jprops, pickle...) with user-defined parsers and type converters to read objects from files. Supports multifile & multiparser objects, typically useful to organize test data. Leverages PEP484 type hints in order to intelligently use the best parser/converter chain, and to try several combinations if relevant",
    "version": "2.1.0"
  },
  "releases": {
    "2.0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "0f19826733421411ccda7ba390f35121",
          "sha256": "a5ae9d178e91c8785435bc32840b87ad02cf4a0ca71a03ddcec56b19f97e782d"
        },
        "downloads": 0,
        "filename": "parsyfiles-2.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0f19826733421411ccda7ba390f35121",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 106761,
        "upload_time": "2017-05-05T13:27:05",
        "url": "https://files.pythonhosted.org/packages/ca/2a/ba17333801542a169f92d40422136311571f436ae334c40dba01ce8de997/parsyfiles-2.0.2-py3-none-any.whl"
      }
    ],
    "2.1.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "69437c98b21e349e046c20129f76710a",
          "sha256": "c0712666c9849eac6b7d694c66cd0d1007c7d7b4bff135ac702b814c1f34b673"
        },
        "downloads": 0,
        "filename": "parsyfiles-2.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "69437c98b21e349e046c20129f76710a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 108036,
        "upload_time": "2017-06-29T12:24:00",
        "url": "https://files.pythonhosted.org/packages/b8/ed/395f27b5cf072f89061eccc82b4c7f0e42d2b6bbd50d46eff1336ee38053/parsyfiles-2.1.0-py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "69437c98b21e349e046c20129f76710a",
        "sha256": "c0712666c9849eac6b7d694c66cd0d1007c7d7b4bff135ac702b814c1f34b673"
      },
      "downloads": 0,
      "filename": "parsyfiles-2.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "69437c98b21e349e046c20129f76710a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 108036,
      "upload_time": "2017-06-29T12:24:00",
      "url": "https://files.pythonhosted.org/packages/b8/ed/395f27b5cf072f89061eccc82b4c7f0e42d2b6bbd50d46eff1336ee38053/parsyfiles-2.1.0-py3-none-any.whl"
    }
  ]
}