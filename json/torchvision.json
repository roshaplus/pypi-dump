{
  "info": {
    "author": "PyTorch Core Team",
    "author_email": "soumith@pytorch.org",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "torch-vision\n============\n\n.. image:: https://travis-ci.org/pytorch/vision.svg?branch=master\n    :target: https://travis-ci.org/pytorch/vision\n\nThis repository consists of:\n\n-  `vision.datasets <#datasets>`__ : Data loaders for popular vision\n   datasets\n-  `vision.models <#models>`__ : Definitions for popular model\n   architectures, such as AlexNet, VGG, and ResNet and pre-trained\n   models.\n-  `vision.transforms <#transforms>`__ : Common image transformations\n   such as random crop, rotations etc.\n-  `vision.utils <#utils>`__ : Useful stuff such as saving tensor (3 x H\n   x W) as image to disk, given a mini-batch creating a grid of images,\n   etc.\n\nInstallation\n============\n\nAnaconda:\n\n.. code:: bash\n\n    conda install torchvision -c soumith\n\npip:\n\n.. code:: bash\n\n    pip install torchvision\n\n>From source:\n\n.. code:: bash\n\n    python setup.py install\n\nDatasets\n========\n\nThe following dataset loaders are available:\n\n-  `MNIST <#mnist>`__\n-  `COCO (Captioning and Detection) <#coco>`__\n-  `LSUN Classification <#lsun>`__\n-  `ImageFolder <#imagefolder>`__\n-  `Imagenet-12 <#imagenet-12>`__\n-  `CIFAR10 and CIFAR100 <#cifar>`__\n-  `STL10 <#stl10>`__\n-  `SVHN <#svhn>`__\n-  `PhotoTour <#phototour>`__\n\nDatasets have the API: - ``__getitem__`` - ``__len__`` They all subclass\nfrom ``torch.utils.data.Dataset`` Hence, they can all be multi-threaded\n(python multiprocessing) using standard torch.utils.data.DataLoader.\n\nFor example:\n\n``torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads)``\n\nIn the constructor, each dataset has a slightly different API as needed,\nbut they all take the keyword args:\n\n-  ``transform`` - a function that takes in an image and returns a\n   transformed version\n-  common stuff like ``ToTensor``, ``RandomCrop``, etc. These can be\n   composed together with ``transforms.Compose`` (see transforms section\n   below)\n-  ``target_transform`` - a function that takes in the target and\n   transforms it. For example, take in the caption string and return a\n   tensor of word indices.\n\nMNIST\n~~~~~\n``dset.MNIST(root, train=True, transform=None, target_transform=None, download=False)``\n\n``root``: root directory of dataset where ``processed/training.pt`` and ``processed/test.pt`` exist\n\n``train``: ``True`` - use training set, ``False`` - use test set.\n\n``transform``: transform to apply to input images\n\n``target_transform``: transform to apply to targets (class labels)\n\n``download``: whether to download the MNIST data\n\n\nCOCO\n~~~~\n\nThis requires the `COCO API to be\ninstalled <https://github.com/pdollar/coco/tree/master/PythonAPI>`__\n\nCaptions:\n^^^^^^^^^\n\n``dset.CocoCaptions(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nExample:\n\n.. code:: python\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n    cap = dset.CocoCaptions(root = 'dir where images are',\n                            annFile = 'json annotation file',\n                            transform=transforms.ToTensor())\n\n    print('Number of samples: ', len(cap))\n    img, target = cap[3] # load 4th sample\n\n    print(\"Image Size: \", img.size())\n    print(target)\n\nOutput:\n\n::\n\n    Number of samples: 82783\n    Image Size: (3L, 427L, 640L)\n    [u'A plane emitting smoke stream flying over a mountain.',\n    u'A plane darts across a bright blue sky behind a mountain covered in snow',\n    u'A plane leaves a contrail above the snowy mountain top.',\n    u'A mountain that has a plane flying overheard in the distance.',\n    u'A mountain view with a plume of smoke in the background']\n\nDetection:\n^^^^^^^^^^\n\n``dset.CocoDetection(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nLSUN\n~~~~\n\n``dset.LSUN(db_path, classes='train', [transform, target_transform])``\n\n-  ``db_path`` = root directory for the database files\n-  ``classes`` =\n-  ``'train'`` - all categories, training set\n-  ``'val'`` - all categories, validation set\n-  ``'test'`` - all categories, test set\n-  [``'bedroom_train'``, ``'church_train'``, ...] : a list of categories to\n   load\n\nCIFAR\n~~~~~\n\n``dset.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)``\n\n``dset.CIFAR100(root, train=True, transform=None, target_transform=None, download=False)``\n\n-  ``root`` : root directory of dataset where there is folder\n   ``cifar-10-batches-py``\n-  ``train`` : ``True`` = Training set, ``False`` = Test set\n-  ``download`` : ``True`` = downloads the dataset from the internet and\n   puts it in root directory. If dataset is already downloaded, does not do\n   anything.\n\nSTL10\n~~~~~\n\n``dset.STL10(root, split='train', transform=None, target_transform=None, download=False)``\n\n-  ``root`` : root directory of dataset where there is folder ``stl10_binary``\n-  ``split`` : ``'train'`` = Training set, ``'test'`` = Test set, ``'unlabeled'`` = Unlabeled set,\n    ``'train+unlabeled'`` = Training + Unlabeled set (missing label marked as ``-1``)\n-  ``download`` : ``True`` = downloads the dataset from the internet and\n    puts it in root directory. If dataset is already downloaded, does not do\n    anything.\n\nSVHN\n~~~~\n\n``dset.SVHN(root, split='train', transform=None, target_transform=None, download=False)``\n\n-  ``root`` : root directory of dataset where there is folder ``SVHN``\n-  ``split`` : ``'train'`` = Training set, ``'test'`` = Test set, ``'extra'`` = Extra training set\n-  ``download`` : ``True`` = downloads the dataset from the internet and\n    puts it in root directory. If dataset is already downloaded, does not do\n    anything.\n\nImageFolder\n~~~~~~~~~~~\n\nA generic data loader where the images are arranged in this way:\n\n::\n\n    root/dog/xxx.png\n    root/dog/xxy.png\n    root/dog/xxz.png\n\n    root/cat/123.png\n    root/cat/nsdf3.png\n    root/cat/asd932_.png\n\n``dset.ImageFolder(root=\"root folder path\", [transform, target_transform])``\n\nIt has the members:\n\n-  ``self.classes`` - The class names as a list\n-  ``self.class_to_idx`` - Corresponding class indices\n-  ``self.imgs`` - The list of (image path, class-index) tuples\n\nImagenet-12\n~~~~~~~~~~~\n\nThis is simply implemented with an ImageFolder dataset.\n\nThe data is preprocessed `as described\nhere <https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset>`__\n\n`Here is an\nexample <https://github.com/pytorch/examples/blob/27e2a46c1d1505324032b1d94fc6ce24d5b67e97/imagenet/main.py#L48-L62>`__.\n\nPhotoTour\n~~~~~~~~~\n\n**Learning Local Image Descriptors Data**\nhttp://phototour.cs.washington.edu/patches/default.htm\n\n.. code:: python\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n    dataset = dset.PhotoTour(root = 'dir where images are',\n                             name = 'name of the dataset to load',\n                             transform=transforms.ToTensor())\n\n    print('Loaded PhotoTour: {} with {} images.'\n          .format(dataset.name, len(dataset.data)))\n\nModels\n======\n\nThe models subpackage contains definitions for the following model\narchitectures:\n\n-  `AlexNet <https://arxiv.org/abs/1404.5997>`__: AlexNet variant from\n   the \"One weird trick\" paper.\n-  `VGG <https://arxiv.org/abs/1409.1556>`__: VGG-11, VGG-13, VGG-16,\n   VGG-19 (with and without batch normalization)\n-  `ResNet <https://arxiv.org/abs/1512.03385>`__: ResNet-18, ResNet-34,\n   ResNet-50, ResNet-101, ResNet-152\n-  `SqueezeNet <https://arxiv.org/abs/1602.07360>`__: SqueezeNet 1.0, and\n   SqueezeNet 1.1\n\nYou can construct a model with random weights by calling its\nconstructor:\n\n.. code:: python\n\n    import torchvision.models as models\n    resnet18 = models.resnet18()\n    alexnet = models.alexnet()\n    vgg16 = models.vgg16()\n    squeezenet = models.squeezenet1_0()\n\nWe provide pre-trained models for the ResNet variants, SqueezeNet 1.0 and 1.1,\nand AlexNet, using the PyTorch `model zoo <http://pytorch.org/docs/model_zoo.html>`__.\nThese can be constructed by passing ``pretrained=True``:\n\n.. code:: python\n\n    import torchvision.models as models\n    resnet18 = models.resnet18(pretrained=True)\n    alexnet = models.alexnet(pretrained=True)\n    squeezenet = models.squeezenet1_0(pretrained=True)\n\n\nAll pre-trained models expect input images normalized in the same way, i.e.\nmini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected\nto be atleast 224.\n\nThe images have to be loaded in to a range of [0, 1] and then\nnormalized using `mean=[0.485, 0.456, 0.406]` and `std=[0.229, 0.224, 0.225]`\n\nAn example of such normalization can be found in `the imagenet example here` <https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101>\n\nTransforms\n==========\n\nTransforms are common image transforms. They can be chained together\nusing ``transforms.Compose``\n\n``transforms.Compose``\n~~~~~~~~~~~~~~~~~~~~~~\n\nOne can compose several transforms together. For example.\n\n.. code:: python\n\n    transform = transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n                              std = [ 0.229, 0.224, 0.225 ]),\n    ])\n\nTransforms on PIL.Image\n~~~~~~~~~~~~~~~~~~~~~~~\n\n``Scale(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRescales the input PIL.Image to the given 'size'. 'size' will be the\nsize of the smaller edge.\n\nFor example, if height > width, then image will be rescaled to (size \\*\nheight / width, size) - size: size of the smaller edge - interpolation:\nDefault: PIL.Image.BILINEAR\n\n``CenterCrop(size)`` - center-crops the image to the given size\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at the center to have a region of the given\nsize. size can be a tuple (target\\_height, target\\_width) or an integer,\nin which case the target will be of a square shape (size, size)\n\n``RandomCrop(size, padding=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at a random location to have a region of the\ngiven size. size can be a tuple (target\\_height, target\\_width) or an\ninteger, in which case the target will be of a square shape (size, size)\nIf ``padding`` is non-zero, then the image is first zero-padded on each\nside with ``padding`` pixels.\n\n``RandomHorizontalFlip()``\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandomly horizontally flips the given PIL.Image with a probability of\n0.5\n\n``RandomSizedCrop(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandom crop the given PIL.Image to a random size of (0.08 to 1.0) of the\noriginal size and and a random aspect ratio of 3/4 to 4/3 of the\noriginal aspect ratio\n\nThis is popularly used to train the Inception networks - size: size of\nthe smaller edge - interpolation: Default: PIL.Image.BILINEAR\n\n``Pad(padding, fill=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nPads the given image on each side with ``padding`` number of pixels, and\nthe padding pixels are filled with pixel value ``fill``. If a ``5x5``\nimage is padded with ``padding=1`` then it becomes ``7x7``\n\nTransforms on torch.\\*Tensor\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``Normalize(mean, std)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nGiven mean: (R, G, B) and std: (R, G, B), will normalize each channel of\nthe torch.\\*Tensor, i.e. channel = (channel - mean) / std\n\nConversion Transforms\n~~~~~~~~~~~~~~~~~~~~~\n\n-  ``ToTensor()`` - Converts a PIL.Image (RGB) or numpy.ndarray (H x W x\n   C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W)\n   in the range [0.0, 1.0]\n-  ``ToPILImage()`` - Converts a torch.\\*Tensor of range [0, 1] and\n   shape C x H x W or numpy ndarray of dtype=uint8, range[0, 255] and\n   shape H x W x C to a PIL.Image of range [0, 255]\n\nGeneric Transforms\n~~~~~~~~~~~~~~~~~~\n\n``Lambda(lambda)``\n^^^^^^^^^^^^^^^^^^\n\nGiven a Python lambda, applies it to the input ``img`` and returns it.\nFor example:\n\n.. code:: python\n\n    transforms.Lambda(lambda x: x.add(10))\n\nUtils\n=====\n\nmake\\_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale\\_each=False)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGiven a 4D mini-batch Tensor of shape (B x C x H x W),\nor a list of images all of the same size,\nmakes a grid of images\n\nnormalize=True will shift the image to the range (0, 1),\nby subtracting the minimum and dividing by the maximum pixel value.\n\nif range=(min, max) where min and max are numbers, then these numbers are used to\nnormalize the image.\n\nscale_each=True will scale each image in the batch of images separately rather than\ncomputing the (min, max) over all images.\n\n`Example usage is given in this notebook` <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>\n\nsave\\_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale\\_each=False)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSaves a given Tensor into an image file.\n\nIf given a mini-batch tensor, will save the tensor as a grid of images.\n\nAll options after `filename` are passed through to `make_grid`. Refer to it's documentation for\nmore details\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/pytorch/vision",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchvision",
    "platform": "",
    "project_url": "https://pypi.org/project/torchvision/",
    "release_url": "https://pypi.org/project/torchvision/0.1.8/",
    "requires_python": "",
    "summary": "image and video datasets and models for torch deep learning",
    "version": "0.1.8"
  },
  "releases": {
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "md5": "dd0c6261184599679bdad83387b7ff9e",
          "sha256": "734f95a39d6be2355479b6809278e249cebaa7b6644e57f81c10e8b449e18b5f"
        },
        "downloads": 13,
        "filename": "torchvision-0.1.6-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd0c6261184599679bdad83387b7ff9e",
        "packagetype": "bdist_wheel",
        "python_version": "py2",
        "size": 16179,
        "upload_time": "2017-01-18T23:55:09",
        "url": "https://files.pythonhosted.org/packages/fc/1f/657b9e8c0b2abf3808727543cd00fa62c417efd201a0b2c3703004046204/torchvision-0.1.6-py2-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "253de5948d0f45d3bac223bcaca12023",
          "sha256": "5ec9edf604160b4dcf857a66bd499502173e74d4d7a83bd55b08244f6dacb9d7"
        },
        "downloads": 11,
        "filename": "torchvision-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "253de5948d0f45d3bac223bcaca12023",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 16179,
        "upload_time": "2017-01-18T23:34:18",
        "url": "https://files.pythonhosted.org/packages/ce/39/62f84a4b2aa94a12130857c3aeaa9ad5eebd279cc331bc9c9cbeb27acc27/torchvision-0.1.6-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "dd55df31db1c5f46739c35820b478c25",
          "sha256": "8b6b29e5cdeff91d6ba9a3448db442a22f6839f7a362fd09f11919b915072519"
        },
        "downloads": 16,
        "filename": "torchvision-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "dd55df31db1c5f46739c35820b478c25",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 11305,
        "upload_time": "2017-01-18T23:35:47",
        "url": "https://files.pythonhosted.org/packages/a7/15/b78b0a56af3395c7e93ab2d70e9b99ed63aef4522a260334f9dda36edc0f/torchvision-0.1.6.tar.gz"
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "ebafe580c103f71e245d3d8c013d771b",
          "sha256": "ee509d6b1cb10be6daad4b7e72712d914cae5f7edca61737795e0d7d41c17826"
        },
        "downloads": 160,
        "filename": "torchvision-0.1.7-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ebafe580c103f71e245d3d8c013d771b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 23286,
        "upload_time": "2017-01-19T21:12:07",
        "url": "https://files.pythonhosted.org/packages/0f/c0/262ab7e4ff08c3c1c74e02285bc225e8135b25ea527b762f3d690417a657/torchvision-0.1.7-py2.py3-none-any.whl"
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "28ec9aac051af27b21d96cf5883fb799",
          "sha256": "2885d02c90541a9888b8881a722862ff53bbf73a2d7617d7c670ccd33121029f"
        },
        "downloads": 517,
        "filename": "torchvision-0.1.8-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "28ec9aac051af27b21d96cf5883fb799",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "size": 37843,
        "upload_time": "2017-03-29T22:24:13",
        "url": "https://files.pythonhosted.org/packages/8c/52/33d739bcc547f22c522def535a8da7e6e5a0f6b98594717f519b5cb1a4e1/torchvision-0.1.8-py2.py3-none-any.whl"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "28ec9aac051af27b21d96cf5883fb799",
        "sha256": "2885d02c90541a9888b8881a722862ff53bbf73a2d7617d7c670ccd33121029f"
      },
      "downloads": 517,
      "filename": "torchvision-0.1.8-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "28ec9aac051af27b21d96cf5883fb799",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "size": 37843,
      "upload_time": "2017-03-29T22:24:13",
      "url": "https://files.pythonhosted.org/packages/8c/52/33d739bcc547f22c522def535a8da7e6e5a0f6b98594717f519b5cb1a4e1/torchvision-0.1.8-py2.py3-none-any.whl"
    }
  ]
}