{
  "info": {
    "author": "Florian Finkernagel",
    "author_email": "finkernagel@imt.uni-marburg.de",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python",
      "Programming Language :: Python :: 3"
    ],
    "description": "Introduction\n----------------\n\n[pypipegraph](https://github.com/TyberiusPrime/pypipegraph): is an MIT-licensed library for constructing a workflow piece by piece and\nexecuting just the parts of it that need to be (re-)done.  It supports using\nmultiple cores (SMP) and (eventually, alpha code right now) machines (cluster)\nand is a hybrid between a dependency tracker (think 'make') and a cluster\nengine.\n\nMore specifically, you construct Jobs, which encapsulate output (i.e. stuff that\nneeds to be done), invariants (which force re-evaluation of output jobs if \nthey change), and stuff inbetween (e.g. load data from disk).\n\nFrom your point of view, you create a pypipegraph, you create jobs, chain\nthem together, then ask the pypipegraph to run.\nIt examines all jobs for their need to run (either because the have not been\nfinished, or because they have been invalidated), \ndistributes them across multiple python instances, and get's\nthem executed in a sensible order.\n\nIt is robust against jobs dying for whatever reason (only the failed job and \neverything 'downstream' will be affected, independend jobs will continue running),\nallows you to resume at any point 'in between' jobs, and isolates jobs against each other.\n\npypipegraph supports Python 3.\n\n30 second summary\n------------------\n::\n\n    pypipegraph.new_pipeline()\n    output_filenameA = 'sampleA.txt'\n    def do_the_work():\n        op = open(output_filename, 'wb').write(\"hello world\")\n    jobA = pypipegraph.FileGeneratingJob(output_filenameA, do_the_work)\n    output_filenameB = 'sampleB.txt'\n    def do_the_work():\n         op = open(output_filenameB, 'wb').write(open(output_filenameA, 'rb').read() + \",  once again\")\n    jobB = pypipegraph.FileGeneratingJob(output_filenameB, do_the_work)\n    jobB.depends_on(jobA)\n    pypipegraph.run()\n    print 'the pipegraph is done and has returned control to you.'\n    print 'sampleA.txt contains \"hello world\"'\n    print 'sampleB.txt contains \"hello world, once again\"\n\n\n\nJobs\n-------------\n\nAll jobs have a unique name (job_id), and all but invariant preserving jobs encapsulate\npython callbacks to do their work.\n\n\nThere are four basic kinds of jobs:\n\n* `Output generating jobs`_. These are generally run in a fork of the cs, they might use multi cores (if you call an external pnon-python program), and they modify the outside world (create files, change databases, call the web).\n\n  They do not modify the state of your program - neither on the mcp nor on the cs level.\n\n* `Invariant preserving jobs`_. These jobs are used to check that nothing changed. If it did, the downstream jobs need to be redone. Typical examples of what is checked are file modification time, file checksum, parameters, or changes to python functions.\n\n  They are evaluated in the mcp and never 'ran' anywhere.\n\n* `Compute slave modifying jobs`_. These jobs load data into a compute slave that is then available (read-only) in all dependend output generating jobs (and it even only needs memory once thanks to linux forking behaviour). \n\n  They are not run if there isn't an output job depending on them that needs to be done. They run single core per machine (since they must be run within the compute slave). They may run multiple times (on different machines). They modify compute slaves!\n\n* `Job generating jobs`_. They extend the job graph itself - either by creating new jobs depending on the data at hand (for example, create a graph for every line in a file generated by an output job), or by injecting further dependencies.\n\n  They are executed in a compute slave (since they might well use data that was loaded by cs modifying jobs) and the jobs generated are transfered back to the mcp (so the mcp is still isolated from their execution function).\n  **Note**: Job generating jobs are run each time the pipeline runs - otherwise we could not check whether the generated jobs have been done. That probably also means that their dependencies should be 'lightweight' in terms of runtime.\n\nIn addition, there are `compound jobs`_ that combine jobs for convienance.\n\n\nInvariant preserving jobs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nParameterInvariant\n__________________\nEncapsulate a tuple defining some parameters to another job. If the parameters change, dependend jobs are redone.\n\nFileTimeInvariant\n_________________\nCapture the file modification time of an input file.\n\nFileChecksumInvariant\n_____________________\nCapture the file modification time, file size and the md5 sum of a file. If the size changes, invalidate dependands. If the time changes, check md5, if it changed, invalidate dependands. Otherwise, update internal file modification time.\n\nFunctionInvariant\n_________________\nCompare (slightly sanitized) byte code of arbitrary python functions to its last definition. If it changed: invalidate dependands.\n\n\n\n\nOutput generating jobs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFileGeneratingJob\n_________________\nThis job creates an output file. \n\nIt get's rerun if the output file does not exist, if it is 0 byte in size, or if the job has been invalidated. Not creating the output file in its callback raises an exception.\n\nIf the job raises an exception, the output file created so far will be removed. \n(optionally, if you pass rename_broken = True when creating, the output filename get's renamed into outputfilename + '.broken')\n\nThe job receives an implicit FunctionInvariant on its callback. This can be supressed by calling myjob.ignore_code_changes()\n\nThe jobid doubles up as the output filename.\n\n\n\nMultiFileGeneratingJob\n________________________________\nSame as a FileGeneratingJob_, just that multiple (defined) files are checked and produced.\n\n\nTempFileGeneratingJob\n_______________________\n\nThis job creates a file that directly dependand jobs might access.\nAfter those are through though, the file get's removed.\nIt does not get removed if any directly dependand job dies (instead of finishing ok), so that it does not need to be redone later.\nIt does get removed if the TempFileGeneratingJob breaks (except if you pass rename_broken = True when construcing it).\n\n\nCompute slave modifying jobs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAttributeLoadingJob\n___________________\nLoad data (from wherever), and store it as attribute on an object.\nOnly occurs if there is another Job depending on it that needs o be done.\nMight occur multiple times on different machines.\n\nThe attribute might disappear after all directly dependand jobs have been run (Todo: I'm not sure\nhow smart this idea is. On the one hand, it allows us to clear some memory. On the other hand,\nit makes some things mighty cumbersome.)\n\nDataLoadingJob\n_______________\nSimilar to an AttributeLoadingJob_, but does not do anything about the data storage - that's\nyour job.\n\nDataLoadingJobs don't directly require their prerequisits to be done - so chaining a bunch of them,\nand having a FileGeneratingJob_ at either makes all of them run (given that the FileGeneratingJob_ is\nnot done) or none of them.\n\nOn the plus side, the data loaded via this job does not get automatically eaten once all dependend\njobs have been done. (Todo)\n\n\n\nJob generating jobs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDependencyInjectionJob\n________________________\nThis job injects further dependencies into jobs that already depends on it.\n\nAn example: Say you want to extract the titles from a bunch of downloaded websites.\nBut the websites are not defined when first creating jobs - you first have to download an overview\npage.\nSo you write a job that takes all .html in a folder and combines their titles (Z).\nYou also create a job that downloads the overview (A) .\nThen you write a DependencyInjectionJob C (upon wich Z depends on) that depends on A, creates a bunch of jobs (B1...Bn) that\ndownload the single websites, and you return them. Z now also depends on B1..Bn and since Z also depended on C, \nit could not have run before it had the additional dependencies injected.\n\nIt is an error to add dependencies on a job that is not dependand on this DependencyInjectionJob.\n\nA DependencyInjectionJob has an implicit FunctionInvariant_.\n\n\nJobGeneratingJob\n________________\nThis job generates new jobs that depend on the output of an earlier one.\n\nExample: You want to draw a graph for each of the member of your sales team.\nWhat members there are you of course only know after querying the database for\nthem.  You write a JobGeneratingJob that queries the database for a list of\nsales team members (or depends on some other job if the action is more\nexpensive). For each sales team member it creates a FileGeneratingJob_ that\nqueries the database for this members figures and draws the graph.\nThe FileGeneratingJobs now can run in parallel even on different machines...\n\nA JobGeneratingJob has an implicit FunctionInvariant_.\n\n\nCompound jobs\n~~~~~~~~~~~~~~~\n\nPlotJob\n_________\nThis job wraps plotting with pyggplot. It takes two functions: one calculates\nthe dataframe for the plot, and that result is cached. The other one\nloads that dataframe and returns a pyggplot.Plot.\n\nBoth have their own FunctionInvariant_, so you can fiddle with the plot function without having the calculation part\nrerun.\n\nThe calc function must return a pydataframe.DataFrame, the plot function a pyggplot.Plot\n\nCachedJob\n__________\nA CachedJob is a combination of of a AttributeLoadingJob and a FileGeneratingJob.\nIt has a single callback, that returns some hard-to-compute value, which is\npickled to a file (jobid doubles as cache file name). \nThe AttributeLoadingJob loads the data in question if necessary.\n\nThe calc function does not get run if there are no dependencies.\n\nIt also has an implicit FunctionInvariant_ on it's calc function (supress just like a FileGeneratingJob_ with ignore_code_changes())\n\n\nExceptions\n-----------\npypipegraph has a small set of exceptions (all descending from PyPipelineGraphError).\n* RuntimeError get's thrown by pypipegraph.run if a job raised an exception, communication lines were broken etc\n* JobContractError is stored in a job's .exception if the job's callback did not comply with it's requirements (e.g. a FileGeneratingJob did not actually create the file)\n* CycleError: you have fabricated a cycle in your dependencies. Unfortunatly it's currently not reported where the cycle is (though some simple circles are reported early on)\n\n\nRuntime\n----------\nWhile the pypipegraph is running, you can terminate it with CTRL-C, and query it about which jobs are running \nby pressing Enter (that might take up to 5 seconds though, which incidentially is the 'check on slaves' timeout)\n\n\nExecuting structure\n-----------------------\n\nYou write a 'master control program' (mcp) that creates Jobs and at one point,\nyou hand over control to the pypipegraph.\nThe mcp then talks to a resource-coordinator (either a local instance that says\n'take all of this machine' or a network service that coordinates between multiple\nunning pypipegraphs) and spawns one compute slave (cs) for each machine.\n\nNow each compute slave receives a copy of all jobs (which are just definitions,\nand therefore pretty small).  One by one the mcp (talking to the\nresource-coordinator) asks the cs to execute jobs (while talking to the\nresource-coordinater to share resources with others), collects their feedback, prunes the graph on errors and\nreturns control to you once all of them have been done (or failed ;) ).\n\nThe mcp knows (thanks to the resource coordinator) about the resources available\n(number of cpu cores, memory) and doesn't overload the nodes (by spawning more\nprocesses than there are cores or by spawning too many memory hungry jobs at once).\n\n\nGenerated Files\n-----------------------\nBesides your output files, a pipegraph creates some auxillary files:\n* ./.pypipegraph_status_robust - stores the invariant data of all jobs\n* ./logs/ppg_run.txt  - the chattery debug output of every decision the pipegraph makes (only if logs exists). All logging is also send to localhost 5005, and you can listen with util/log_listener.py\n* ./logs/ppg_errors.txt - a log of all failed jobs and their exception/stdout/stderr (only if logs exists and the file is writable)\n* ./logs/ppg_graph.txt - a dump of the connected graph structure (which job depends on which) (only if logs exists)\n\n\n\nNotes\n--------\n* A pipegraph and it's jobs can only be run once (but you can create multiple pipegraphs serially).\n* It is an error to create jobs before new_pipegraph() has been called.\n* Jobs magically associated with the currently existing pipegraph.\n* Invariant status is kept in a magic .pypipegraph_status file.\n* Jobs are singletonized on their id (within the existance of one pipegraph). \n  Little harm is done in defining a job multiple times. \n* Adding jobs gives you an iterable of jobs (which depends_on also takes). Adding a job and an iterable also gives you an iterable. So does adding an iterable and a job or an iterable and an iterable...\n* Executing jobs (all `Output jobs`_) have resource attributes: cores_needed (default 1, -1 means 'all you can get'), memory_needed (default = -1, means don't worry about it, just start one per core, assume memory / cores. If you specify something above memory/core it's treated as if you need (your_memory_specification / (memory/core)) cores). memory_needed is in bytes!\n* Beware of passing instance functions to FunctionInvariants - if the job creation code is done again for a different instance, it will raise an exception, because the bound function from before is not the same function you pass in now. Pass in class.function instead of self.function\n* pypipegraph is developed and tested on Ubuntu. It will not work reasonably on Windows - it's job model makes heavy use of fork() and windows process creating does not implicitly copy-on-write the current process' memory contents.\n\n\nPython function gotchas\n-------------------------\nPlease keep in mind that in python functions by default bind to the name of variables in their scope, no to their values.\nThis means that ::\n\n    for filename in ('A', 'B', 'C'):\n       def shu():\n           write_to_file(filename=filename, text='hello world')\n       job = pypipegraph.FileGeneratingJob(i, shu)\n\nwill not do what you want - you'll end up with three jobs, all writing to the same file (and the appropriate JobContractExceptions because two of them did not create their output files).\nWhat you need to do is rebind the variable::\n\n    for filename in ('A', 'B', 'C'):\n       def shu(filename=filename):  #that's the magic line. Also works for lambdas\n           write_to_file(filename=filename, text='hello world')\n       job = pypipegraph.FileGeneratingJob(i, shu)\n\n\nDevelopment notes\n------------------\n* We use nosetest for testing (nosetests test_pypipegraph.py), and create a subdirectory for each test to isolate test cases. \n* There usually are some test cases not yet implemented. These are expected to raise NotImplementedError()s.\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "http://code.google.com/p/pypipegraph/",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pypipegraph",
    "platform": "",
    "project_url": "https://pypi.org/project/pypipegraph/",
    "release_url": "https://pypi.org/project/pypipegraph/0.159/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "A workflow (job) engine/pipeline for bioinformatics and scientific computing.",
    "version": "0.159"
  },
  "releases": {
    "0.126": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b6ec42a60b898c6b065dac89f3fc13da",
          "sha256": "adc1564ae5957f397d5f9d486a8e5d513cbd5e32a5e514c13fe7d44c04766013"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.126.tar.gz",
        "has_sig": false,
        "md5_digest": "b6ec42a60b898c6b065dac89f3fc13da",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 50921,
        "upload_time": "2012-01-30T15:22:45",
        "url": "https://files.pythonhosted.org/packages/c1/ff/c6f771327f03ddc7e66783b01e71d483ae91d0fd763c3a0142c204288802/pypipegraph-0.126.tar.gz"
      }
    ],
    "0.128": [
      {
        "comment_text": "",
        "digests": {
          "md5": "b2a1ac16d9d8b0de0257063467cb3194",
          "sha256": "cf723ed49e290c2f068b709fb2c53eb9dfae45b0a584718b5ccccb2184128fb5"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.128.tar.gz",
        "has_sig": false,
        "md5_digest": "b2a1ac16d9d8b0de0257063467cb3194",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 47555,
        "upload_time": "2012-01-30T15:30:36",
        "url": "https://files.pythonhosted.org/packages/af/33/8fb04dcc9deb780fe82a0fb1b70a3e05e892944f98aa46032f9d65bb5c10/pypipegraph-0.128.tar.gz"
      }
    ],
    "0.151": [
      {
        "comment_text": "",
        "digests": {
          "md5": "76661b015c968290389480fd062bbe4f",
          "sha256": "c325f60d391ab243fa544fb04a7083ea9534015c46b3db91a8c4b0e0eafa55db"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.151.tar.gz",
        "has_sig": false,
        "md5_digest": "76661b015c968290389480fd062bbe4f",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 49714,
        "upload_time": "2012-04-12T14:38:50",
        "url": "https://files.pythonhosted.org/packages/dc/ab/c4788b666b5db3b082e29ed610fde2b58e234b1dd8b564e9b6a838e3dd45/pypipegraph-0.151.tar.gz"
      }
    ],
    "0.156": [
      {
        "comment_text": "",
        "digests": {
          "md5": "3381fef2b9552dbd4a83c21323586f77",
          "sha256": "e5610d74467181335e6448cabc51811d4614a472a1c4f5b798d85a49b55ab4ef"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.156.tar.gz",
        "has_sig": false,
        "md5_digest": "3381fef2b9552dbd4a83c21323586f77",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 53186,
        "upload_time": "2012-04-20T10:24:33",
        "url": "https://files.pythonhosted.org/packages/f3/17/9065ca8340ac9cc61b58442c3cee2c7e4bc8cfd727e94e09322fb54d8d70/pypipegraph-0.156.tar.gz"
      }
    ],
    "0.157": [
      {
        "comment_text": "",
        "digests": {
          "md5": "6ad9dccadd313eea6e3b04c7ff2ec333",
          "sha256": "b852718bd72c937fb8423b68741ab7233628ea53b04def0f90b1b133e9b95cfb"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.157.tar.gz",
        "has_sig": false,
        "md5_digest": "6ad9dccadd313eea6e3b04c7ff2ec333",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 50463,
        "upload_time": "2012-05-22T09:32:24",
        "url": "https://files.pythonhosted.org/packages/ad/95/848aa22f6de6260edb43e0a6527107b7c66a3f3d09e8ee7ca15352eb9657/pypipegraph-0.157.tar.gz"
      }
    ],
    "0.158": [
      {
        "comment_text": "",
        "digests": {
          "md5": "5906b5ca97865d0f715fd11d44ea66ff",
          "sha256": "ab49a9d2831b21441b983526601c799740b5b7faf17fcdcaca5f620a9d9a8849"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.158.tar.gz",
        "has_sig": false,
        "md5_digest": "5906b5ca97865d0f715fd11d44ea66ff",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 47421,
        "upload_time": "2017-11-22T13:40:10",
        "url": "https://files.pythonhosted.org/packages/f8/b6/99826689285bbcac6901f7abe6c88d82f34c3ea45a7dbf0abc29354f3c3b/pypipegraph-0.158.tar.gz"
      }
    ],
    "0.159": [
      {
        "comment_text": "",
        "digests": {
          "md5": "1a71b9d60c79fb2f52f78debaf0d3c29",
          "sha256": "7c9c0b583a20afb87a080e3fae90df198dcd8fc3bc1586ec612fc492702a16ef"
        },
        "downloads": -1,
        "filename": "pypipegraph-0.159.tar.gz",
        "has_sig": false,
        "md5_digest": "1a71b9d60c79fb2f52f78debaf0d3c29",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 50984,
        "upload_time": "2017-12-11T11:08:55",
        "url": "https://files.pythonhosted.org/packages/5f/da/5741f4b3499859fa09dead1895fb7447eeea2bc0a80e7630a308bf4e2671/pypipegraph-0.159.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "1a71b9d60c79fb2f52f78debaf0d3c29",
        "sha256": "7c9c0b583a20afb87a080e3fae90df198dcd8fc3bc1586ec612fc492702a16ef"
      },
      "downloads": -1,
      "filename": "pypipegraph-0.159.tar.gz",
      "has_sig": false,
      "md5_digest": "1a71b9d60c79fb2f52f78debaf0d3c29",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 50984,
      "upload_time": "2017-12-11T11:08:55",
      "url": "https://files.pythonhosted.org/packages/5f/da/5741f4b3499859fa09dead1895fb7447eeea2bc0a80e7630a308bf4e2671/pypipegraph-0.159.tar.gz"
    }
  ]
}