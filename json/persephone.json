{
  "info": {
    "author": "Oliver Adams",
    "author_email": "oliver.adams@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "# Persephone v0.1.8 (Beta version)\n\nPersephone (/p\u0259r\u02c8s\u025bf\u0259ni/) is an automatic phoneme transcription tool.\nTraditional speech recognition tools require a large pronunciation lexicon\n(describing how words are pronounced) and much training data so that the system\ncan learn to output orthographic transcriptions. In contrast, Persephone is\ndesigned for situations where training data is limited, perhaps as little as an\nhour of transcribed speech. Such limitations on data are common in the\ndocumentation of low-resource languages. It is possible to use such small\namounts of data to train a transcription model that can help aid transcription,\nyet such technology has not been widely adopted.\n\nThe goal of Persephone is to make state-of-the-art phonemic transcription\naccessible to people involved in language documentation. Creating an\neasy-to-use user interface is central to this. The user interface and APIs are a\nwork in progress and currently Persephone must be run via a command line.\n\nThe tool is implemented in Python/Tensorflow with extensibility in mind. Currently just one model is implemented, which uses bidirectional LSTMs and the connectionist temporal classification (CTC) loss function.\n\nWe are happy to offer direct help to anyone who wants to use it. If you're\nhaving trouble, contact Oliver Adams at oliver.adams@gmail.com. We are also\nvery welcome to thoughts, constructive criticism, help with design, development\nand documentation, along with any bug reports or pull requests you may have.\n\n#### Contributors\n\nPersephone has been built based on the code contributions of:\n* Oliver Adams\n* [Janis Lesinskis](https://www.customprogrammingsolutions.com/)\n* Ben Foley\n* Nay San\n\nIf you use this code in a publication, please cite the [workshop\npaper](https://halshs.archives-ouvertes.fr/halshs-01656683/document) (which is\ncurrently being refined into a conference paper):\n\n```\n@inproceedings{adams17alta,\ntitle = {Phonemic transcription of low-resource tonal languages},\nauthor = {Oliver Adams, Trevor Cohn, Graham Neubig, Alexis Michaud},\nbooktitle = {Australasian Language Technology Association Workshop 2017},\nmonth = {December},\nyear = {2017}\n}\n```\n\n## Quickstart\n\nThis guide is written to help you get the tool working on your machine. We will\nuse a example setup that involves training a phoneme transcription tool\nfor [Yongning Na](http://lacito.vjf.cnrs.fr/pangloss/languages/Na_en.php). For\nthis we use a small (even by language\ndocumentation standards) sub-sampling of elicited speech of\nYongning Na, a language of Southwestern China.\n\nThe example that we will run can be run on most personal computers without a\ngraphics processing unit (GPU), since I've made the settings less\ncomputationally demanding than it would be for optimal transcription quality.\nIdeally you'd have access to a server with more memory and a GPU, but this\nisn't necessary.\n\nThe code has been tested on Mac and Linux systems. It can be run on Windows using the Docker container described below.\n\nFor now you must open up a terminal to enter commands at the command line. (The\ncommands below are prefixed with a \"$\". Don't enter the \"$\", just whatever\ncomes afterwards).\n\n### 1. Installation\n\n#### Installation option 1: Using the Docker container\n\nTo simplify setup and system dependencies, a Docker container has been created.\nThis just requires [Docker to be installed](https://docs.docker.com/install/).\nOnce you have installed docker you can fetch our container with:\n\n```\n$ docker pull oadams/persephone\n```\n\nThen run it in interactive mode:\n\n```\n$ docker run -it oadams/persephone\n```\n\nThis will place you in an environment where Persephone and its\ndependencies have been installed, along with the example Na data.\n\n#### Installation option 2: A \"native\" install\n\nEnsure Python 3 is installed.\n\nYou will also need to install some system dependencies. For your convienence we\nhave an install script for dependencies for Ubuntu. To install the Ubuntu\nbinaries, run `./bootstrap_ubuntu.sh` to install ffmpeg packages. On MacOS we\nsuggest installing via Homebrew with `brew install ffmpeg`.\n\nWe now need to set up a virtual environment and install the library.\n\n```\n$ python3 -m virtualenv -p python3 persephone-venv\n$ source persephone-venv/bin/activate\n$ pip install -U pip\n$ pip install persephone\n```\n\n(This library can be installed system-wide but it is recommended to install in a virtualenv.)\n\nI've uploaded an example dataset that includes some Yongning Na data that has already been preprocessed. We'll use this example dataset in this tutorial. Once we confirm that the software itself is working on your computer, we can discuss preprocessing of your own data.\n\nCreate a working directory for storage of the data and running experiments:\n\n```\nmkdir persephone-tutorial/\ncd persephone-tutorial/\nmkdir data\n```\n\nGet the data [here](https://cloudstor.aarnet.edu.au/plus/s/YJXTLHkYvpG85kX/download)\n\nUnzip `na_example_small.zip`. There should now be a directory `na_example/`, with\nsubdirfectories `wav/` and `label/`. You can put `na_example` anywhere, but\nfor the rest of this tutorial I assume it is in the working directory: `persephone-tutorial/data/na_example/`.\n\n### 2. Training a toy Na model\n\nOne way to conduct experiments is to run the code from the iPython interpreter. Back to the terminal:\n\n```\n$ ipython\n> from persephone import corpus\n> corp = corpus.ReadyCorpus(\"data/na_example\")\n> from persephone import run\n> run.train_ready(corp)\n```\n\nYou'll should now see something like:\n\n```\nNumber of training utterances: 1024\nBatch size: 16\nBatches per epoch: 64\n2018-01-18 10:30:22.290964: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\nexp_dir ./exp/0, epoch 0\n\tBatch...0...1...2...3...\n```\n\nThe message may vary a bit depending on your CPU but if it says something like this then training is very likely working. Contact me if you have any trouble getting to this point, or if you had to deviate from the above instructions to get to this point.\n\nOn the current settings it will train through at least 10 \"epochs\", very likely more. If you don't have a GPU then this will take quite a while, though you should notice it converging in performance within a couple hours on most personal computers.\n\nAfter a few epochs you can see how its going by going to opening up\n`exp/<experiment_number>/train_log.txt`. This will show you\nthe error rates on the training set and the held-out validation set. In the\n`exp/<experiment_number>/decoded` subdirectory, you'll see the validation set reference in `refs` and the model hypotheses for each epoch in `epoch<epoch_num>_hyps`.\n\nCurrently the tool assumes each utterance is in its own audio file, and that for each utterance in the training set there is a corresponding transcription file with phonemes (or perhaps characters) delimited by spaces.\n\n### 3. Using your own data\n\nIf you have gotten this far, congratulations! You're now ready to start using\nyour own data. The example setup we created with the Na data illustrates a\ncouple key points, including how your data should be formatted, and how you\nmake the system read that data. In fact, if you format your data in the same\nway, you can create your own Persephone `Corpus` object with:\n\n```\ncorp = corpus.ReadyCorpus(\"<your-corpus-directory>\", label_type=\"extension\")\n```\nwhere extension is \"txt\", \"phonemes\", \"tones\", or whatever your file has after the dot.\n\nIf you are using the Docker container then to get data in and out of the container you need to create a \"volume\" that shares data between your computer (the host) and the container. If your data is stored in `/home/username/mydata` on your machine and in the container you want to store it in `/persephone/mydata` then run:\n```\ndocker run -it -v /home/username/mydata:/persephone/mydata oadams/persephone\n```\nThis is simply an extension of the earlier command to run docker, which additionally specifies the portal with which data is transferred to and from the container. If Persephone\u2014abducted by Hades\u2014is the queen of the underworld, then you might consider this volume to be the gates of hell.\n\n#### Formatting your data\n\nInterfacing with data is a key bottleneck in useability for speech recognition\nsystems. Providing a simple and flexible interface to your data is currently the\nmost important priority for Persephone at the moment. This is a work in\nprogress.\n\nCurrent data formatting requirements:\n* Audio files are stored in `<your-corpus>/wav/`. The WAV format is supported.\n  Persephone will automatically convert wavs to be 16bit mono 16000Hz.\n* Transcriptions are stored in text files in `<your-corpus>/label/`\n* Each audio file is short (ideally no longer than 10 seconds). There is a\n  script added by Ben Foley, `persephone/scripts/split_eafs.py`, to split\n  audio files into utterance-length units based on ELAN input files.\n* Each audio file in `wav/` has a corresponding transcription file in\n  `label/` with the same *prefix* (the bit of the filename before the\n  extension). For\n  example, if there is `wav/utterance_one.wav` then there should be\n  `label/utterance_one.<extension>`. `<extension>` can be whatever you want,\n  but it should describe how the labelling is done. For example, if it is\n  phonemic then `wav/utterance_one.phonemes` is a meaningful filename.\n* Each transcript file includes a space-delimited list of *labels* to\n  the model should learn to transcribe. For example:\n  * `data/na_example/label/crdo-NRU_F4_ACCOMP_PFV.0.phonemes` contains\n    `l e dz \u026f z e l e dz \u026f z e`\n  * `data/na_example/label/crdo-NRU_F4_ACCOMP_PFV.0.phonemes_and_tones`\n    might contain: `l e \u02e7 dz \u026f \u02e5 z e \u02e9 | l e \u02e7 dz \u026f \u02e5 z e \u02e9`\n* Persephone is agnostic to what your chosen labels are. It simply tries to\n  figure out how to map speech to that labelling. These labels can be\n  multiple characters long: the spaces demarcate labels. Labels can be any\n  unicode character(s).\n* Spaces are used to delimit the units that the tool predicts. Typically these\n  units are phonemes or tones, however they could also just be orthographic\n  characters (though performance is likely to be a bit lower: consider trying\n  to transcribe \"$100\"). The model can't tell the difference between digraphs\n  and unigraphs as long as they're tokenized in this format, demarcated with\n  spaces.\n\nIf your data observes this format then you can load it via the `ReadyCorpus` class.\nIf your data does not observe this format, you have two options:\n\n1. Do your own separate preprocessing to get the data in this format. If you're\nnot a programmer this is probably the best option for you. If you have ELAN\nfiles, this probably means using `persephone/scripts/split_eaf.py`.\n2. Create a Python class that inherits from `persephone.corpus.Corpus` (as does\n`ReadyCorpus`) and does all your preprocessing. The API (and thus\ndocumentation) for this is work in progress, but the key point is that\n`<corpusobject>.train_prefixes`, `<corpusobject>.valid_prefixes`, and\n`<corpusobject>.test_prefixes` are lists of prefixes for the relevant subset of\nthe data. For now, look at `ReadyCorpus` in `persephone/corpus.py` for an\nexample. For an example on a full dataset, see at `persephone/datasets/na.py`\n(beware: here be dragons).\n\n#### Creating validation and test sets\n\nCurrently `ReadyCorpus` splits the supplied data into three sets (training,\nvalidation and test) in a 95:5:5 ratio. The training set is what your model is\nexposed to during training. Validation is a held-out set that is used to gauge\nduring training how well the model is performing. Testing is what is used to\nquantitatively assess model performance after training is complete.\n\nWhen you first load your corpus, `ReadyCorpus` randomly allocates files to each\nof these subsets. If you'd like to do change the prefixes of which utterances\nare in in each set, modify `<your-corpus>/valid_prefixes.txt` and\n`<your-corpus>/test_prefixes.txt`. The training set consists of all the available\nutterances in neither of these text files.\n\n### 4. Miscellaneous Considerations\n\n#### On choosing an appropriate label granularity\n\nQuestion: Suprasegmentals like tone, glottalizzation, nasalization, and length are all\nphonemic in the language I am using. Do they belong in one grouping or\nseparately?\n\nAnswer: I'm wary of making sweeping claims about the best approach to handle all these\nsorts of phenomena that will realise themselves differently between languages,\nsince I'm neither a linguist nor do I have strong understanding for what\nfeatures the model will learn each situation. (Regarding tones, the literature\non this is also inconclusive in general). The best thing is to empirically test\nboth approaches:\n\n1. Having features as part of the phoneme token. For example, a nasalized /o/\nbecomes /\u00f5/.\n2. Having a separate token that follows the phoneme. For example, a high\ntone /o\u02e5/ becomes two tokens: /o \u02e5/.\n\nSince there are many ways you can mix and match these, one\nconsideration to keep in mind is how much larger the label vocabulary\nbecomes by merging two tokens into one. You don't want this\nvocabulary to become too big because then its harder to learn\nfeatures common to different tokens, and the model is less likely to\npick the right one even if it's on the right track. In the case of\nvowel nasalization, maybe you only double the number of vowels, so it\nmight be worth having merged tokens for that. If there are 5\ndifferent tones though, you might make that vowel vocabulary about 5\ntimes bigger by combining them into one token, so its less likely to\nbe good (though who knows, it might still yield performance\nimprovements).\n\n\n",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/oadams/persephone",
    "keywords": "speech-recognition machine-learning acoustic-models artificial-intelligence neural-networks",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "persephone",
    "platform": "",
    "project_url": "https://pypi.org/project/persephone/",
    "release_url": "https://pypi.org/project/persephone/0.1.8/",
    "requires_dist": [
      "pympi-ling (==1.69)",
      "scikit-learn (==0.19.1)",
      "tensorflow (==1.4.1)",
      "scipy (==1.0.0)",
      "python-speech-features (==0.6)",
      "numpy (==1.14.0)",
      "nltk (==3.2.5)",
      "GitPython (==2.1.8)",
      "ipython (==6.2.1)"
    ],
    "requires_python": "",
    "summary": "A tool for developing automatic phoneme transcription models",
    "version": "0.1.8"
  },
  "releases": {
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "md5": "f4ada57d8d9e41412f9d1c9b26a5264b",
          "sha256": "5c43b77253227ea2f99014c031d493170a1d7a1ad28b3aa8f4fcefb248cb66ea"
        },
        "downloads": -1,
        "filename": "persephone-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f4ada57d8d9e41412f9d1c9b26a5264b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 69912,
        "upload_time": "2018-02-09T10:21:06",
        "url": "https://files.pythonhosted.org/packages/39/82/ecb5ed51ad8dd125864aa3e0aa7c3b3ad276ccd4c2402fd06ed452459bb4/persephone-0.1.7-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "a4c20245d4c0c1ce53134c35fcdb8bd9",
          "sha256": "dd55a11e027930848c97dfedc85fbca209f142a4c103fd2d4ad980ce130d7e97"
        },
        "downloads": -1,
        "filename": "persephone-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "a4c20245d4c0c1ce53134c35fcdb8bd9",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 54926,
        "upload_time": "2018-02-09T10:21:09",
        "url": "https://files.pythonhosted.org/packages/55/86/a122a4b02f0df2fb140d0227bb8b362b5eb2655dbee7ee9679f83ea03861/persephone-0.1.7.tar.gz"
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "md5": "c48e39540a87c997de189a926605ba90",
          "sha256": "e0242287e807cc65c427449802a6ab92563c11a956a062d42f5434d692bb398b"
        },
        "downloads": -1,
        "filename": "persephone-0.1.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c48e39540a87c997de189a926605ba90",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 70033,
        "upload_time": "2018-02-10T06:27:57",
        "url": "https://files.pythonhosted.org/packages/f7/2c/89d403edd41f372dce63de9ab88b9a3cf2227ac2999a83514dd7db793485/persephone-0.1.8-py3-none-any.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "f7bc8a8c999710c3ee63a5cfacd32051",
          "sha256": "276ff1fcd18d758a44bfa9caffa928e7504dbb23d65d697e5dbf1c7e163f3dbb"
        },
        "downloads": -1,
        "filename": "persephone-0.1.8.tar.gz",
        "has_sig": false,
        "md5_digest": "f7bc8a8c999710c3ee63a5cfacd32051",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 55029,
        "upload_time": "2018-02-10T06:28:03",
        "url": "https://files.pythonhosted.org/packages/b6/b1/2897bdbcef93b6885bb3cc8f560df0d20a66f3bc72e1106be7aff5b7dec8/persephone-0.1.8.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "c48e39540a87c997de189a926605ba90",
        "sha256": "e0242287e807cc65c427449802a6ab92563c11a956a062d42f5434d692bb398b"
      },
      "downloads": -1,
      "filename": "persephone-0.1.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c48e39540a87c997de189a926605ba90",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "size": 70033,
      "upload_time": "2018-02-10T06:27:57",
      "url": "https://files.pythonhosted.org/packages/f7/2c/89d403edd41f372dce63de9ab88b9a3cf2227ac2999a83514dd7db793485/persephone-0.1.8-py3-none-any.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "f7bc8a8c999710c3ee63a5cfacd32051",
        "sha256": "276ff1fcd18d758a44bfa9caffa928e7504dbb23d65d697e5dbf1c7e163f3dbb"
      },
      "downloads": -1,
      "filename": "persephone-0.1.8.tar.gz",
      "has_sig": false,
      "md5_digest": "f7bc8a8c999710c3ee63a5cfacd32051",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 55029,
      "upload_time": "2018-02-10T06:28:03",
      "url": "https://files.pythonhosted.org/packages/b6/b1/2897bdbcef93b6885bb3cc8f560df0d20a66f3bc72e1106be7aff5b7dec8/persephone-0.1.8.tar.gz"
    }
  ]
}