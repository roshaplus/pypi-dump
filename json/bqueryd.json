{
  "info": {
    "author": "Carst Vaartjes",
    "author_email": "cvaartjes@visualfabriq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Information Technology",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# Bqueryd\n\nA companion library to [bquery](https://github.com/visualfabriq/bquery/) to make distributed bquery calls possible. Think of it as a far more rudimentary alternative to [Hadoop](http://hadoop.apache.org/) or [Dask](https://dask.pydata.org/en/latest/)\n\n## The Idea\n\nWeb applications or client analysis tools do not perform the heavy lifting of calculations over large sets of data themselves, the data is stored on a collection of other servers which respond to queries over the network. Data files that are used in computations are stored in [bcolz](http://bcolz.blosc.org/en/latest/) files.\nFor _really_ large datasets, the bcolz files can also be split up into 'shards' over several servers, and a query can then be performed over several servers and the results combined to the calling function by the bqueryd library.\n\n## Getting started\n\nMake sure you have Python virtualenv installed first.\nAs a start we need some interesting data, that is reasonably large in size. Download some Taxi data from the [NYC Taxi & Limousine Commission](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)\n\n    virtualenv bqueryd_getting_started\n    cd bqueryd_getting_started\n    . bin/activate\n    wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv\n    pip install bqueryd pandas\n\nWe are only downloading the data for one month, a more interesting test is of course download the data for an entire year. But this is a good start. The data for one month is already 10 million records.\n\nRun ipython, and let's convert the CSV file to a bcolz file.\n\n\n    import bcolz\n    import pandas as pd\n    data = pd.read_csv('yellow_tripdata_2016-01.csv',  parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n    ct = bcolz.ctable.fromdataframe(data, rootdir='tripdata_2016-01.bcolz')\n\nNow we have a bcolz file on disk that can be queried using [bquery](https://github.com/visualfabriq/bquery/). But we also want to show how to use the distributed functionality  of bqueryd, so we split the file that we have just created into some smaller chunks.\n\n    import bcolz, bquery\n    ct = bquery.open(rootdir='tripdata_2016-01.bcolz')\n    NR_SHARDS = 10\n    step = len(ct) / NR_SHARDS\n    remainder = len(ct) % NR_SHARDS\n    count = 0\n    for idx in range(0, len(ct), step):\n        if idx == len(ct)*(NR_SHARDS-1):\n            step = step + remainder\n        print 'Creating file tripdata_2016-01-%s.bcolzs'%count\n        ct_shard = bcolz.fromiter(\n            ct.iter(idx, idx+step),\n            ct.dtype,\n            step,\n            rootdir='tripdata_2016-01-%s.bcolzs'%count,\n            mode='w'\n        )\n        ct_shard.flush()\n        count += 1\n\n## Running bqueryd\n\nNow to test using bqueryd. If the bqueryd was successfully installed using pip, and your virtualenvironment is activated, you should now have a script named ```bqueryd``` on your path. You can start up a controller. Before starting bqueryd, also make sure that you have a locally running [Redis](https://redis.io/) server.\n\n    bqueryd controller &\n\nIf you already have a controller running, you can now also run ```bqueryd``` without any arguments and it will try and connect to your controller and then drop you into an [IPython](https://ipython.org/) shell to communicate with your bqueryd cluster.\n\n    bqueryd\n\nFrom the ipython prompt you have access to a variable named 'rpc'. (if you had at least one running controller). From the rpc variable you can execute commands to the bqueryd cluster. For example:\n\n    >>> rpc.info()\n\nWill show status information on your current cluster, with only one controller node running there is not so much info yet. First exist your ipython session to the shell.\nLets also start two worker nodes:\n\n    bqueryd worker --data_dir=`pwd` &\n    bqueryd worker --data_dir=`pwd` &\n\nAt this point you should have a controller and two workers running in the background. When you execute ```bqueryd``` again and do:\n\n    >>> rpc.info()\n\nThere should be more information on the running controller plus the two worker nodes. By default worker nodes check for bcolz files in the ```/srv/bcolz/``` directory. In the previous section we ran some worker nodes with a command line argument --data_dir to use the bcolz files in the current directory.\n\nSo what kind of other commands can we send to the nodes? Here are some things to try:\n\n    >>> rpc.ping()\n    >>> rpc.sleep(10)\n    >>> rpc.loglevel('debug')\n    >>> rpc.sleep(2)\n    >>> rpc.loglevel('info')\n    >>> rpc.killworkers()\n\nNotice the last command sent, this kills all the workers connected to all running controllers in the network. The controllers still keep on running. In typical setup the nodes will have been started up and kept running by a tool like [Supervisor](http://supervisord.org/) By using the 'killworkers' command it effectively re-boots all your workers.\n\nThe 'sleep' call is just for testing to see if any workers are responding. The call is not performed on the caller or the connecting node, but perfomeed by a worker chosen at random.\n\nIt is possible to stop all workers and all controllers in the bqueryd network by issuing the command:\n\n    >>> rpc.killall()\n\n\n## Configuration\n\nThere is minimally **one** thing to configure to use bqueryd on a network, assuming that all other defaults are chosen. **The address of the Redis server used**.\nThis is set in the file named ```/etc/bqueryd.cfg```\nYou could create this file and change the line to read, for example:\n\n    redis_url = redis://127.0.0.1:6379/0\n\nAnd change the IP address to the address of your running Redis instance. This needs to be done on every machine that you plan on running a bqueryd node.\n\nAs a convenience there is also en example configuration file for running a bquery installation using Supervisor in [misc/supervisor.conf](misc/supervisor.conf)\n\n## Doing calculations\n\nThe whole point of having a bqueryd cluster running is doing some calculations. So once you have a controller with some worker nodes running and connected, you can drop into the bqueryd ipython shell, and for example do:\n\n    >>> rpc.groupby(['tripdata_2016-01.bcolz'], ['payment_type'], ['fare_amount'], [])\n\nBut we can also use the sharded data to do the same calculation:\n\n    >>> import os\n    >>> bcolzs_files = [x for x in os.listdir('.') if x.endswith('.bcolzs')]\n    >>> rpc.groupby(bcolzs_files, ['payment_type'], [['fare_amount', 'sum', 'fare_amount']], [], aggregate=True)\n\nTo see how long a rpc call took, you can check:\n\n    >>> rpc.last_call_duration\n\nThe sharded version actually takes longer to run than the version using the bcolz file only. But if we start up more workers, the call is speeded up. For relatively small files like in this example, the speedup is small, but for larger datasets the overhead is worthwhile. Start up a few more workers, and run the query above.\n\n## Executing arbitrary code\n\nIt is possible to your bqueryd workers to import and execute arbitrary Python code. **This is a potentially huge big security risk if you do not run your nodes on trusted servers behind a good firewall** Make sure you know what you are doing before just starting up and running bqueryd nodes. With that being said, if you have a cluster running, try something like:\n\n    >>> rpc.execute_code(function='os.listdir', args=['.'], wait=True)\n\nThis should pick a random worker from those connected to the controller and run the Python listdir command in the args specified. The use of this is to run code to execute other functions from the built-in bquery/bcolz aggregation logic. This enables one to perform other business specific operations over the netwok using bqueryd nodes.\n\n## Distributing bcolz files\n\nIf your system is properly configured to use [boto](https://github.com/boto/boto3) for communication with Amazon Web Services, you can use bqueryd to atomically distribute collections of files to all nodes in the bqueryd cluster.\nCreate some bcolz files in the default bqueryd directory ```/srv/bcolz/``` (or move the ones we created in the getting started section of this readme)\nTo manage the download process, some other special types of workers need to be started.\n\n    bqueryd downloader &\n    bqueryd movebcolz &\n\nMake sure you have an Amazon S3 bucket that you have write access to from your boto installation. Then you can specify to use that bucket for downloads.\nThen from a bquery interactive shell you can distribute the files with:\n\n    >>> rpc.distribute(['tripdata_2016-01-1.bcolzs'], '<some-bucket-name>')\n\nThe first parameter is a list of filesnames to distribute. To download files that already exist in a bucket (they might have veen created there by some other process):\n\n    >>> rpc.download(filenames=['tripdata_2016-01-1.bcolzs', 'tripdata_2016-01-2.bcolzs'], bucket='<some-bucket-name>')\n\nIf you specify multiple files to download, or have several servers running a bquery node, the download will be co-ordinated across all servers and files. Only when all files are downloaded on all nodes are they switched into use by the calculation nodes. When distributing really large datafiles one would not some nodes to be out of sync serving new or old data out of step with other nodes.\n\n### Cancelling Downloads\n\nTo list what downloads are currently in progress, from the bquery shell do:\n\n    >>> rpc.downloads()\n\nThis will return a list of download tickets in progress, and the progress per ticket, eg:\n\n    [('bqueryd_download_ticket_e0f42ed5ef93e084', '0/1')]\n\nThe first entry is the ticket number, followed by the number of nodes on which the download is taking place plus the completed nodes.\nTo cancel a download:\n\n    >>>  rpc.delete_download(bqueryd_download_ticket_e0f42ed5ef93e084')",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/visualfabriq/bqueryd",
    "keywords": "",
    "license": "GPL2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bqueryd",
    "platform": "any",
    "project_url": "https://pypi.org/project/bqueryd/",
    "release_url": "https://pypi.org/project/bqueryd/0.2.1/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "A distribution framework for Bquery",
    "version": "0.2.1"
  },
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "d76c5a21e664a59b2cc50a6470a2cedf",
          "sha256": "3ccdeb97c87c3e5609c28f0f5352b47ef46dc33b83511794e53a90644019c457"
        },
        "downloads": -1,
        "filename": "bqueryd-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d76c5a21e664a59b2cc50a6470a2cedf",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 24444,
        "upload_time": "2018-01-19T16:04:13",
        "url": "https://files.pythonhosted.org/packages/2e/50/5381eefd1f147ea635179b100928150cf6c63614ad1b43a8402b3e37fa7f/bqueryd-0.2.0.tar.gz"
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "921b64b2860dcaafe5605034e58944c8",
          "sha256": "4abbdd2fce6489b1c9d165c47a491bc8b3b3b37a1616caefcd810101be8c084c"
        },
        "downloads": -1,
        "filename": "bqueryd-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "921b64b2860dcaafe5605034e58944c8",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 24485,
        "upload_time": "2018-01-19T16:08:29",
        "url": "https://files.pythonhosted.org/packages/4d/25/4cd3b4503918d743b28ee27b554e19ef4e4422389f82aa6e7c1b4d86bc7f/bqueryd-0.2.1.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "921b64b2860dcaafe5605034e58944c8",
        "sha256": "4abbdd2fce6489b1c9d165c47a491bc8b3b3b37a1616caefcd810101be8c084c"
      },
      "downloads": -1,
      "filename": "bqueryd-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "921b64b2860dcaafe5605034e58944c8",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 24485,
      "upload_time": "2018-01-19T16:08:29",
      "url": "https://files.pythonhosted.org/packages/4d/25/4cd3b4503918d743b28ee27b554e19ef4e4422389f82aa6e7c1b4d86bc7f/bqueryd-0.2.1.tar.gz"
    }
  ]
}