{
  "info": {
    "author": "Antti Puurula",
    "author_email": "antti.puurula@yahoo.com",
    "bugtrack_url": "",
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: GNU General Public License v2 (GPLv2)",
      "Programming Language :: Cython",
      "Programming Language :: Python :: 2.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "Wordbatch\n=========\n\nParallel text feature extraction for machine learning.\n\nWordbatch produces parallel feature extraction from raw text data for uses such as deep learning and text analytics. The most basic use for Wordbatch is as a drop-in replacement for the more basic non-parallelized extraction available in toolkits such as Scikit-learn, TfLearn, Keras, and Neon. Wordbatch additionally provides customizable preprocessing and feature extractors that improve predictive performance.\n\nUnlike text preprocessing in these toolkits that either deal with text as a single batch or as a stream, Wordbatch works best with large minibatches of text data. Wordbatch internally stores per-batch statistics of the data, and applies these for uses such as dictionary selection, spelling correction, and online IDF weighting. The larger the batches, the better choices Wordbatch can make in extracting features.\n\nThe current text preprocessing options include passing any function as text normalization to be parallelized, a constant-time adaptive version of Norvig spelling correction, and passing any function for parallel stemming.\n\nCurrently four basic feature extractor classes are provided:\n\n- WordHash is simply the Scikit-learn HashingVectorizer wrapped with the Wordbatch parallelization, providing multiplied processing speeds\n- WordBag is a flexible alternative to Wordhash, providing cababilities missing from Scikit-learn, such as IDF and per n-gram order weighting of hashed features, windowed and distance-weighted polynomial interactions, and more transforms for counts.\n- WordSeq provides sequences of word integers, as used by the deep learning toolkits for input into LSTM models.\n- WordVec provides embedding transforms from words into wordvectors\n\nA list of extractors can be defined. For example, word vector sequences can be projected into per-document vectors, and concatenated with the vectors from other word vector embeddings.\n\nStarting with 1.1, Wordbatch provides an OpenMP-parallelized version of the FTRL that has become the most popular algorithm for online learning of linear models in Kaggle competions. The implementation should be the fastest available version of FTRL.\n\nWordbatch is written with Cython, and uses concurrent threading, multiprocessing and OpenMP parallelization for circumventing the Python GIL. License is GNU GPL 2.0, and less restrictive licenses are available on request.\n\nRequirements\n============\nLinux. Python 2.7\n\nInstallation\n============\npip install wordbatch\n\nGetting started\n===============\n\n| #from sklearn.feature_extraction.text import HashingVectorizer\n| #from sklearn.linear_model import *\n| #vct= HashingVectorizer()\n| #clf= SGDRegressor()\n|\n| import wordbatch\n| from wordbatch.models import FTRL\n| vct= wordbatch.WordBatch(extractors=[(wordbatch.WordBag, {\"hash_ngrams\":2, \"hash_ngrams_weights\":[0.5, -1.0], \"hash_size\":2**23, \"norm\":'l2', \"tf\":'log', \"idf\":50.0})])\n| clf= FTRL(alpha=1.0, beta=1.0, L1=0.00001, L2=1.0, D=2 ** 25, iters=1)\n|\n| train_texts= [\"Cut down a tree with a herring? It can't be done.\", \"Don't say that word.\", \"How can we not say the word if you don't tell us what it is?\"]\n| train_labels= [1, 0, 1]\n| test_texts= [\"Wait! I said it! I said it! Ooh! I said it again!\"]\n|\n| clf.fit(vct.transform(train_texts), train_labels)\n| preds= clf.predict(vct.transform(test_texts))\n\n\nExample scripts\n===============\n\nThe directory /scripts/ contains four scripts for demonstrating the basic extractors, and a Scikit-learn ensemble model to combine predictions. To run the scripts you should first install the dependencies: Nervana Neon, NLTK, TextBlob and Pandas. The scripts also use the TripAdvisor dataset (http://times.cs.uiuc.edu/~wang296/Data/) for training models, and the precomputed word embeddings glove.twitter.27B.100d and glove.6B.50d (http://nlp.stanford.edu/projects/glove/). The test data from Crowdflower Open data & Kaggle is provided in the /data directory.\n\n- wordhash_regressor.py shows wordbatch.WordHash, and feature extraction concurrent with file reading\n- wordhash_regressor.py shows wordbatch.WordBag, and online feature extraction and parallel FTRL training\n- wordseq_regressor.py shows wordbatch.WordSeq, and training a Bi-LSTM regression model\n- wordvec_regressor.py shows wordbatch.WordVec, and combining word vector embeddings for FTRL training\n- classify_airline_sentiment.py show how to combine predictions from the four scripts using a Random Forest Regressor on the airline sentiment data\n\nSpark integration\n=================\nStarting from 1.2, Wordbatch has full spark integration. All processing steps will be parallelized by Spark, simply by setting wb.use_sc=True and providing data in the RDD format produced by wb.lists2rddbatches(texts). \n\nA basic script using this is wordbag_regressor_spark.py, which is the wordbag_regressor.py script modified to run on Spark. This converts each minibatch of training data into an RDD, does feature extraction on the RDD, and collects the resulting features for local FTRL model training. A more practical script should read the data from parallelized storage, and implement model training on the RDD as well.\n\nParallel prediction is also demonstrated in wordbag_regressor_spark.py. By calling the class with predict_parallel(), it will parallelize prediction either locally or on Spark, depending on whether a SparkContext has been set for the class.\n",
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/anttttti/Wordbatch",
    "keywords": null,
    "license": "GNU GPL 2.0",
    "maintainer": null,
    "maintainer_email": null,
    "name": "Wordbatch",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/Wordbatch/",
    "release_url": "https://pypi.org/project/Wordbatch/1.2.0b0/",
    "requires_python": null,
    "summary": "Parallel text feature extraction for machine learning",
    "version": "1.2.0b0"
  },
  "releases": {
    "1.2.0b0": [
      {
        "comment_text": "",
        "digests": {
          "md5": "3bd710a64907498422f0a97f4e4a4ef3",
          "sha256": "a556b4418ac3f40cf6664019e88d53314fea1325755b87c23751b731eca9688a"
        },
        "downloads": 18,
        "filename": "Wordbatch-1.2.0b0.tar.gz",
        "has_sig": false,
        "md5_digest": "3bd710a64907498422f0a97f4e4a4ef3",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 1420132,
        "upload_time": "2017-01-11T09:44:42",
        "url": "https://files.pythonhosted.org/packages/1c/82/f6abc4612bc40b5bfccc14a8b9b73a5d2b8af0bc20cdfc41c6bc17d41e09/Wordbatch-1.2.0b0.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "3bd710a64907498422f0a97f4e4a4ef3",
        "sha256": "a556b4418ac3f40cf6664019e88d53314fea1325755b87c23751b731eca9688a"
      },
      "downloads": 18,
      "filename": "Wordbatch-1.2.0b0.tar.gz",
      "has_sig": false,
      "md5_digest": "3bd710a64907498422f0a97f4e4a4ef3",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 1420132,
      "upload_time": "2017-01-11T09:44:42",
      "url": "https://files.pythonhosted.org/packages/1c/82/f6abc4612bc40b5bfccc14a8b9b73a5d2b8af0bc20cdfc41c6bc17d41e09/Wordbatch-1.2.0b0.tar.gz"
    }
  ]
}