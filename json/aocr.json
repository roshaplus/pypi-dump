{
  "info": {
    "author": "Ed Medvedev",
    "author_email": "edward.medvedev@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "Attention-OCR\n=============\n\nAuthors: `Qi Guo <http://qiguo.ml>`__ and `Yuntian\nDeng <https://github.com/da03>`__\n\nVisual Attention based OCR. The model first runs a sliding CNN on the\nimage (images are resized to height 32 while preserving aspect ratio).\nThen an LSTM is stacked on top of the CNN. Finally, an attention model\nis used as a decoder for producing the final outputs.\n\n.. figure:: http://cs.cmu.edu/~yuntiand/OCR-2.jpg\n   :alt: example image 0\n\n   example image 0\n\nPrerequsites\n------------\n\nMost of our code is written based on Tensorflow 1.2. Besides, we use\npython package ``distance`` to calculate edit distance for evaluation.\n(However, that is not mandatory, if distance is not installed, we will\ndo exact match).\n\nUsage\n-----\n\nNote: We assume that the working directory is ``Attention-OCR``.\n\nTrain\n~~~~~\n\nData Preparation\n^^^^^^^^^^^^^^^^\n\nWe need a file (specified by parameter ``data-path``) containing the\npath of images and the corresponding characters, e.g.:\n\n::\n\n    path/to/image1 abc\n    path/to/image2 def\n\nAnd we also need to specify a ``data-base-dir`` parameter such that we\nread the images from path ``data-base-dir/path/to/image``. If\n``data-path`` contains absolute path of images, then ``data-base-dir``\nneeds to be set to ``/``.\n\nA Toy Example\n^^^^^^^^^^^^^\n\nFor a toy example, we have prepared a training dataset of the specified\nformat, which is a subset of `Synth\n90k <http://www.robots.ox.ac.uk/~vgg/data/text/>`__\n\n::\n\n    wget http://www.cs.cmu.edu/~yuntiand/sample.tgz\n\n::\n\n    tar zxf sample.tgz\n\n::\n\n    python src/launcher.py --phase=train --data-path=sample/sample.txt --data-base-dir=sample --log-path=log.txt --no-load-model\n\nAfter a while, you will see something like the following output in\n``log.txt``:\n\n::\n\n    ...\n    2016-06-08 20:47:22,335 root  INFO     Created model with fresh parameters.\n    2016-06-08 20:47:52,852 root  INFO     current_step: 0\n    2016-06-08 20:48:01,253 root  INFO     step_time: 8.400597, step perplexity: 38.998714\n    2016-06-08 20:48:01,385 root  INFO     current_step: 1\n    2016-06-08 20:48:07,166 root  INFO     step_time: 5.781749, step perplexity: 38.998445\n    2016-06-08 20:48:07,337 root  INFO     current_step: 2\n    2016-06-08 20:48:12,322 root  INFO     step_time: 4.984972, step perplexity: 39.006730\n    2016-06-08 20:48:12,347 root  INFO     current_step: 3\n    2016-06-08 20:48:16,821 root  INFO     step_time: 4.473902, step perplexity: 39.000267\n    2016-06-08 20:48:16,859 root  INFO     current_step: 4\n    2016-06-08 20:48:21,452 root  INFO     step_time: 4.593249, step perplexity: 39.009864\n    2016-06-08 20:48:21,530 root  INFO     current_step: 5\n    2016-06-08 20:48:25,878 root  INFO     step_time: 4.348195, step perplexity: 38.987707\n    2016-06-08 20:48:26,016 root  INFO     current_step: 6\n    2016-06-08 20:48:30,851 root  INFO     step_time: 4.835423, step perplexity: 39.022887\n\nNote that it takes quite a long time to reach convergence, since we are\ntraining the CNN and attention model simultaneously.\n\nTest and visualize attention results\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe test data format shall be the same as training data format. We have\nalso prepared a test dataset of the specified format, which includes\nICDAR03, ICDAR13, IIIT5k and SVT.\n\n::\n\n    wget http://www.cs.cmu.edu/~yuntiand/evaluation_data.tgz\n\n::\n\n    tar zxf evaluation_data.tgz\n\nWe also provide a trained model on Synth 90K:\n\n::\n\n    wget http://www.cs.cmu.edu/~yuntiand/model.tgz\n\n::\n\n    tar zxf model.tgz\n\n::\n\n    python src/launcher.py --phase=test --visualize --data-path=evaluation_data/svt/test.txt --data-base-dir=evaluation_data/svt --log-path=log.txt --load-model --model-dir=model --output-dir=results\n\nAfter a while, you will see something like the following output in\n``log.txt``:\n\n::\n\n    2016-06-08 22:36:31,638 root  INFO     Reading model parameters from model/translate.ckpt-47200\n    2016-06-08 22:36:40,529 root  INFO     Compare word based on edit distance.\n    2016-06-08 22:36:41,652 root  INFO     step_time: 1.119277, step perplexity: 1.056626\n    2016-06-08 22:36:41,660 root  INFO     1.000000 out of 1 correct\n    2016-06-08 22:36:42,358 root  INFO     step_time: 0.696687, step perplexity: 2.003350\n    2016-06-08 22:36:42,363 root  INFO     1.666667 out of 2 correct\n    2016-06-08 22:36:42,831 root  INFO     step_time: 0.466550, step perplexity: 1.501963\n    2016-06-08 22:36:42,835 root  INFO     2.466667 out of 3 correct\n    2016-06-08 22:36:43,402 root  INFO     step_time: 0.562091, step perplexity: 1.269991\n    2016-06-08 22:36:43,418 root  INFO     3.366667 out of 4 correct\n    2016-06-08 22:36:43,897 root  INFO     step_time: 0.477545, step perplexity: 1.072437\n    2016-06-08 22:36:43,905 root  INFO     4.366667 out of 5 correct\n    2016-06-08 22:36:44,107 root  INFO     step_time: 0.195361, step perplexity: 2.071796\n    2016-06-08 22:36:44,127 root  INFO     5.144444 out of 6 correct\n\nExample output images in ``results/correct`` (the output directory is\nset via parameter ``output-dir`` and the default is ``results``): (Look\ncloser to see it clearly.)\n\nFormat: Image ``index`` (``predicted``/``ground truth``) ``Image file``\n\nImage 0 (j/j): |example image 0|\n\nImage 1 (u/u): |example image 1|\n\nImage 2 (n/n): |example image 2|\n\nImage 3 (g/g): |example image 3|\n\nImage 4 (l/l): |example image 4|\n\nImage 5 (e/e): |example image 5|\n\nParameters\n----------\n\n-  Control\n\n   -  ``phase``: Determine whether to train or test.\n   -  ``visualize``: Valid if ``phase`` is set to test. Output the\n      attention maps on the original image.\n   -  ``load-model``: Load model from ``model-dir`` or not.\n\n-  Input and output\n\n   -  ``data-base-dir``: The base directory of the image path in\n      ``data-path``. If the image path in ``data-path`` is absolute\n      path, set it to ``/``.\n   -  ``data-path``: The path containing data file names and labels.\n      Format per line: ``image_path characters``.\n   -  ``model-dir``: The directory for saving and loading model\n      parameters (structure is not stored).\n   -  ``log-path``: The path to put log.\n   -  ``output-dir``: The path to put visualization results if\n      ``visualize`` is set to True.\n   -  ``steps-per-checkpoint``: Checkpointing (print perplexity, save\n      model) per how many steps\n\n-  Optimization\n\n   -  ``num-epoch``: The number of whole data passes.\n   -  ``batch-size``: Batch size. Only valid if ``phase`` is set to\n      train.\n   -  ``initial-learning-rate``: Initial learning rate, note the we use\n      AdaDelta, so the initial value doe not matter much.\n\n-  Network\n\n   -  ``target-embedding-size``: Embedding dimension for each target.\n   -  ``attn-use-lstm``: Whether or not use LSTM attention decoder cell.\n   -  ``attn-num-hidden``: Number of hidden units in attention decoder\n      cell.\n   -  ``attn-num-layers``: Number of layers in attention decoder cell.\n      (Encoder number of hidden units will be\n      ``attn-num-hidden``\\ \\*\\ ``attn-num-layers``).\n   -  ``target-vocab-size``: Target vocabulary size. Default is =\n      26+10+3 # 0: PADDING, 1: GO, 2: EOS, >2: 0-9, a-z\n\nReferences\n----------\n\n`Convert a formula to its LaTex\nsource <https://github.com/harvardnlp/im2markup>`__\n\n`What You Get Is What You See: A Visual Markup\nDecompiler <https://arxiv.org/pdf/1609.04938.pdf>`__\n\n`Torch attention OCR <https://github.com/da03/torch-Attention-OCR>`__\n\n.. |example image 0| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_0.jpg\n.. |example image 1| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_1.jpg\n.. |example image 2| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_2.jpg\n.. |example image 3| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_3.jpg\n.. |example image 4| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_4.jpg\n.. |example image 5| image:: http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_5.jpg\n\n",
    "docs_url": null,
    "download_url": "https://github.com/emedvedev/attention-ocr/archive/0.0.2.tar.gz",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://github.com/emedvedev/attention-ocr",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aocr",
    "platform": "",
    "project_url": "https://pypi.org/project/aocr/",
    "release_url": "https://pypi.org/project/aocr/0.0.2/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "Optical character recognition model for Tensorflow based on Visual Attention.",
    "version": "0.0.2"
  },
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "7d832dbb0caa5aae9e4f8ba15bf2361d",
          "sha256": "0fefb96c65bc3f6ca2524005c122740c52e919036354f3608e0be4571bfbd0c6"
        },
        "downloads": 0,
        "filename": "aocr-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7d832dbb0caa5aae9e4f8ba15bf2361d",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 28909,
        "upload_time": "2017-07-21T19:21:16",
        "url": "https://files.pythonhosted.org/packages/c1/ea/3c58d216ab6e7cfcd9c3882ac577d8f6040506271d1222dcee4c45929e26/aocr-0.0.1.tar.gz"
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "5c56df2353d9877d71f077c504ee92b2",
          "sha256": "6fa5cfd3fa170aba44b34fc02d8058e92d84f440caa34981fe049e068d8099af"
        },
        "downloads": 0,
        "filename": "aocr-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5c56df2353d9877d71f077c504ee92b2",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 28669,
        "upload_time": "2017-07-21T19:36:31",
        "url": "https://files.pythonhosted.org/packages/df/fd/cc669956579e6d4d7cb9e5fe046f04c0d93455dd276d091aac762059adf1/aocr-0.0.2.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "5c56df2353d9877d71f077c504ee92b2",
        "sha256": "6fa5cfd3fa170aba44b34fc02d8058e92d84f440caa34981fe049e068d8099af"
      },
      "downloads": 0,
      "filename": "aocr-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "5c56df2353d9877d71f077c504ee92b2",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 28669,
      "upload_time": "2017-07-21T19:36:31",
      "url": "https://files.pythonhosted.org/packages/df/fd/cc669956579e6d4d7cb9e5fe046f04c0d93455dd276d091aac762059adf1/aocr-0.0.2.tar.gz"
    }
  ]
}