{
  "info": {
    "author": "Hans Roh",
    "author_email": "hansroh@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: Text Processing :: Indexing"
    ],
    "description": "======================\nDeLune: Search Engine\n======================\n\n.. contents:: Table of Contents\n\n\nIntroduce\n============\n\nDeLune (former Wissen) Search Engine is a simple search engine mostly written in Python and C in year 2008.\n\nAt that time, I would like to study Lucene_ earlier version with Lupy_ and CLucene_. And I also had maden my own search engine for excercise.\n\nIts file format, numeric compressing algorithm, indexing process are quiet similar with Lucene. But querying and result-fetching parts was built from my imagination. As a result it's entirely unorthodox and possibly inefficient. DeLune's searching mechanism is similar with DNA-RNA-Protein working model translated into 'Index File-Temporary Small Replication Buffer-Query Result'.\n\n* Every searcher (Cell) has a single index file handlers group (DNA group in nuclear)\n* Thread has multiple small buffer (RNA) for replicating index as needed part\n* Query class (Ribosome) creates query result (Protein) by synthesising buffers' inforamtion (RNAs)\n* Repeat from 2nd if expected more results\n\n.. _Lucene: https://lucene.apache.org/core/\n.. _Lupy: https://pypi.python.org/pypi/Lupy\n.. _CLucene: http://clucene.sourceforge.net/\n\n\nInstallation\n=============\n\nDeLune contains C extension, so need C compiler.\n\n.. code:: bash\n\n  pip install delune\n\nOn posix, it might be required some packages,\n\n.. code:: bash\n\n  apt-get install gcc zlib1g-dev\n\n\nQuick Start\n============\n\nAll field text type should be str type, otherwise encoding should be specified.\n\nIndexing and Searching\n-------------------------\n\nHere's an example indexing only one document.\n\n.. code:: python\n\n  import delune\n\n  # indexing\n  analyzer = delune.standard_analyzer (max_term = 3000)\n  col = delune.collection (\"./col\", delune.CREATE, analyzer)\n  indexer = col.get_indexer ()\n\n  song = \"violin sonata in c k.301\"\n  composer = u\"wolfgang amadeus mozart\"\n  birth = 1756\n  home = \"50.665629/8.048906\" # Lattitude / Longitude of Salzurg\n  genre = \"01011111\" # (rock serenade jazz piano symphony opera quartet sonata)\n\n  document = delune.document ()\n\n  # object to return, any object serializable by pickle\n  document.content ([song, composer])\n\n  # text content to generating auto snippet by given query terms\n  document.snippet (song)\n\n  # add searchable fields\n  document.field (\"default\", song, delune.TEXT)\n  document.field (\"composer\", composer, delune.TEXT)\n  document.field (\"birth\", birth, delune.INT16)\n  document.field (\"genre\", genre, delune.BIT8)\n  document.field (\"home\", home, delune.COORD)\n\n  indexer.add_document (document)\n  indexer.close ()\n\n  # searching\n  analyzer = delune.standard_analyzer (max_term = 8)\n  col = delune.collection (\"./col\", delune.READ, analyzer)\n  searcher = col.get_searcher ()\n  print searcher.query (u'violin', offset = 0, fetch = 2, sort = \"tfidf\", summary = 30)\n  searcher.close ()\n\n\nResult will be like this:\n\n.. code:: python\n\n  {\n   'code': 200, \n   'time': 0, \n   'total': 1\n   'result': [\n    [\n     ['violin sonata in c k.301', 'wofgang amadeus mozart'], # content\n     '<b>violin</b> sonata in c k.301', # auto snippet\n     14, 0, 0, 0 # additional info\n    ]\n   ],   \n   'sorted': [None, 0], \n   'regex': 'violin|violins',   \n  }\n\nDeLune's document can be any Python objects pickalbe, delune stored document zipped pickled format. But you want to fetch partial documents by key or index, document skeleton shoud be a list or dictionary, but still inner data type can be any picklable objects. I think if your data need much more reading operations than writngs/updatings, DeLune can be as both simple schemaless data storage and fulltext search engine. DeLune's RESTful API and replication is end of this document.\n\nLearning and Classification\n---------------------------\n\nHere's an example guessing one of 'play golf', 'go to bed' by weather conditions.\n\n.. code:: python\n\n   import delune\n\n   analyzer = delune.standard_analyzer (max_term = 3000)\n\n   # learning\n\n   mdl = delune.model (\"./mdl\", delune.CREATE, analyzer)\n   learner = mdl.get_learner ()\n\n   document = delune.labeled_document (\"Play Golf\", \"cloudy windy warm\")\n   learner.add_document (document)  \n   document = delune.labeled_document (\"Play Golf\", \"windy sunny warm\")\n   learner.add_document (document)  \n   document = delune.labeled_document (\"Go To Bed\", \"cold rainy\")\n   learner.add_document (document)  \n   document = delune.labeled_document (\"Go To Bed\", \"windy rainy warm\")\n   learner.add_document (document)   \n   learner.close ()\n\n   mdl = delune.model (\"./mdl\", delune.MODIFY, analyzer)\n   learner = mdl.get_learner ()\n   learner.listbydf () # show all terms with DF (Document Frequency)\n   learner.close ()\n\n   mdl = delune.model (\"./mdl\", delune.MODIFY, analyzer)\n   learner = mdl.get_learner ()\n   learner.build (dfmin = 2) # build corpus DF >= 2\n   learner.close ()\n\n   mdl = delune.model (\"./mdl\", delune.MODIFY, analyzer)\n   learner = mdl.get_learner ()\n   learner.train (\n     cl_for = delune.ALL, # for which classifier\n     selector = delune.CHI2, # feature selecting method\n     select = 0.99, # how many features?\n     orderby = delune.MAX, # feature ranking by what?\n     dfmin = 2 # exclude DF < 2\n   )\n   learner.close ()\n\n\n   # gusessing\n\n   mdl = delune.model (\"./mdl\", delune.READ, analyzer)\n   classifier = mdl.get_classifier ()\n   print classifier.guess (\"rainy cold\", cl = delune.NAIVEBAYES)\n   print classifier.guess (\"rainy cold\", cl = delune.FEATUREVOTE)\n   print classifier.guess (\"rainy cold\", cl = delune.TFIDF)\n   print classifier.guess (\"rainy cold\", cl = delune.SIMILARITY)\n   print classifier.guess (\"rainy cold\", cl = delune.ROCCHIO)\n   print classifier.guess (\"rainy cold\", cl = delune.MULTIPATH)\n   print classifier.guess (\"rainy cold\", cl = delune.META)\n   classifier.close ()\n\n\nResult will be like this:\n\n.. code:: python\n\n  {\n    'code': 200, \n    'total': 1, \n    'time': 5,\n    'result': [('Go To Bed', 1.0)],\n    'classifier': 'meta'  \n  }\n\n\nLimitation\n==============\n\nBefore you test DeLune, you should know some limitation.\n\n- DeLune search cannot sort by string type field, but can by int/bit/coord types and TFIDF ranking. \n\n- DeLune classification doesn't have purpose for accuracy but realtime (means within 1 second) guessing performance. So I used relatvely simple and fast classification algorithms. If you need accuracy, it's not fit to you.\n\n\nConfigure DeLune\n==================\n\nWhen indexing/learing it's not necessory to configure, but searching/guessing it should be configure. The reason why DeLune allocates memory per thread for searching and classifying on initializing.\n\n.. code:: python\n\n  delune.configure (\n    numthread, \n    logger, \n    io_buf_size = 4096, \n    mem_limit = 256\n  )\n\n\n- numthread: number of threads which access to DeLune collections and models. if set to 8, you can open multiple collections (or models) and access with 8 threads. If 9th thread try to access to delune, it will raise error\n\n- logger: *see next chapter*\n\n- io_buf_size = 4096: Bytes size of flash buffer for repliacting index files\n\n- mem_limit = 256: Memory limit per a thread, but it's not absolute. It can be over during calculation if need, but when calcuation has been finished, would return memory ASAP.\n\n\nFinally when your app is terminated, call shutdown.\n\n.. code:: python\n\n  delune.shutdown ()\n\n\nLogger\n========\n\n.. code:: python\n\n  from delune.lib import logger\n\n  logger.screen_logger ()\n\n  # it will create file '/var/log.delune.log', and rotated by daily base\n  logger.rotate_logger (\"/var/log\", \"delune\", \"daily\")\n\n\nStandard Analyzer\n====================\n\nAnalyzer is needed by TEXT, TERM types.\n\nBasic Usage is:\n\n.. code:: python\n\n  analyzer = delune.standard_analyzer (\n    max_term = 8, \n    numthread = 1,\n    ngram = True or False,\n    stem_level = 0, 1 or 2 (2 is only applied to English Language),\n    make_lower_case = True or False,\n    stopwords_case_sensitive = True or False,\n    ngram_no_space = True or False,\n    strip_html = True or False,  \n    contains_alpha_only = True or False,  \n    stopwords = [word,...]\n  )\n\n- stem_level: 0 and 1, especially 'en' language has level 2 for hard stemming\n\n- make_lower_case: make lower case for every text\n\n- stopwords_case_sensitive: it will work if make_lower_case is False\n\n- ngram_no_space: if False, '\u6ce3\u65ac \u99ac\u8b16' will be tokenized to _\u6ce3, \u6ce3\u65ac, \u65ac\\_, _\u99ac, \u99ac\u8b16, \u8b16\\_. But if True, addtional bi-gram \u65ac\u99ac will be created between \u65ac\\_ and _\u99ac.\n\n- strip_html\n\n- contains_alpha_only: remove term which doesn't contain alphabet, this option is useful for full-text training in some cases\n\n- stopwords: DeLune has only English stopwords list, You can use change custom stopwords. Stopwords sould be unicode or utf8 encoded bytes\n\nDeLune has some kind of stemmers and n-gram methods for international languages and can use them by this way:\n\n.. code:: python\n\n  analyzer = standard_analyzer (ngram = True, stem_level = 1)\n  col = delune.collection (\"./col\", delune.CREATE, analyzer)\n  indexer = col.get_indexer ()\n  document.field (\"default\", song, delune.TEXT, lang = \"en\")\n\n\nImplemented Stemmers\n---------------------\n\nExcept English stemmer, all stemmers can be obtained at `IR Multilingual Resources at UniNE`__.\n\n  - ar: Arabic\n  - de: German\n  - en: English\n  - es: Spanish\n  - fi: Finnish\n  - fr: French\n  - hu: Hungarian\n  - it: Italian\n  - pt: Portuguese\n  - sv: Swedish\n\n.. __: http://members.unine.ch/jacques.savoy/clef/index.html\n\n\nBi-Gram Index\n----------------\n\nIf ngram is set to True, these languages will be indexed with bi-gram.\n\n  - cn: Chinese\n  - ja: Japanese\n  - ko: Korean\n\nAlso note that if word contains only alphabet, will be used English stemmer.\n\n\nTri-Gram Index\n---------------\n\nThe other languages will be used English stemmer if all spell is Alphabet. And if ngram is set to True, will be indexed with tri-gram if word has multibytes.\n\n**Methods Spec**\n\n  - analyzer.index (document, lang)\n  - analyzer.freq (document, lang)\n  - analyzer.stem (document, lang)\n  - analyzer.count_stopwords (document, lang)\n\n\nCollection\n==================\n\nCollection manages index files, segments and properties.\n\n.. code:: python\n\n  col = delune.collection (\n    indexdir = [dirs], \n    mode = [ CREATE | READ | APPEND ], \n    analyzer = None,\n    logger = None \n  )\n\n- indexdir: path or list of path for using multiple disks efficiently\n- mode\n- analyzer\n- logger: # if logger configured by delune.configure, it's not necessary\n\nCollection has 2 major class: indexer and searcher.\n\n\n\nIndexer\n---------\n\nFor searching documents, it's necessary to indexing text to build Inverted Index for fast term query. \n\n.. code:: python\n\n  indexer = col.get_indexer (\n    max_segments = int,\n    force_merge = True or False,\n    max_memory = 10000000 (10Mb),\n    optimize = True or False\n  )\n\n- max_segments: maximum number of segments of index, if it's over, segments will be merged. also note during indexing, segments will be created 3 times of max_segments and when called index.close (), automatically try to merge until segemtns is proper numbers\n\n- force_merge: When called index.close (), forcely try to merge to a single segment. But it's failed if too big index - on 32bit OS > 2GB, 64bit > 10 GB\n\n- max_memory: if it's over, created new segment on indexing\n\n- optimize: When called index.close (), segments will be merged by optimal number as possible\n\n\nFor add docuemtn to indexer, create document object:\n\n.. code:: python\n\n  document = delune.document ()     \n\nDeLune handle 3 objects as completly different objects between no relationship\n\n- returning content\n- snippet generating field\n- searcherble fields\n\n\n**Returning Content**\n\nDeLune serialize returning contents by pickle, so you can set any objects pickle serializable.\n\n.. code:: python\n\n  document.content ({\"userid\": \"hansroh\", \"preference\": {\"notification\": \"email\", ...}})\n\n  or \n\n  document.content ([32768, \"This is smaple ...\"])\n\n\n**Snippet Generating Field**  \n\nThis field should be unicode/utf8 encoded bytes.\n\n.. code:: python\n\n  document.snippet (\"This is sample...\")\n\n\n**Searchable Fields**\n\ndocument also recieve searchable fields:\n\n.. code:: python\n\n  document.field (name, value, ftype = delune.TEXT, lang = \"un\", encoding = None)\n\n  document.field (\"default\", \"violin sonata in c k.301\", delune.TEXT, \"en\")\n  document.field (\"composer\", \"wolfgang amadeus mozart\", delune.TEXT, \"en\")\n  document.field (\"lastname\", \"mozart\", delune.STRING)\n  document.field (\"birth\", 1756, delune.INT16)\n  document.field (\"genre\", \"01011111\", delune.BIT8)\n  document.field (\"home\", \"50.665629/8.048906\", delune.COORD6)\n\n\n- name: if 'default', this field will be searched by simple string, or use 'name:query_text'\n- value: unicode/utf8 encode text, or should give encoding arg.\n- ftype: *see below*\n- encoding: give like 'iso8859-1' if value is not unicode/utf8\n- lang: language code for standard_analyzer, \"un\" (unknown) is default\n\nAvalible Field types are:\n\n  - TEXT: analyzable full-text, result-not-sortable\n\n  - TERM: analyzable full-text but position data will not be indexed as result can't search phrase, result-not-sortable\n\n  - STRING: exactly string match like nation codes, result-not-sortable\n\n  - LIST: comma seperated STRING, result-not-sortable\n\n  - COORDn, n=4,6,8 decimal precision: comma seperated string 'latitude,longititude', latitude and longititude sould be float type range -90 ~ 90, -180 ~ 180. n is precision of coordinates. n=4 is 10m radius precision, 6 is 1m and 8 is 10cm. result-sortable\n\n  - BITn, n=8,16,24,32,40,48,56,64: bitwise operation, bit makred string required by n, result-sortable\n\n  - INTn, n=8,16,24,32,40,48,56,64: range, int required, result-sortable\n\n\nRepeat add_document as you need and close indexer.\n\n.. code:: python\n\n  for ...:  \n    document = delune.document ()\n    ...\n    indexer.add_document (document) \n    indexer.close ()  \n\nIf searchers using this collection runs with another process or thread, searcher automatically reloaded within a few seconds for applying changed index.\n\n\nSearcher\n---------\n\nFor running searcher, you should delune.configure () first and creat searcher.\n\n.. code:: python\n\n  searcher = col.get_searcher (\n    max_result = 2000,\n    num_query_cache = 200\n  ) \n\n- max_result: max returned number of searching results. default 2000, if set to 0, unlimited results\n\n- num_query_cache: default is 200, if over 200, removed by access time old\n\n\nQuery is simple:\n\n.. code:: python\n\n  searcher.query (\n    qs, \n    offset = 0, \n    fetch = 10, \n    sort = \"tfidf\", \n    summary = 30, \n    lang = \"un\"\n  )\n\n- qs: string (unicode) or utf8 encoded bytes. for detail query syntax, see below\n- offset: return start position of result records\n- fetch: number of records from offset\n- sort: \"(+-)tfidf\" or \"(+-)field name\", field name should be int/bit type, and '-' means descending (high score/value first) and default if not specified. if sort is \"\", records order is reversed indexing order\n- summary: number of terms for snippet\n- lang: default is \"un\" (unknown)\n\n\nFor deleting indexed document:\n\n.. code:: python\n\n  searcher.delete (qs)\n\nAll documents will be deleted immediatly. And if searchers using this collection run with another process or thread, theses searchers automatically reloaded within a few seconds.\n\nFinally, close searcher.\n\n.. code:: python\n\n  searcher.close ()\n\n\n**Query Syntax**\n\n  - violin composer:mozart birth:1700~1800 \n\n    search 'violin' in default field, 'mozart' in composer field and search range between 1700, 1800 in birth field\n\n  - violin allcomposer:wolfgang mozart\n\n    search 'violin' in default field and any terms after allcomposer will be searched in composer field\n\n  - violin -sonata birth:~1800\n\n    not contain sonata in default field\n\n  - violin -composer:mozart\n\n    not contain mozart in composer field\n\n  - violin or piano genre:00001101/all\n\n    matched all 5, 6 and 8th bits are 1. also /any or /none is available  \n\n  - violin or ((piano composer:mozart) genre:00001101/any)\n\n    support unlimited priority '()' and 'or' operators\n\n  - (violin or ((allcomposer:mozart wolfgang) -amadeus)) sonata (genre:00001101/none home:50.6656,8.0489~10000)\n\n    search home location coordinate (50.6656, 8.0489) within 10 Km\n\n  - \"violin sonata\" genre:00001101/none home:50.6656/8.0489~10\n\n    search exaclt phrase \"violin sonata\"\n\n  - \"violin^3 piano\" -composer:\"ludwig van beethoven\"\n\n    search loose phrase \"violin sonata\" within 3 terms\n\n\nModel\n=============\n\nModel manages index, train files, segments and properties.\n\n.. code:: python\n\n  mdl = delune.model (\n    indexdir = [dirs],\n    mode = [ CREATE | READ | MODIFY | APPEND ], \n    analyzer = None, \n    logger = None\n  )\n\n\nLearner\n---------\n\nFor building model, on DeLune, there're 3 steps need.\n\n- Step I. Index documents to learn\n- Step II. Build Corpus\n- Step III. Selecting features and save trained model\n\n**Step I. Index documents** \n\nLearner use delune.labeled_document, not delune.document. And can additional searchable fields if you need. Label is name of category.\n\n.. code:: python\n\n  learner = mdl.get_learner ()\n  for label, document in trainset:\n\n    labeled_document = delune.labeled_document (label, document)\t  \t      \n    # addtional searcherble fields if you need\n    labeled_document.field (name, value, ftype = TEXT, lang = \"un\", encoding = None)    \n    learner.add_document (labeled_document)\n\n  learner.close ()\n\n\n**Step II. Building Corpus** \n\nDocument Frequency (DF) is one of major factor of classifier. Low DF is important to searching but not to classifier. One of importance part of learning is selecting valuable terms, but so low DF terms is not very helpful for classifying new document because new document has also low probablity of appearance.\n\nSo for learnig/classification efficient, it's useful to eliminate too low and too high DF terms. For example, Let's assume you index 30,000 web pages for learing and there're about 100,000 terms. If you build corpus with all terms, it takes so long time for learing. But if you remove DF < 10 and DF > 7000 terms, 75% - 80% of all terms will be removed.\n\n.. code:: python  \n\n  # reopen model with MODIFY\n  mdl = delune.model (indexdir, MODIFY)\n  learner = mdl.get_learner ()\n\n  # show terms order by DF for examin\n  learner.listbydf (dfmin = 10, dfmax = 7000)\n\n  # build corpus and save\n  learner.build (dfmin = 10, dfmax = 7000)\n\nAs a result, corpus built with about 25,000 terms. It will take time by number of terms.\n\n\n**Step III. Feature Selecting and Saving Model** \n\nFeatures means most valuable terms to classify new documents. It is important understanding many/few features is not good for best result. It maybe most important to select good features for classification.\n\nFor example of my URL classification into 2 classes works show below results. Classifier is NAIVEBAYES, selector is GSS and min DF is 2. Train set is 20,000, test set is 2,000.\n\n  - features 3,000 => 82.9% matched, 73 documents is unclassified\n  - features 2,000 => 82.9% matched, 73 documents is unclassified\n  - features 1,500 => 83.4% matched, 75 documents is unclassified\n  - features 1,000 => 83.6% matched, 79 documents is unclassified\n  - features   500 => 83.1% matched, 86 documents is unclassified\n  - features   200 => 81.1% matched, 108 documents is unclassified\n  - features   50 => 76.0% matched, 155 documents is unclassified\n  - features   10 => 58.7% matched, 326 documents is unclassified\n\nAs results show us that over 2,000 snd under 1,000 features will be unchanged or degraded for classification quality. Also to the most classifiers, too few features increase unclassified ratio but especially to NAIVEBAYES, too many features will increase unclassified ratio cause of its calculating way.\n\n.. code:: python  \n\n  mdl = delune.model (indexdir, MODIFY)\n  learner = mdl.get_learner ()\n\n  learner.train (\n    cl_for = [\n      ALL (default) | NAIVEBAYES | FEATUREVOTE | \n      TFIDF | SIMILARITY | ROCCHIO | MULTIPATH\n    ],\n    select = number of features if value is > 1 or ratio,\n    selector = [\n      CHI2 | GSS | DF | NGL | MI | TFIDF | IG | OR | \n      OR4P | RS | LOR | COS | PPHI | YULE | RMI\n    ],\n    orderby = [SUM | MAX | AVG],\n    dfmin = 0, \n    dfmax = 0\n  )\n  learner.close ()\n\n- cl_for: train for which classifier, if not specified this features used default for every classifiers haven't own feature set. So train () can be called repeatly for each classifiers\n\n- select: number of features if value is > 1 or ratio to all terms. Generally it might be not over 7,000 features for classifying web pages or news articles into 20 classes.\n\n- selector: mathemetical term scoring alorithm to selecting features considering relation between term and term / term and label. Also DF, and term frequency (TF) etc.\n\n- orderby: final scoring method. one of sum, max, average value\n\n- dfmin, dfmax: In spite of it had been already removed by build(), it can be also additional removed for optimal result for specific classifier\n\n\nIf you remove training data for specific classifier,\n\n.. code:: python  \n\n  mdl = delune.model (indexdir, MODIFY)\n  learner = mdl.get_learner ()\n\n  learner.untrain (cl_for)\n  learner.close ()\n\n\n**Finding Best Training Options**\n\nGenerally, differnce attibutes of data set, it hard to say which options are best. It is stongly necessary number of times repeating process between train () and guess () for best result and that's not easy process.\n\n- index ()\n- build ()\n- train (initial options)\n- measure results with guess ()\n- append additional documents, build () if need\n- train (another options)\n- measure results again with guess ()\n- ...\n- find best optiaml training options with your data set\n\nFor getting result accuracy, your pre-requisite data should be splitted into train set for tran () and test set for guess () to measure like `precision and recall`_.\n\nFor example, there were 27,000 web pages to training set and 2,700 test set for classifying to spam page or not. Total indexed terms are 199,183 and I eliminated 94% terms by DF < 30 or DF > 7000 and remains only 10,221 terms.\n\n- F: selected features by OR(Odds Ratio) MAX\n- NB: NAIVEBAYES, RO: ROCCHIO\n- Numbers means: Matched % Ratio Excluding Unclassified (Unclassified Documents)\n\n  - F 7,000: NB 97.2 (1,100), RO 95.4 (50)\n  - F 5,000: NB 97.4 (493), RO 94.8 (69) \n  - F 4,000: NB 96.6 (282), RO 91.6 (96)\n  - F 3,000: NB 93.2 (214), RO 86.2 (151)\n  - F 2,000: NB 89.4 (293), RO 80.1 (281)\n\nWhich do you choice? In my case, I choose F 5,000 with ROCCHIO cause of low unclassified ratio. But if speed was more important I might choice F 3,000 with NAIVEBAYES.\n\nAnyway everything is done, and if you has been found optimal parameters, you can optimize classifier model.\n\n.. code:: python\n\n  mdl = delune.model (indexdir, delune.MODIFY, an)\n  learner = mdl.get_learner ()\n  learner.optimize ()\n  learner.close ()\n\nNote that once called optimize (),\n\n- you cannot add additional training documents\n- you cannot rebuild corpus by calling build () again\n- but you can still call train () any time\n\nThe reason why when low/high DF terms are eliminated by optimize (), related index files will be also shrinked unrecoverably for performance. Then if these works are needed, you should do from step I again.\n\nIf you don't do optimize it make SIMILARITY and ROCCHIO classifiers inefficient (also it will be NOT influence to NAIVEBAYES, TFDIF, FEATUREVOTE classifiers). But you think it's more important retraining regulary rather than speed performance, you should not optimize.\n\n.. _`precision and recall`: https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n**Feature Selecting Methods**\n\n  - CHI2 = Chi Square Statistic\n  - GSS = GSS Coefficient \n  - DF = Document Frequency\n  - CF = Category Frequency\n  - NGL = NGL\n  - MI = Mutual Information\n  - TFIDF = Term Frequecy - Inverted Document Frequency\n  - IG = Information Gain\n  - OR = Odds Ratio\n  - OR4P = Kind of Odds Ratio(? can't remember)\n  - RS = Relevancy Score\n  - LOR = Log Odds Ratio\n  - COS = Cosine Similarity \n  - PPHI = Pearson's PHI\n  - YULE = Yule\n  - RMI = Residual Mutual Information\n\nI personally prefer OR, IG and GSS selectors with MAX method.\n\n\nClassifier\n------------\n\nFinally,\n\n.. code:: python  \n\n  classifier = mdl.get_classifier ()\n  classifier.quess (\n    qs, \n    lang = \"un\", \n    cl = [ \n      NAIVEBAYES (Default) | FEATUREVOTE | ROCCHIO | \n      TFIDF | SIMILARITY | META | MULTIPATH\n    ],\n    top = 0,\n    cond = \"\"\n  )\n\n  classifier.cluster (\n    qs, \n    lang = \"un\"    \n  )\n\n  classifier.close ()\n\n- qs: full text stream to classify\n\n- lang\n\n- cl: which classifer, META is default\n\n- top: how many high scored classified results, default is 0, means high scored result(s) only\n\n- cond: conditional document selecting query. Some classifier execute calculating with lots of documents like ROCCHIO and SIMILARITY, so it's useful shrinking number of documents. This  only work when you put additional searchable fields using labeled_document.field (...).\n\n**Implemented Classifiers**\n\n  - NAIVEBAYES: Naive Bayes Probablility, default guessing\n  - FEATUREVOTE: Feature Voting Classifier\n  - ROCCHIO: Rocchio Classifier\n  - TFIDF: Max TDIDF Score\n  - SIMILARITY: Max Cosine Similarity\n  - MULTIPATH: Experimental Multi Path Classifier, terms of classifying document will be clustered into multiple sets by co-word frequency before guessing\n  - META: merging and decide with multiple results guessed by NAIVEBAYES, FEATUREVOTE, ROCCHIO Classifiers\n\nIf you need speed most of all, NAIVEBAYES is a good choice. NAIVEBAYES is an old theory but it still works with very high performance at both speed and accuracy if given proper training set.\n\nMore detail for each classifier alorithm, googling please.\n\n\n**Optimizing Each Classifiers**\n\nFor give some detail options to a classifier you can use setopt (classfier, option name = option value,...).\n\n\n.. code:: python  \n\n  classifier = mdl.get_classifier ()\n  classifier.setopt (delune.ROCCHIO, topdoc = 200)\n\nSIMILARITY, ROCCHIO classifiers basically have to compare with entire indexed document documents, but DeLune can compare with selected documents by 'topdoc' option. These number of documents will be selected by high TFIDF score for classifying performance reason. Default topdoc value is 100. If you set to 0, DeLune will compare with all documents have one of features at least. But on my experience, there's no critical difference except speed performance.\n\nCurrently available options are:\n\n* ALL\n\n  - verbose = False\n\n* ROCCHIO\n\n  - topdoc = 100\n\n* MULTIPATH\n\n  + subcl = [ FEATUREVOTE (default) | NAIVEBAYES | ROCCHIO ]\n  + scoreby = [ IG (default) | MI | OR | R ]\n  + choiceby = [ AVG (default) | MIN ], when scorring between term and each terms in cluster, which do you want to use value\n  + threshold = 1.0, float value for creating new cluster and this value is measured with Information Gain and value range is somewhat different by number of training documents.\n\n\nDocument Cluster\n-----------------\n\nTODO\n\n.. code:: python  \n\n  cluster = mdl.get_dcluster ()\n\n\nTerm Cluster\n-------------\n\nTODO\n\n.. code:: python  \n\n  cluster = mdl.get_tcluster ()\n\n\n\nHandling Multiple Searchers & Classifiers\n===========================================\n\nIn case of creating multiple searchers and classifers, delune.task might be useful.\nHere's a script named 'config.py'\n\n.. code:: python\n\n  import delune\n  from delune.lib import logger\n\n  def start_delune (numthreads, logger):    \n    delune.configure (numthreads, logger)\n\n    analyzer = delune.standard_analyzer ()\n    col = delune.collection (\"./data1\", delune.READ, analyzer)\n    delune.assign (\"data1\", col.get_searcher (max_result = 2000))\n\n    analyzer = delune.standard_analyzer (max_term = 1000, stem = 2)\n    mdl = delune.model (\"./data2\", delune.READ, analyzer)\n    delune.assign (\"data2\", mdl.get_classifier ())\n\nThe first argument of assign () is alias for searcher or classifier.\n\nIf you call config.start_delune () at any script, you can just import delune and use it at another python scripts.\n\n.. code:: python\n\n  import delune\n\n  delune.query (\"data1\", \"mozart sonatas\")\n  delune.guess (\"data2\", \"mozart sonatas\")\n\n  # close and resign  \n  delune.close (\"data1\")\n  delune.resign (\"data1\")\n\n\nAt the end of you app, call delune.shutdown ()\n\n.. code:: python\n\n  import delune\n\n  delune.shutdown ()\n\n\nAPI Export Using Skitai\n=========================\n\n**New in version 0.12.14**\n\nYou can use RESTful API with `Skitai-Saddle`_.\n\nCopy and save below code to app.py.\n\n.. code:: python\n\n  import os\n  import delune\n  import skitai  \n\n  if __name__ == \"__main__\":\n    pref = skitai.pref ()\n    pref.use_reloader = 1\n    pref.debug = 1\n\n    config = pref.config\n    config.sched = \"0/5 * * * *\"  \n    config.local = \"http://127.0.0.1:5000/v1\"\n\n    config.remote = os.environ.get (\"DELUNE_ORIGIN\")\n    config.enable_mirror = config.remote\n\n    config.resource_dir = skitai.joinpath ('resources')\n    config.enable_index = True\n\n    config.logpath = None\n    skitai.trackers ('delune:collection')\n    skitai.mount (\"/v1\", delune, \"app\", pref)\n    skitai.run (  \n      workers = 2,\n      port = 5000,\n      logpath = config.logpath\n    )\n\nThis app run indexing job for every 5 minutes at backgound.\n\nIf you want read-only replica, set origin server at your account environement,\n\n.. code:: bash  \n\n  export DELUNE_ORIGIN=http://192.168.1.200:5000/v1\n\nAll collections will be replicated from http://192.168.1.200:5000/v1 API for every 5 minutes.\n\nThen run app.\n\n.. code:: bash\n\n  python app.py -v\n\nHere's example of client side indexing script using API.\n\n.. code:: python\n\n  colopt = {\n    'data_dir': [\n    \t'models/0/books',\n    \t'models/1/books',\n    \t'models/2/books'\n    ],\n    'analyzer': {\n    \t\"ngram\": 0,\n    \t\"stem_level\": 1,\t\t\t\t\t\t\n    \t\"strip_html\": 0,\n    \t\"make_lower_case\": 1\t\t\n    },\n    'indexer': {\n    \t'force_merge': 0,\n    \t'max_memory': 10000000,\n    \t'max_segments': 10,\n    \t'lazy_merge': (0.3, 1),\n    },\t\n    'searcher': {\n      'max_result': 2000,\n      'num_query_cache': 200\n    }\n  }\t\n\n  import requests    \n  session = requests.Session ()\n\n  # check current collections\n  r = session.get ('http://127.0.0.1:5000/v1/').json ()\n  if 'books' not in r [\"collections\"]:  \n    # collections dose not exist, then create\n    session.post ('http://127.0.0.1:5000/v1/books', colopt)\n\n  dbc = db.connect (...)\n  cursor = dbc.curosr ()\n  cursor.execute (...)  \n\n  numdoc = 0\n  while 1:\n    row = cursor.fetchone ()\n    if not row: break\n    doc = delune.document (row._id)\n    doc.content ({\"author\": row.author, \"title\": row.title , \"abstract\": row.abstract})\n    doc.snippet (row.abstract)\n    doc.field ('default', \"%s %s\" % (row.title, row.abstract), delune.TEXT, 'en')\n    doc.field ('title', row.title, delune.TEXT, 'en')\n    doc.field ('author', row.author, delune.STRING)\n    doc.field ('isbn', row.isbn, delune.STRING)\n    doc.field ('year', row.year, delune.INT16) \n\n    session.post ('http://127.0.0.1:5000/v1/books/documents', doc.as_json ())\n    numdoc += 1\n    if numdoc % 1000:\n    \tsession.get ('http://127.0.0.1:5000/v1/books/commit')\n\n  cursor.close ()\n  dbc.close ()\n\nall APIs are:\n\n.. code:: python\n\n  # add new collection with options\n  session.post ('http://127.0.0.1:5000/v1\", colopt)  \n  # get collection status and options\n  session.get ('http://127.0.0.1:5000/v1/books\")  \n  # modify collection options\n  session.patch ('http://127.0.0.1:5000/v1/books\", colopt)  \n  # remove collection but preserve all index files\n  session.remove ('http://127.0.0.1:5000/v1/books\")\n  # remove collection with all index files\n  session.remove ('http://127.0.0.1:5000/v1/books?side_effect=data\")\n  # undo remove collection with all index files\n  session.get ('http://127.0.0.1:5000/v1/books?side_effet=undo\")  \n\n  # get collection locks\n  session.get ('http://127.0.0.1:5000/v1/books/locks\")  \n  # create 'custom' lock\n  session.post ('http://127.0.0.1:5000/v1/books/locks/custom\")  \n  # delete 'custom' lock\n  session.delete ('http://127.0.0.1:5000/v1/books/locks/custom\")\n\n  # add new document\n  session.post (\n    'http://127.0.0.1:5000/v1/books/documents\", \n    doc.as_json ()\n  )\n  # modify document\n  session.patch (\n    'http://127.0.0.1:5000/v1/books/documents/\" + row._id, \n    doc.as_json ()\n  )\n  # delete document by document_id\n  session.delete ('http://127.0.0.1:5000/v1/books/documents/\" + row._id)\n\n  # truncate all documents from collection\n  session.delete ('http://127.0.0.1:5000/v1/books/documents?truncate_confirm=books')\n\n  # search\n  session.get ('http://127.0.0.1:5000/v1/books/search?q=title:book\")\n  # guess\n  session.get ('http://127.0.0.1:5000/v1/books/guess?q=title:book\")\n  # delete documents by search\n  session.delete ('http://127.0.0.1:5000/v1/books/search?q=title:book\")\n\n  # commit document queue\n  session.get ('http://127.0.0.1:5000/v1/books/commit')\n  # remove document queue\n  session.get ('http://127.0.0.1:5000/v1/books/rollback')  \n\nNote: DeLune doesn't check uniqueness of document ID, it means if you post multiple documents with same document ID, delune will index all of them with regardless document ID. If you want to keep uniqueness, you SHOULD use 'patch' method NOT 'post'.\n\nFor more detail about API, see `app.py`_.\n\n.. _`Skitai-Saddle`: https://pypi.python.org/pypi/skitai\n.. _`app.py`: https://gitlab.com/hansroh/delune/blob/master/delune/export/skitai/app.py\n\n\nLinks\n======\n\n- `GitLab Repository`_\n- Bug Report: `GitLab issues`_\n\n.. _`GitLab Repository`: https://gitlab.com/hansroh/delune\n.. _`GitLab issues`: https://gitlab.com/hansroh/delune/issues\n\n\n\nChange Log\n============\n\n  DeLune\n\n  0.1\n\n  - change package name from Wissen to DeLune\n\n  Wissen\n\n  0.13\n\n  - fix using lock\n  - add truncate collection API\n  - fix updating document\n  - change replicating way to use sticky session connection with origin server\n  - fix file creation mode on posix\n  - fix using lock with multiple workers\n  - change delune.document method names\n  - fix index queue file locking\n\n  0.12 \n\n  - add biword arg to standard_analyzer\n  - change export package name from appack to package\n  - add Skito-Saddle app\n  - fix analyzer.count_stopwords return value\n  - change development status to Alpha\n  - add delune.assign(alias, searcher/classifier) and query(alias), guess(alias)\n  - fix threads count and memory allocation\n  - add example for Skitai-Saddle app to mannual\n\n  0.11 \n\n  - fix HTML strip and segment merging etc.\n  - add MULTIPATH classifier\n  - add learner.optimize ()\n  - make learner.build & learner.train efficient\n\n  0.10 - change version format, remove all str*_s ()\n\n  0.9 - support Python 3.x\n\n  0.8 - change license from BSD to GPL V3\n\n\n",
    "docs_url": null,
    "download_url": "https://pypi.python.org/pypi/delune",
    "downloads": {
      "last_day": 0,
      "last_month": 0,
      "last_week": 0
    },
    "home_page": "https://gitlab.com/hansroh/delune",
    "keywords": "",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "delune",
    "platform": "posix",
    "project_url": "https://pypi.org/project/delune/",
    "release_url": "https://pypi.org/project/delune/0.2.1/",
    "requires_dist": [],
    "requires_python": "",
    "summary": "DeLune Full-Text Search & Classification Engine",
    "version": "0.2.1"
  },
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "3368a99d1d1dba71b9acab0950e34419",
          "sha256": "437c03cfe6484fd611e5d612f39f3b389e0aa0c7baf6e77c07c219cd683d3e28"
        },
        "downloads": 0,
        "filename": "delune-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3368a99d1d1dba71b9acab0950e34419",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "size": 1837,
        "upload_time": "2017-09-13T13:45:39",
        "url": "https://files.pythonhosted.org/packages/c8/3a/54158ec391bf2b2b8b652c27fea327202e1b04672e3aa37fde7f07e8b758/delune-0.1-py3-none-any.whl"
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "md5": "82f29715e1de4a6fefa244e16096b8d0",
          "sha256": "b802ae374b706cabc12d972cd5afd51e7b980db85d343b7987b777fe15507a52"
        },
        "downloads": 0,
        "filename": "delune-0.2-cp35-cp35m-win_amd64.whl",
        "has_sig": false,
        "md5_digest": "82f29715e1de4a6fefa244e16096b8d0",
        "packagetype": "bdist_wheel",
        "python_version": "cp35",
        "size": 2574902,
        "upload_time": "2017-09-14T06:52:26",
        "url": "https://files.pythonhosted.org/packages/5b/4f/1f5e227f43815bb306410aaf030cbbc93b838ae30d0484352ca10308bdf8/delune-0.2-cp35-cp35m-win_amd64.whl"
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "7431b1f39d0292051b6fbb811e0959c7",
          "sha256": "71525911230fb2a69f53c98a06066c0ab16a1e45a2bdd79127a2935f6456a77c"
        },
        "downloads": 0,
        "filename": "delune-0.2.1-cp35-cp35m-win_amd64.whl",
        "has_sig": false,
        "md5_digest": "7431b1f39d0292051b6fbb811e0959c7",
        "packagetype": "bdist_wheel",
        "python_version": "cp35",
        "size": 2574932,
        "upload_time": "2017-09-14T07:10:57",
        "url": "https://files.pythonhosted.org/packages/52/21/23e7d3d6c80321d1232adeaf1b85da7aff4e0bf341509d10154556edb330/delune-0.2.1-cp35-cp35m-win_amd64.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "23d149bdba30fc6c446806c82b65d448",
          "sha256": "b90c883cf760d589923ba454600ff9d14747f4d071460821f8a2942f25b59e91"
        },
        "downloads": 0,
        "filename": "delune-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "23d149bdba30fc6c446806c82b65d448",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 1730717,
        "upload_time": "2017-09-14T07:11:26",
        "url": "https://files.pythonhosted.org/packages/0f/32/805594aa74ee1e0a0451f0dd2961108bab2702621fffe0029bd44c0f95ea/delune-0.2.1.tar.gz"
      }
    ],
    "0.2b1": [
      {
        "comment_text": "",
        "digests": {
          "md5": "e3f1ba30b9d8a586d92cb7db067999d1",
          "sha256": "ea1d3ce083297afc85f823e9b7c4a9a4288e4b4979947de91ad66cef789a0827"
        },
        "downloads": 0,
        "filename": "delune-0.2b1-cp35-cp35m-win_amd64.whl",
        "has_sig": false,
        "md5_digest": "e3f1ba30b9d8a586d92cb7db067999d1",
        "packagetype": "bdist_wheel",
        "python_version": "cp35",
        "size": 2574931,
        "upload_time": "2017-09-14T06:45:24",
        "url": "https://files.pythonhosted.org/packages/89/c0/8ee8767cdfa71a85a934fedcba1ac8b9f43b12f8a451621ee8ebd0cb3a25/delune-0.2b1-cp35-cp35m-win_amd64.whl"
      },
      {
        "comment_text": "",
        "digests": {
          "md5": "57a4746b38390d5abfde3ecdf3fda806",
          "sha256": "400852ecc5b96c629df970a3cf8cd3e56c721d14fa32e01acc1fcb05bebc6916"
        },
        "downloads": 0,
        "filename": "delune-0.2b1.tar.gz",
        "has_sig": false,
        "md5_digest": "57a4746b38390d5abfde3ecdf3fda806",
        "packagetype": "sdist",
        "python_version": "source",
        "size": 1730700,
        "upload_time": "2017-09-14T06:45:54",
        "url": "https://files.pythonhosted.org/packages/6e/db/1ae0fce34e4f9abd18d9ec59d354c5fb650a8c053517d4051cad4a59b7f1/delune-0.2b1.tar.gz"
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "md5": "7431b1f39d0292051b6fbb811e0959c7",
        "sha256": "71525911230fb2a69f53c98a06066c0ab16a1e45a2bdd79127a2935f6456a77c"
      },
      "downloads": 0,
      "filename": "delune-0.2.1-cp35-cp35m-win_amd64.whl",
      "has_sig": false,
      "md5_digest": "7431b1f39d0292051b6fbb811e0959c7",
      "packagetype": "bdist_wheel",
      "python_version": "cp35",
      "size": 2574932,
      "upload_time": "2017-09-14T07:10:57",
      "url": "https://files.pythonhosted.org/packages/52/21/23e7d3d6c80321d1232adeaf1b85da7aff4e0bf341509d10154556edb330/delune-0.2.1-cp35-cp35m-win_amd64.whl"
    },
    {
      "comment_text": "",
      "digests": {
        "md5": "23d149bdba30fc6c446806c82b65d448",
        "sha256": "b90c883cf760d589923ba454600ff9d14747f4d071460821f8a2942f25b59e91"
      },
      "downloads": 0,
      "filename": "delune-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "23d149bdba30fc6c446806c82b65d448",
      "packagetype": "sdist",
      "python_version": "source",
      "size": 1730717,
      "upload_time": "2017-09-14T07:11:26",
      "url": "https://files.pythonhosted.org/packages/0f/32/805594aa74ee1e0a0451f0dd2961108bab2702621fffe0029bd44c0f95ea/delune-0.2.1.tar.gz"
    }
  ]
}